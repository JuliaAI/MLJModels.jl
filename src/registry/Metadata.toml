
[BetaML.BetaMLGenericImputer]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Known}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Known}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Imputation.BetaMLGenericImputer"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Impute missing values using a vector (one per column) of arbitrary learning models (classifiers/regressors) that implement `m = Model([options])`, `train!(m,X,Y)` and `predict(m,X)` (default to Random Forests), from the Beta Machine Learning Toolkit (BetaML). Experimental."
":name" = "BetaMLGenericImputer"
":human_name" = "beta ml generic imputer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":transform"]
":hyperparameters" = "`(:models, :recursivePassages, :verbosity, :rng)`"
":hyperparameter_types" = "`(\"Union{Nothing, Vector}\", \"Int64\", \"BetaML.Api.Verbosity\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.RandomForestRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Trees.RandomForestRegressor"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "A simple Random Forest ensemble for regression with support for Missing data, from the Beta Machine Learning Toolkit (BetaML)."
":name" = "RandomForestRegressor"
":human_name" = "random forest regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":predict"]
":hyperparameters" = "`(:nTrees, :maxDepth, :minGain, :minRecords, :maxFeatures, :splittingCriterion, :β, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Int64\", \"Float64\", \"Int64\", \"Int64\", \"Function\", \"Float64\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.RandomForestClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.Finite}}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}, AbstractVector{<:Union{Missing, ScientificTypesBase.Finite}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Trees.RandomForestClassifier"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "A simple Random Forest ensemble for classification with support for Missing data, from the Beta Machine Learning Toolkit (BetaML)."
":name" = "RandomForestClassifier"
":human_name" = "random forest classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":fit", ":predict"]
":hyperparameters" = "`(:nTrees, :maxDepth, :minGain, :minRecords, :maxFeatures, :splittingCriterion, :β, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Int64\", \"Float64\", \"Int64\", \"Int64\", \"Function\", \"Float64\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.PerceptronClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Infinite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Infinite}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Perceptron.PerceptronClassifier"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "The classical perceptron algorithm using one-vs-all for multiclass, from the Beta Machine Learning Toolkit (BetaML)."
":name" = "PerceptronClassifier"
":human_name" = "perceptron classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":fit", ":predict"]
":hyperparameters" = "`(:initialθ, :initialθ₀, :maxEpochs, :shuffle, :forceOrigin, :returnMeanHyperplane, :rng)`"
":hyperparameter_types" = "`(\"Vector{Float64}\", \"Float64\", \"Int64\", \"Bool\", \"Bool\", \"Bool\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.DecisionTreeRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Trees.DecisionTreeRegressor"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "A simple Decision Tree for regression with support for Missing data, from the Beta Machine Learning Toolkit (BetaML)."
":name" = "DecisionTreeRegressor"
":human_name" = "decision tree regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":predict"]
":hyperparameters" = "`(:maxDepth, :minGain, :minRecords, :maxFeatures, :splittingCriterion, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Float64\", \"Int64\", \"Int64\", \"Function\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.BetaMLGMMImputer]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Imputation.BetaMLGMMImputer"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Impute missing values using a probabilistic approach (Gaussian Mixture Models) fitted using the Expectation-Maximisation algorithm, from the Beta Machine Learning Toolkit (BetaML). Experimental."
":name" = "BetaMLGMMImputer"
":human_name" = "beta mlgmm imputer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":transform"]
":hyperparameters" = "`(:nClasses, :probMixtures, :mixtures, :tol, :minVariance, :minCovariance, :initStrategy, :verbosity, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Vector{Float64}\", \"Symbol\", \"Float64\", \"Float64\", \"Float64\", \"String\", \"BetaML.Api.Verbosity\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.PegasosClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Infinite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Infinite}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Perceptron.PegasosClassifier"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "The gradient-based linear \"pegasos\" classifier using one-vs-all for multiclass, from the Beta Machine Learning Toolkit (BetaML)."
":name" = "PegasosClassifier"
":human_name" = "pegasos classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":fit", ":predict"]
":hyperparameters" = "`(:initialθ, :initialθ₀, :λ, :η, :maxEpochs, :shuffle, :forceOrigin, :returnMeanHyperplane, :rng)`"
":hyperparameter_types" = "`(\"Vector{Float64}\", \"Float64\", \"Float64\", \"Function\", \"Int64\", \"Bool\", \"Bool\", \"Bool\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.KMedoids]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`AbstractArray{<:ScientificTypesBase.Multiclass}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Clustering.KMedoids"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "The K-medoids clustering algorithm with customisable distance function, from the Beta Machine Learning Toolkit (BetaML)."
":name" = "KMedoids"
":human_name" = "k medoids"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":fitted_params", ":predict", ":transform"]
":hyperparameters" = "`(:K, :dist, :initStrategy, :Z₀, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Function\", \"String\", \"Union{Nothing, Matrix{Float64}}\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.BetaMLGMMRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Infinite}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Infinite}}}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.GMM.BetaMLGMMRegressor"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "A non-linear regressor derived from fitting the data on a probabilistic model (Gaussian Mixture Model). Relatively fast."
":name" = "BetaMLGMMRegressor"
":human_name" = "beta mlgmm regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":predict"]
":hyperparameters" = "`(:nClasses, :probMixtures, :mixtures, :tol, :minVariance, :minCovariance, :initStrategy, :maxIter, :verbosity, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Vector{Float64}\", \"Symbol\", \"Float64\", \"Float64\", \"Float64\", \"String\", \"Int64\", \"BetaML.Api.Verbosity\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.KMeans]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`AbstractArray{<:ScientificTypesBase.Multiclass}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Clustering.KMeans"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "The classical KMeans clustering algorithm, from the Beta Machine Learning Toolkit (BetaML)."
":name" = "KMeans"
":human_name" = "k means"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":fitted_params", ":predict", ":transform"]
":hyperparameters" = "`(:K, :dist, :initStrategy, :Z₀, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Function\", \"String\", \"Union{Nothing, Matrix{Float64}}\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.DecisionTreeClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.Finite}}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}, AbstractVector{<:Union{Missing, ScientificTypesBase.Finite}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Trees.DecisionTreeClassifier"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "A simple Decision Tree for classification with support for Missing data, from the Beta Machine Learning Toolkit (BetaML)."
":name" = "DecisionTreeClassifier"
":human_name" = "decision tree classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":fit", ":predict"]
":hyperparameters" = "`(:maxDepth, :minGain, :minRecords, :maxFeatures, :splittingCriterion, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Float64\", \"Int64\", \"Int64\", \"Function\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.GMMClusterer]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`AbstractArray{<:ScientificTypesBase.Multiclass}`"
":target_scitype" = "`AbstractArray{<:ScientificTypesBase.Multiclass}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractArray{<:ScientificTypesBase.Multiclass}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.GMM.GMMClusterer"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "A Expectation-Maximisation clustering algorithm with customisable mixtures, from the Beta Machine Learning Toolkit (BetaML)."
":name" = "GMMClusterer"
":human_name" = "gmm clusterer"
":is_supervised" = "`false`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:K, :p₀, :mixtures, :tol, :minVariance, :minCovariance, :initStrategy, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"AbstractVector{Float64}\", \"Symbol\", \"Float64\", \"Float64\", \"Float64\", \"String\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.MissingImputator]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Imputation.MissingImputator"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Impute missing values using an Expectation-Maximisation clustering algorithm, from the Beta Machine Learning Toolkit (BetaML). Old API, consider also `BetaMLGMMImputer` (equivalent, experimental)"
":name" = "MissingImputator"
":human_name" = "missing imputator"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":transform"]
":hyperparameters" = "`(:K, :p₀, :mixtures, :tol, :minVariance, :minCovariance, :initStrategy, :verbosity, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"AbstractVector{Float64}\", \"Symbol\", \"Float64\", \"Float64\", \"Float64\", \"String\", \"BetaML.Api.Verbosity\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.BetaMLMeanImputer]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Imputation.BetaMLMeanImputer"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Impute missing values using feature (column) mean, with optional record normalisation (using l-`norm` norms), from the Beta Machine Learning Toolkit (BetaML). Experimental."
":name" = "BetaMLMeanImputer"
":human_name" = "beta ml mean imputer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":transform"]
":hyperparameters" = "`(:norm,)`"
":hyperparameter_types" = "`(\"Int64\",)`"
":hyperparameter_ranges" = "`(nothing,)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.BetaMLRFImputer]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Known}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Known}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Imputation.BetaMLRFImputer"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Impute missing values using Random Forests, from the Beta Machine Learning Toolkit (BetaML). Experimental."
":name" = "BetaMLRFImputer"
":human_name" = "beta mlrf imputer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":transform"]
":hyperparameters" = "`(:nTrees, :maxDepth, :minGain, :minRecords, :maxFeatures, :forcedCategoricalCols, :splittingCriterion, :recursivePassages, :verbosity, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Union{Nothing, Int64}\", \"Float64\", \"Int64\", \"Union{Nothing, Int64}\", \"Vector{Int64}\", \"Union{Nothing, Function}\", \"Int64\", \"BetaML.Api.Verbosity\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.KernelPerceptronClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Infinite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Infinite}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Perceptron.KernelPerceptronClassifier"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "The kernel perceptron algorithm using one-vs-one for multiclass, from the Beta Machine Learning Toolkit (BetaML)."
":name" = "KernelPerceptronClassifier"
":human_name" = "kernel perceptron classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":fit", ":predict"]
":hyperparameters" = "`(:K, :maxEpochs, :initialα, :shuffle, :rng)`"
":hyperparameter_types" = "`(\"Function\", \"Int64\", \"Vector{Int64}\", \"Bool\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[NearestNeighborModels.KNNClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "NearestNeighborModels"
":package_license" = "MIT"
":load_path" = "NearestNeighborModels.KNNClassifier"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/alan-turing-institute/NearestNeighborModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "K-Nearest Neighbors classifier: predicts the class associated with a new point\nby taking a vote over the classes of the K-nearest points.\n"
":name" = "KNNClassifier"
":human_name" = "knn classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:K, :algorithm, :metric, :leafsize, :reorder, :weights)`"
":hyperparameter_types" = "`(\"Int64\", \"Symbol\", \"Distances.Metric\", \"Int64\", \"Bool\", \"NearestNeighborModels.KNNKernel\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[NearestNeighborModels.MultitargetKNNClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Finite}}`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Finite}}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Finite}}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Density{ScientificTypesBase.Finite}}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "NearestNeighborModels"
":package_license" = "MIT"
":load_path" = "NearestNeighborModels.MultitargetKNNClassifier"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/alan-turing-institute/NearestNeighborModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "K-Nearest Neighbors classifier: predicts the class associated with a new point\nby taking a vote over the classes of the K-nearest points.\n"
":name" = "MultitargetKNNClassifier"
":human_name" = "multitarget knn classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":predict_mode"]
":hyperparameters" = "`(:K, :algorithm, :metric, :leafsize, :reorder, :weights, :output_type)`"
":hyperparameter_types" = "`(\"Int64\", \"Symbol\", \"Distances.Metric\", \"Int64\", \"Bool\", \"NearestNeighborModels.KNNKernel\", \"Type{<:Union{AbstractDict{K, V} where {K<:Union{String, Symbol}, V<:(AbstractVector)}, NamedTuple{names, T} where {N, names, T<:Tuple{Vararg{AbstractVector, N}}}}}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[NearestNeighborModels.MultitargetKNNRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "NearestNeighborModels"
":package_license" = "MIT"
":load_path" = "NearestNeighborModels.MultitargetKNNRegressor"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/alan-turing-institute/NearestNeighborModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "K-Nearest Neighbors regressor: predicts the response associated with a new point\nby taking an weighted average of the response of the K-nearest points.\n"
":name" = "MultitargetKNNRegressor"
":human_name" = "multitarget knn regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:K, :algorithm, :metric, :leafsize, :reorder, :weights)`"
":hyperparameter_types" = "`(\"Int64\", \"Symbol\", \"Distances.Metric\", \"Int64\", \"Bool\", \"NearestNeighborModels.KNNKernel\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[NearestNeighborModels.KNNRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "NearestNeighborModels"
":package_license" = "MIT"
":load_path" = "NearestNeighborModels.KNNRegressor"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/alan-turing-institute/NearestNeighborModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "K-Nearest Neighbors regressor: predicts the response associated with a new point\nby taking an weighted average of the response of the K-nearest points.\n"
":name" = "KNNRegressor"
":human_name" = "knn regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:K, :algorithm, :metric, :leafsize, :reorder, :weights)`"
":hyperparameter_types" = "`(\"Int64\", \"Symbol\", \"Distances.Metric\", \"Int64\", \"Bool\", \"NearestNeighborModels.KNNKernel\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionNeighbors.ABODDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "OutlierDetectionNeighbors"
":package_license" = "MIT"
":load_path" = "OutlierDetectionNeighbors.ABODDetector"
":package_uuid" = "51249a0a-cb36-4849-8e04-30c7f8d311bb"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionNeighbors.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nABODDetector(k = 5,\n             metric = Euclidean(),\n             algorithm = :kdtree,\n             static = :auto,\n             leafsize = 10,\n             reorder = true,\n             parallel = false,\n             enhanced = false)\n```\n\nDetermine outliers based on the angles to its nearest neighbors. This implements the `FastABOD` variant described in the paper, that is, it uses the variance of angles to its nearest neighbors, not to the whole dataset, see [1]. \n\n*Notice:* The scores are inverted, to conform to our notion that higher scores describe higher outlierness.\n\n## Parameters\n\n```\nk::Integer\n```\n\nNumber of neighbors (must be greater than 0).\n\n```\nmetric::Metric\n```\n\nThis is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric.\n\n```\nalgorithm::Symbol\n```\n\nOne of `(:kdtree, :balltree)`. In a `kdtree`, points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A *brutetree* linearly searches all points in a brute force fashion and works with any Metric. A *balltree* recursively splits points into groups bounded by hyper-spheres and works with any Metric.\n\n```\nstatic::Union{Bool, Symbol}\n```\n\nOne of `(true, false, :auto)`. Whether the input data for fitting and transform should be statically or dynamically allocated. If `true`, the data is statically allocated. If `false`, the data is dynamically allocated. If `:auto`, the data is dynamically allocated if the product of all dimensions except the last is greater than 100.\n\n```\nleafsize::Int\n```\n\nDetermines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points.\n\n```\nreorder::Bool\n```\n\nWhile building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true.\n\n```\nparallel::Bool\n```\n\nParallelize `score` and `predict` using all threads available. The number of threads can be set with the `JULIA_NUM_THREADS` environment variable. Note: `fit` is not parallel.\n\n```\nenhanced::Bool\n```\n\nWhen `enhanced=true`, it uses the enhanced ABOD (EABOD) adaptation proposed by [2].\n\n## Examples\n\n```julia\nusing OutlierDetection: ABODDetector, fit, score\ndetector = ABODDetector()\nX = rand(10, 100)\nresult = fit(detector, X)\ntest_scores = transform(detector, result.model, X)\n```\n\n## References\n\n[1] Kriegel, Hans-Peter; S hubert, Matthias; Zimek, Arthur (2008): Angle-based outlier detection in high-dimensional data.\n\n[2] Li, Xiaojie; Lv, Jian Cheng; Cheng, Dongdong (2015): Angle-Based Outlier Detection Algorithm with More Stable Relationships.\n"
":name" = "ABODDetector"
":human_name" = "abod detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:k, :metric, :algorithm, :static, :leafsize, :reorder, :parallel, :enhanced)`"
":hyperparameter_types" = "`(\"Integer\", \"Distances.Metric\", \"Symbol\", \"Union{Bool, Symbol}\", \"Integer\", \"Bool\", \"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionNeighbors.DNNDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "OutlierDetectionNeighbors"
":package_license" = "MIT"
":load_path" = "OutlierDetectionNeighbors.DNNDetector"
":package_uuid" = "51249a0a-cb36-4849-8e04-30c7f8d311bb"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionNeighbors.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nDNNDetector(d = 0,\n            metric = Euclidean(),\n            algorithm = :kdtree,\n            leafsize = 10,\n            reorder = true,\n            parallel = false)\n```\n\nAnomaly score based on the number of neighbors in a hypersphere of radius `d`. Knorr et al. [1] directly converted the resulting outlier scores to labels, thus this implementation does not fully reflect the approach from the paper.\n\n## Parameters\n\n```\nd::Real\n```\n\nThe hypersphere radius used to calculate the global density of an instance.\n\n```\nmetric::Metric\n```\n\nThis is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric.\n\n```\nalgorithm::Symbol\n```\n\nOne of `(:kdtree, :balltree)`. In a `kdtree`, points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A *brutetree* linearly searches all points in a brute force fashion and works with any Metric. A *balltree* recursively splits points into groups bounded by hyper-spheres and works with any Metric.\n\n```\nstatic::Union{Bool, Symbol}\n```\n\nOne of `(true, false, :auto)`. Whether the input data for fitting and transform should be statically or dynamically allocated. If `true`, the data is statically allocated. If `false`, the data is dynamically allocated. If `:auto`, the data is dynamically allocated if the product of all dimensions except the last is greater than 100.\n\n```\nleafsize::Int\n```\n\nDetermines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points.\n\n```\nreorder::Bool\n```\n\nWhile building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true.\n\n```\nparallel::Bool\n```\n\nParallelize `score` and `predict` using all threads available. The number of threads can be set with the `JULIA_NUM_THREADS` environment variable. Note: `fit` is not parallel.\n\n## Examples\n\n```julia\nusing OutlierDetection: DNNDetector, fit, score\ndetector = DNNDetector()\nX = rand(10, 100)\nresult = fit(detector, X)\ntest_scores = transform(detector, result.model, X)\n```\n\n## References\n\n[1] Knorr, Edwin M.; Ng, Raymond T. (1998): Algorithms for Mining Distance-Based Outliers in Large Datasets.\n"
":name" = "DNNDetector"
":human_name" = "dnn detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:metric, :algorithm, :static, :leafsize, :reorder, :parallel, :d)`"
":hyperparameter_types" = "`(\"Distances.Metric\", \"Symbol\", \"Union{Bool, Symbol}\", \"Integer\", \"Bool\", \"Bool\", \"Real\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionNeighbors.LOFDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "OutlierDetectionNeighbors"
":package_license" = "MIT"
":load_path" = "OutlierDetectionNeighbors.LOFDetector"
":package_uuid" = "51249a0a-cb36-4849-8e04-30c7f8d311bb"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionNeighbors.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLOFDetector(k = 5,\n            metric = Euclidean(),\n            algorithm = :kdtree,\n            leafsize = 10,\n            reorder = true,\n            parallel = false)\n```\n\nCalculate an anomaly score based on the density of an instance in comparison to its neighbors. This algorithm introduced the notion of local outliers and was developed by Breunig et al., see [1].\n\n## Parameters\n\n```\nk::Integer\n```\n\nNumber of neighbors (must be greater than 0).\n\n```\nmetric::Metric\n```\n\nThis is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric.\n\n```\nalgorithm::Symbol\n```\n\nOne of `(:kdtree, :balltree)`. In a `kdtree`, points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A *brutetree* linearly searches all points in a brute force fashion and works with any Metric. A *balltree* recursively splits points into groups bounded by hyper-spheres and works with any Metric.\n\n```\nstatic::Union{Bool, Symbol}\n```\n\nOne of `(true, false, :auto)`. Whether the input data for fitting and transform should be statically or dynamically allocated. If `true`, the data is statically allocated. If `false`, the data is dynamically allocated. If `:auto`, the data is dynamically allocated if the product of all dimensions except the last is greater than 100.\n\n```\nleafsize::Int\n```\n\nDetermines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points.\n\n```\nreorder::Bool\n```\n\nWhile building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true.\n\n```\nparallel::Bool\n```\n\nParallelize `score` and `predict` using all threads available. The number of threads can be set with the `JULIA_NUM_THREADS` environment variable. Note: `fit` is not parallel.\n\n## Examples\n\n```julia\nusing OutlierDetection: LOFDetector, fit, score\ndetector = LOFDetector()\nX = rand(10, 100)\nresult = fit(detector, X)\ntest_scores = transform(detector, result.model, X)\n```\n\n## References\n\n[1] Breunig, Markus M.; Kriegel, Hans-Peter; Ng, Raymond T.; Sander, Jörg (2000): LOF: Identifying Density-Based Local Outliers.\n"
":name" = "LOFDetector"
":human_name" = "lof detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:k, :metric, :algorithm, :static, :leafsize, :reorder, :parallel)`"
":hyperparameter_types" = "`(\"Integer\", \"Distances.Metric\", \"Symbol\", \"Union{Bool, Symbol}\", \"Integer\", \"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionNeighbors.KNNDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "OutlierDetectionNeighbors"
":package_license" = "MIT"
":load_path" = "OutlierDetectionNeighbors.KNNDetector"
":package_uuid" = "51249a0a-cb36-4849-8e04-30c7f8d311bb"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionNeighbors.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nKNNDetector(k=5,\n            metric=Euclidean,\n            algorithm=:kdtree,\n            leafsize=10,\n            reorder=true,\n            reduction=:maximum)\n```\n\nCalculate the anomaly score of an instance based on the distance to its k-nearest neighbors.\n\n## Parameters\n\n```\nk::Integer\n```\n\nNumber of neighbors (must be greater than 0).\n\n```\nmetric::Metric\n```\n\nThis is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric.\n\n```\nalgorithm::Symbol\n```\n\nOne of `(:kdtree, :balltree)`. In a `kdtree`, points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A *brutetree* linearly searches all points in a brute force fashion and works with any Metric. A *balltree* recursively splits points into groups bounded by hyper-spheres and works with any Metric.\n\n```\nstatic::Union{Bool, Symbol}\n```\n\nOne of `(true, false, :auto)`. Whether the input data for fitting and transform should be statically or dynamically allocated. If `true`, the data is statically allocated. If `false`, the data is dynamically allocated. If `:auto`, the data is dynamically allocated if the product of all dimensions except the last is greater than 100.\n\n```\nleafsize::Int\n```\n\nDetermines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points.\n\n```\nreorder::Bool\n```\n\nWhile building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true.\n\n```\nparallel::Bool\n```\n\nParallelize `score` and `predict` using all threads available. The number of threads can be set with the `JULIA_NUM_THREADS` environment variable. Note: `fit` is not parallel.\n\n```\nreduction::Symbol\n```\n\nOne of `(:maximum, :median, :mean)`. (`reduction=:maximum`) was proposed by [1]. Angiulli et al. [2] proposed sum to reduce the distances, but mean has been implemented for numerical stability.\n\n## Examples\n\n```julia\nusing OutlierDetection: KNNDetector, fit, score\ndetector = KNNDetector()\nX = rand(10, 100)\nresult = fit(detector, X)\ntest_scores = transform(detector, result.model, X)\n```\n\n## References\n\n[1] Ramaswamy, Sridhar; Rastogi, Rajeev; Shim, Kyuseok (2000): Efficient Algorithms for Mining Outliers from Large Data Sets.\n\n[2] Angiulli, Fabrizio; Pizzuti, Clara (2002): Fast Outlier Detection in High Dimensional Spaces.\n"
":name" = "KNNDetector"
":human_name" = "knn detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:k, :metric, :algorithm, :static, :leafsize, :reorder, :parallel, :reduction)`"
":hyperparameter_types" = "`(\"Integer\", \"Distances.Metric\", \"Symbol\", \"Union{Bool, Symbol}\", \"Integer\", \"Bool\", \"Bool\", \"Symbol\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionNeighbors.COFDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "OutlierDetectionNeighbors"
":package_license" = "MIT"
":load_path" = "OutlierDetectionNeighbors.COFDetector"
":package_uuid" = "51249a0a-cb36-4849-8e04-30c7f8d311bb"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionNeighbors.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nCOFDetector(k = 5,\n            metric = Euclidean(),\n            algorithm = :kdtree,\n            leafsize = 10,\n            reorder = true,\n            parallel = false)\n```\n\nLocal outlier density based on chaining distance between graphs of neighbors, as described in [1].\n\n## Parameters\n\n```\nk::Integer\n```\n\nNumber of neighbors (must be greater than 0).\n\n```\nmetric::Metric\n```\n\nThis is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric.\n\n```\nalgorithm::Symbol\n```\n\nOne of `(:kdtree, :balltree)`. In a `kdtree`, points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A *brutetree* linearly searches all points in a brute force fashion and works with any Metric. A *balltree* recursively splits points into groups bounded by hyper-spheres and works with any Metric.\n\n```\nstatic::Union{Bool, Symbol}\n```\n\nOne of `(true, false, :auto)`. Whether the input data for fitting and transform should be statically or dynamically allocated. If `true`, the data is statically allocated. If `false`, the data is dynamically allocated. If `:auto`, the data is dynamically allocated if the product of all dimensions except the last is greater than 100.\n\n```\nleafsize::Int\n```\n\nDetermines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points.\n\n```\nreorder::Bool\n```\n\nWhile building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true.\n\n```\nparallel::Bool\n```\n\nParallelize `score` and `predict` using all threads available. The number of threads can be set with the `JULIA_NUM_THREADS` environment variable. Note: `fit` is not parallel.\n\n## Examples\n\n```julia\nusing OutlierDetection: COFDetector, fit, score\ndetector = COFDetector()\nX = rand(10, 100)\nresult = fit(detector, X)\ntest_scores = transform(detector, result.model, X)\n```\n\n## References\n\n[1] Tang, Jian; Chen, Zhixiang; Fu, Ada Wai-Chee; Cheung, David Wai-Lok (2002): Enhancing Effectiveness of Outlier Detections for Low Density Patterns.\n"
":name" = "COFDetector"
":human_name" = "cof detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:k, :metric, :algorithm, :static, :leafsize, :reorder, :parallel)`"
":hyperparameter_types" = "`(\"Integer\", \"Distances.Metric\", \"Symbol\", \"Union{Bool, Symbol}\", \"Integer\", \"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[PartialLeastSquaresRegressor.KPLSRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "PartialLeastSquaresRegressor"
":package_license" = "MIT"
":load_path" = "PartialLeastSquaresRegressor.KPLSRegressor"
":package_uuid" = "f4b1acfe-f311-436c-bb79-8483f53c17d5"
":package_url" = "https://github.com/lalvim/PartialLeastSquaresRegressor.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "A Kernel Partial Least Squares Regressor. A Kernel PLS2 NIPALS algorithms. Can be used mainly for regression."
":name" = "KPLSRegressor"
":human_name" = "kpls regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":predict"]
":hyperparameters" = "`(:n_factors, :kernel, :width)`"
":hyperparameter_types" = "`(\"Integer\", \"String\", \"Real\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[PartialLeastSquaresRegressor.PLSRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "PartialLeastSquaresRegressor"
":package_license" = "MIT"
":load_path" = "PartialLeastSquaresRegressor.PLSRegressor"
":package_uuid" = "f4b1acfe-f311-436c-bb79-8483f53c17d5"
":package_url" = "https://github.com/lalvim/PartialLeastSquaresRegressor.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "A Partial Least Squares Regressor. Contains PLS1, PLS2 (multi target) algorithms. Can be used mainly for regression."
":name" = "PLSRegressor"
":human_name" = "pls regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":predict"]
":hyperparameters" = "`(:n_factors,)`"
":hyperparameter_types" = "`(\"Int64\",)`"
":hyperparameter_ranges" = "`(nothing,)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJLinearModels.QuantileRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJLinearModels"
":package_license" = "MIT"
":load_path" = "MLJLinearModels.QuantileRegressor"
":package_uuid" = "6ee0df7b-362f-4a72-a706-9e79364fb692"
":package_url" = "https://github.com/alan-turing-institute/MLJLinearModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Robust regression with objective ``∑ρ(Xθ - y) + λ|θ|₂² + γ|θ|₁`` where `ρ` is the Quantile Loss.\n→ based on [MLJLinearModels](https://github.com/alan-turing-institute/MLJLinearModels.jl)\n→ do `@load QuantileRegressor pkg=\"MLJLinearModels\" to use the model.`\n→ do `?QuantileRegressor` for documentation."
":name" = "QuantileRegressor"
":human_name" = "quantile regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":fitted_params", ":predict", ":QuantileRegressor"]
":hyperparameters" = "`(:delta, :lambda, :gamma, :penalty, :fit_intercept, :penalize_intercept, :scale_penalty_with_samples, :solver)`"
":hyperparameter_types" = "`(\"Real\", \"Real\", \"Real\", \"Union{String, Symbol}\", \"Bool\", \"Bool\", \"Bool\", \"Union{Nothing, MLJLinearModels.Solver}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJLinearModels.LogisticClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJLinearModels"
":package_license" = "MIT"
":load_path" = "MLJLinearModels.LogisticClassifier"
":package_uuid" = "6ee0df7b-362f-4a72-a706-9e79364fb692"
":package_url" = "https://github.com/alan-turing-institute/MLJLinearModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Classifier corresponding to the loss function ``L(y, Xθ) + λ|θ|₂²/2 + γ|θ|₁`` where `L` is the logistic loss.\n→ based on [MLJLinearModels](https://github.com/alan-turing-institute/MLJLinearModels.jl)\n→ do `@load LogisticClassifier pkg=\"MLJLinearModels\" to use the model.`\n→ do `?LogisticClassifier` for documentation."
":name" = "LogisticClassifier"
":human_name" = "logistic classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":fit", ":fitted_params", ":predict", ":LogisticClassifier"]
":hyperparameters" = "`(:lambda, :gamma, :penalty, :fit_intercept, :penalize_intercept, :scale_penalty_with_samples, :solver)`"
":hyperparameter_types" = "`(\"Real\", \"Real\", \"Union{String, Symbol}\", \"Bool\", \"Bool\", \"Bool\", \"Union{Nothing, MLJLinearModels.Solver}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJLinearModels.MultinomialClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJLinearModels"
":package_license" = "MIT"
":load_path" = "MLJLinearModels.MultinomialClassifier"
":package_uuid" = "6ee0df7b-362f-4a72-a706-9e79364fb692"
":package_url" = "https://github.com/alan-turing-institute/MLJLinearModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Classifier corresponding to the loss function ``L(y, Xθ) + λ|θ|₂²/2 + γ|θ|₁`` where `L` is the multinomial loss.\n→ based on [MLJLinearModels](https://github.com/alan-turing-institute/MLJLinearModels.jl)\n→ do `@load MultinomialClassifier pkg=\"MLJLinearModels\" to use the model.`\n→ do `?MultinomialClassifier` for documentation."
":name" = "MultinomialClassifier"
":human_name" = "multinomial classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":fit", ":fitted_params", ":predict", ":MultinomialClassifier"]
":hyperparameters" = "`(:lambda, :gamma, :penalty, :fit_intercept, :penalize_intercept, :scale_penalty_with_samples, :solver)`"
":hyperparameter_types" = "`(\"Real\", \"Real\", \"Union{String, Symbol}\", \"Bool\", \"Bool\", \"Bool\", \"Union{Nothing, MLJLinearModels.Solver}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJLinearModels.LADRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJLinearModels"
":package_license" = "MIT"
":load_path" = "MLJLinearModels.LADRegressor"
":package_uuid" = "6ee0df7b-362f-4a72-a706-9e79364fb692"
":package_url" = "https://github.com/alan-turing-institute/MLJLinearModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Robust regression with objective ``∑ρ(Xθ - y) + λ|θ|₂² + γ|θ|₁`` where `ρ` is the Absolute Loss.\n→ based on [MLJLinearModels](https://github.com/alan-turing-institute/MLJLinearModels.jl)\n→ do `@load LADRegressor pkg=\"MLJLinearModels\" to use the model.`\n→ do `?LADRegressor` for documentation."
":name" = "LADRegressor"
":human_name" = "lad regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":fitted_params", ":predict", ":LADRegressor"]
":hyperparameters" = "`(:lambda, :gamma, :penalty, :fit_intercept, :penalize_intercept, :scale_penalty_with_samples, :solver)`"
":hyperparameter_types" = "`(\"Real\", \"Real\", \"Union{String, Symbol}\", \"Bool\", \"Bool\", \"Bool\", \"Union{Nothing, MLJLinearModels.Solver}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJLinearModels.RidgeRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJLinearModels"
":package_license" = "MIT"
":load_path" = "MLJLinearModels.RidgeRegressor"
":package_uuid" = "6ee0df7b-362f-4a72-a706-9e79364fb692"
":package_url" = "https://github.com/alan-turing-institute/MLJLinearModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Regression with objective function ``|Xθ - y|₂²/2 + λ|θ|₂²/2``.\n→ based on [MLJLinearModels](https://github.com/alan-turing-institute/MLJLinearModels.jl)\n→ do `@load RidgeRegressor pkg=\"MLJLinearModels\" to use the model.`\n→ do `?RidgeRegressor` for documentation."
":name" = "RidgeRegressor"
":human_name" = "ridge regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":fitted_params", ":predict", ":RidgeRegressor"]
":hyperparameters" = "`(:lambda, :fit_intercept, :penalize_intercept, :scale_penalty_with_samples, :solver)`"
":hyperparameter_types" = "`(\"Real\", \"Bool\", \"Bool\", \"Bool\", \"Union{Nothing, MLJLinearModels.Solver}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJLinearModels.RobustRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJLinearModels"
":package_license" = "MIT"
":load_path" = "MLJLinearModels.RobustRegressor"
":package_uuid" = "6ee0df7b-362f-4a72-a706-9e79364fb692"
":package_url" = "https://github.com/alan-turing-institute/MLJLinearModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Robust regression with objective ``∑ρ(Xθ - y) + λ|θ|₂² + γ|θ|₁`` for a given robust `ρ`.\n→ based on [MLJLinearModels](https://github.com/alan-turing-institute/MLJLinearModels.jl)\n→ do `@load RobustRegressor pkg=\"MLJLinearModels\" to use the model.`\n→ do `?RobustRegressor` for documentation."
":name" = "RobustRegressor"
":human_name" = "robust regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":fitted_params", ":predict", ":RobustRegressor"]
":hyperparameters" = "`(:rho, :lambda, :gamma, :penalty, :fit_intercept, :penalize_intercept, :scale_penalty_with_samples, :solver)`"
":hyperparameter_types" = "`(\"MLJLinearModels.RobustRho\", \"Real\", \"Real\", \"Union{String, Symbol}\", \"Bool\", \"Bool\", \"Bool\", \"Union{Nothing, MLJLinearModels.Solver}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJLinearModels.ElasticNetRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJLinearModels"
":package_license" = "MIT"
":load_path" = "MLJLinearModels.ElasticNetRegressor"
":package_uuid" = "6ee0df7b-362f-4a72-a706-9e79364fb692"
":package_url" = "https://github.com/alan-turing-institute/MLJLinearModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Regression with objective function ``|Xθ - y|₂²/2 + λ|θ|₂²/2 + γ|θ|₁``.\n→ based on [MLJLinearModels](https://github.com/alan-turing-institute/MLJLinearModels.jl)\n→ do `@load ElasticNetRegressor pkg=\"MLJLinearModels\" to use the model.`\n→ do `?ElasticNetRegressor` for documentation."
":name" = "ElasticNetRegressor"
":human_name" = "elastic net regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":fitted_params", ":predict", ":ElasticNetRegressor"]
":hyperparameters" = "`(:lambda, :gamma, :fit_intercept, :penalize_intercept, :scale_penalty_with_samples, :solver)`"
":hyperparameter_types" = "`(\"Real\", \"Real\", \"Bool\", \"Bool\", \"Bool\", \"Union{Nothing, MLJLinearModels.Solver}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJLinearModels.LinearRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJLinearModels"
":package_license" = "MIT"
":load_path" = "MLJLinearModels.LinearRegressor"
":package_uuid" = "6ee0df7b-362f-4a72-a706-9e79364fb692"
":package_url" = "https://github.com/alan-turing-institute/MLJLinearModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Regression with objective function ``|Xθ - y|₂²/2``.\n→ based on [MLJLinearModels](https://github.com/alan-turing-institute/MLJLinearModels.jl)\n→ do `@load LinearRegressor pkg=\"MLJLinearModels\" to use the model.`\n→ do `?LinearRegressor` for documentation."
":name" = "LinearRegressor"
":human_name" = "linear regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":fitted_params", ":predict", ":LinearRegressor"]
":hyperparameters" = "`(:fit_intercept, :solver)`"
":hyperparameter_types" = "`(\"Bool\", \"Union{Nothing, MLJLinearModels.Solver}\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJLinearModels.LassoRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJLinearModels"
":package_license" = "MIT"
":load_path" = "MLJLinearModels.LassoRegressor"
":package_uuid" = "6ee0df7b-362f-4a72-a706-9e79364fb692"
":package_url" = "https://github.com/alan-turing-institute/MLJLinearModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Regression with objective function ``|Xθ - y|₂²/2 + λ|θ|₁``.\n→ based on [MLJLinearModels](https://github.com/alan-turing-institute/MLJLinearModels.jl)\n→ do `@load LassoRegressor pkg=\"MLJLinearModels\" to use the model.`\n→ do `?LassoRegressor` for documentation."
":name" = "LassoRegressor"
":human_name" = "lasso regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":fitted_params", ":predict", ":LassoRegressor"]
":hyperparameters" = "`(:lambda, :fit_intercept, :penalize_intercept, :scale_penalty_with_samples, :solver)`"
":hyperparameter_types" = "`(\"Real\", \"Bool\", \"Bool\", \"Bool\", \"Union{Nothing, MLJLinearModels.Solver}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJLinearModels.HuberRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJLinearModels"
":package_license" = "MIT"
":load_path" = "MLJLinearModels.HuberRegressor"
":package_uuid" = "6ee0df7b-362f-4a72-a706-9e79364fb692"
":package_url" = "https://github.com/alan-turing-institute/MLJLinearModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Robust regression with objective ``∑ρ(Xθ - y) + λ|θ|₂² + γ|θ|₁`` where `ρ` is the Huber Loss.\n→ based on [MLJLinearModels](https://github.com/alan-turing-institute/MLJLinearModels.jl)\n→ do `@load HuberRegressor pkg=\"MLJLinearModels\" to use the model.`\n→ do `?HuberRegressor` for documentation."
":name" = "HuberRegressor"
":human_name" = "huber regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":fitted_params", ":predict", ":HuberRegressor"]
":hyperparameters" = "`(:delta, :lambda, :gamma, :penalty, :fit_intercept, :penalize_intercept, :scale_penalty_with_samples, :solver)`"
":hyperparameter_types" = "`(\"Real\", \"Real\", \"Real\", \"Union{String, Symbol}\", \"Bool\", \"Bool\", \"Bool\", \"Union{Nothing, MLJLinearModels.Solver}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.ProbabilisticSGDClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.ProbabilisticSGDClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nProbabilisticSGDClassifier\n```\n\nA model type for constructing a probabilistic sgd classifier, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nProbabilisticSGDClassifier = @load ProbabilisticSGDClassifier pkg=ScikitLearn\n```\n\nDo `model = ProbabilisticSGDClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`ProbabilisticSGDClassifier(loss=...)`.\n# Hyper-parameters\n\n- `loss = log`\n\n- `penalty = l2`\n\n- `alpha = 0.0001`\n\n- `l1_ratio = 0.15`\n\n- `fit_intercept = true`\n\n- `max_iter = 1000`\n\n- `tol = 0.001`\n\n- `shuffle = true`\n\n- `verbose = 0`\n\n- `epsilon = 0.1`\n\n- `n_jobs = nothing`\n\n- `random_state = nothing`\n\n- `learning_rate = optimal`\n\n- `eta0 = 0.0`\n\n- `power_t = 0.5`\n\n- `early_stopping = false`\n\n- `validation_fraction = 0.1`\n\n- `n_iter_no_change = 5`\n\n- `class_weight = nothing`\n\n- `warm_start = false`\n\n- `average = false`\n\n"
":name" = "ProbabilisticSGDClassifier"
":human_name" = "probabilistic sgd classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:loss, :penalty, :alpha, :l1_ratio, :fit_intercept, :max_iter, :tol, :shuffle, :verbose, :epsilon, :n_jobs, :random_state, :learning_rate, :eta0, :power_t, :early_stopping, :validation_fraction, :n_iter_no_change, :class_weight, :warm_start, :average)`"
":hyperparameter_types" = "`(\"String\", \"String\", \"Float64\", \"Float64\", \"Bool\", \"Int64\", \"Union{Nothing, Float64}\", \"Bool\", \"Int64\", \"Float64\", \"Union{Nothing, Int64}\", \"Any\", \"String\", \"Float64\", \"Float64\", \"Bool\", \"Float64\", \"Int64\", \"Any\", \"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.RidgeCVClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.RidgeCVClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nRidgeCVClassifier\n```\n\nA model type for constructing a ridge regression classifier with built-in cross-validation, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nRidgeCVClassifier = @load RidgeCVClassifier pkg=ScikitLearn\n```\n\nDo `model = RidgeCVClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`RidgeCVClassifier(alphas=...)`.\n# Hyper-parameters\n\n- `alphas = [0.1, 1.0, 10.0]`\n\n- `fit_intercept = true`\n\n- `normalize = false`\n\n- `scoring = nothing`\n\n- `cv = 5`\n\n- `class_weight = nothing`\n\n- `store_cv_values = false`\n\n"
":name" = "RidgeCVClassifier"
":human_name" = "ridge regression classifier with built-in cross-validation"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:alphas, :fit_intercept, :normalize, :scoring, :cv, :class_weight, :store_cv_values)`"
":hyperparameter_types" = "`(\"AbstractArray{Float64}\", \"Bool\", \"Bool\", \"Any\", \"Int64\", \"Any\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.LogisticClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.LogisticClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLogisticClassifier\n```\n\nA model type for constructing a logistic regression classifier, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nLogisticClassifier = @load LogisticClassifier pkg=ScikitLearn\n```\n\nDo `model = LogisticClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LogisticClassifier(penalty=...)`.\n# Hyper-parameters\n\n- `penalty = l2`\n\n- `dual = false`\n\n- `tol = 0.0001`\n\n- `C = 1.0`\n\n- `fit_intercept = true`\n\n- `intercept_scaling = 1.0`\n\n- `class_weight = nothing`\n\n- `random_state = nothing`\n\n- `solver = lbfgs`\n\n- `max_iter = 100`\n\n- `multi_class = auto`\n\n- `verbose = 0`\n\n- `warm_start = false`\n\n- `n_jobs = nothing`\n\n- `l1_ratio = nothing`\n\n"
":name" = "LogisticClassifier"
":human_name" = "logistic regression classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:penalty, :dual, :tol, :C, :fit_intercept, :intercept_scaling, :class_weight, :random_state, :solver, :max_iter, :multi_class, :verbose, :warm_start, :n_jobs, :l1_ratio)`"
":hyperparameter_types" = "`(\"String\", \"Bool\", \"Float64\", \"Float64\", \"Bool\", \"Float64\", \"Any\", \"Any\", \"String\", \"Int64\", \"String\", \"Int64\", \"Bool\", \"Union{Nothing, Int64}\", \"Union{Nothing, Float64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.RandomForestRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.Continuous}}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.RandomForestRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nRandomForestRegressor\n```\n\nA model type for constructing a random forest regressor, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nRandomForestRegressor = @load RandomForestRegressor pkg=ScikitLearn\n```\n\nDo `model = RandomForestRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`RandomForestRegressor(n_estimators=...)`.\n# Hyper-parameters\n\n- `n_estimators = 100`\n\n- `criterion = mse`\n\n- `max_depth = nothing`\n\n- `min_samples_split = 2`\n\n- `min_samples_leaf = 1`\n\n- `min_weight_fraction_leaf = 0.0`\n\n- `max_features = auto`\n\n- `max_leaf_nodes = nothing`\n\n- `min_impurity_decrease = 0.0`\n\n- `bootstrap = true`\n\n- `oob_score = false`\n\n- `n_jobs = nothing`\n\n- `random_state = nothing`\n\n- `verbose = 0`\n\n- `warm_start = false`\n\n- `ccp_alpha = 0.0`\n\n- `max_samples = nothing`\n\n"
":name" = "RandomForestRegressor"
":human_name" = "random forest regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:n_estimators, :criterion, :max_depth, :min_samples_split, :min_samples_leaf, :min_weight_fraction_leaf, :max_features, :max_leaf_nodes, :min_impurity_decrease, :bootstrap, :oob_score, :n_jobs, :random_state, :verbose, :warm_start, :ccp_alpha, :max_samples)`"
":hyperparameter_types" = "`(\"Int64\", \"String\", \"Union{Nothing, Int64}\", \"Union{Float64, Int64}\", \"Union{Float64, Int64}\", \"Float64\", \"Union{Nothing, Float64, Int64, String}\", \"Union{Nothing, Int64}\", \"Float64\", \"Bool\", \"Bool\", \"Union{Nothing, Int64}\", \"Any\", \"Int64\", \"Bool\", \"Float64\", \"Union{Nothing, Float64, Int64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.ElasticNetCVRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.ElasticNetCVRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nElasticNetCVRegressor\n```\n\nA model type for constructing a elastic net regression with built-in cross-validation, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nElasticNetCVRegressor = @load ElasticNetCVRegressor pkg=ScikitLearn\n```\n\nDo `model = ElasticNetCVRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`ElasticNetCVRegressor(l1_ratio=...)`.\n# Hyper-parameters\n\n- `l1_ratio = 0.5`\n\n- `eps = 0.001`\n\n- `n_alphas = 100`\n\n- `alphas = nothing`\n\n- `fit_intercept = true`\n\n- `normalize = false`\n\n- `precompute = auto`\n\n- `max_iter = 1000`\n\n- `tol = 0.0001`\n\n- `cv = 5`\n\n- `copy_X = true`\n\n- `verbose = 0`\n\n- `n_jobs = nothing`\n\n- `positive = false`\n\n- `random_state = nothing`\n\n- `selection = cyclic`\n\n"
":name" = "ElasticNetCVRegressor"
":human_name" = "elastic net regression with built-in cross-validation"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:l1_ratio, :eps, :n_alphas, :alphas, :fit_intercept, :normalize, :precompute, :max_iter, :tol, :cv, :copy_X, :verbose, :n_jobs, :positive, :random_state, :selection)`"
":hyperparameter_types" = "`(\"Union{Float64, Vector{Float64}}\", \"Float64\", \"Int64\", \"Any\", \"Bool\", \"Bool\", \"Union{Bool, String, AbstractMatrix}\", \"Int64\", \"Float64\", \"Any\", \"Bool\", \"Union{Bool, Int64}\", \"Union{Nothing, Int64}\", \"Bool\", \"Any\", \"String\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.PerceptronClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.PerceptronClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nPerceptronClassifier\n```\n\nA model type for constructing a perceptron classifier, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nPerceptronClassifier = @load PerceptronClassifier pkg=ScikitLearn\n```\n\nDo `model = PerceptronClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`PerceptronClassifier(penalty=...)`.\n# Hyper-parameters\n\n- `penalty = nothing`\n\n- `alpha = 0.0001`\n\n- `fit_intercept = true`\n\n- `max_iter = 1000`\n\n- `tol = 0.001`\n\n- `shuffle = true`\n\n- `verbose = 0`\n\n- `eta0 = 1.0`\n\n- `n_jobs = nothing`\n\n- `random_state = 0`\n\n- `early_stopping = false`\n\n- `validation_fraction = 0.1`\n\n- `n_iter_no_change = 5`\n\n- `class_weight = nothing`\n\n- `warm_start = false`\n\n"
":name" = "PerceptronClassifier"
":human_name" = "perceptron classifier"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:penalty, :alpha, :fit_intercept, :max_iter, :tol, :shuffle, :verbose, :eta0, :n_jobs, :random_state, :early_stopping, :validation_fraction, :n_iter_no_change, :class_weight, :warm_start)`"
":hyperparameter_types" = "`(\"Union{Nothing, String}\", \"Float64\", \"Bool\", \"Int64\", \"Union{Nothing, Float64}\", \"Bool\", \"Int64\", \"Float64\", \"Union{Nothing, Int64}\", \"Any\", \"Bool\", \"Float64\", \"Int64\", \"Any\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.MultiTaskLassoRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.MultiTaskLassoRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nMultiTaskLassoRegressor\n```\n\nA model type for constructing a multi-target lasso regressor, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nMultiTaskLassoRegressor = @load MultiTaskLassoRegressor pkg=ScikitLearn\n```\n\nDo `model = MultiTaskLassoRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`MultiTaskLassoRegressor(alpha=...)`.\n# Hyper-parameters\n\n- `alpha = 1.0`\n\n- `fit_intercept = true`\n\n- `normalize = false`\n\n- `max_iter = 1000`\n\n- `tol = 0.0001`\n\n- `copy_X = true`\n\n- `random_state = nothing`\n\n- `selection = cyclic`\n\n"
":name" = "MultiTaskLassoRegressor"
":human_name" = "multi-target lasso regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:alpha, :fit_intercept, :normalize, :max_iter, :tol, :copy_X, :random_state, :selection)`"
":hyperparameter_types" = "`(\"Float64\", \"Bool\", \"Bool\", \"Int64\", \"Float64\", \"Bool\", \"Any\", \"String\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.LinearRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.LinearRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLinearRegressor\n```\n\nA model type for constructing a ordinary least-squares regressor (OLS), based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nLinearRegressor = @load LinearRegressor pkg=ScikitLearn\n```\n\nDo `model = LinearRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LinearRegressor(fit_intercept=...)`.\n# Hyper-parameters\n\n- `fit_intercept = true`\n\n- `normalize = false`\n\n- `copy_X = true`\n\n- `n_jobs = nothing`\n\n"
":name" = "LinearRegressor"
":human_name" = "ordinary least-squares regressor (OLS)"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:fit_intercept, :normalize, :copy_X, :n_jobs)`"
":hyperparameter_types" = "`(\"Bool\", \"Bool\", \"Bool\", \"Union{Nothing, Int64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.DBSCAN]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.DBSCAN"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nDBSCAN\n```\n\nA model type for constructing a dbscan, based on [ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nDBSCAN = @load DBSCAN pkg=ScikitLearn\n```\n\nDo `model = DBSCAN()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `DBSCAN(eps=...)`.\n\nDensity-Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density.\n"
":name" = "DBSCAN"
":human_name" = "dbscan"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params"]
":hyperparameters" = "`(:eps, :min_samples, :metric, :algorithm, :leaf_size, :p, :n_jobs)`"
":hyperparameter_types" = "`(\"Float64\", \"Int64\", \"String\", \"String\", \"Int64\", \"Union{Nothing, Float64}\", \"Union{Nothing, Int64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.RidgeRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.RidgeRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nRidgeRegressor\n```\n\nA model type for constructing a ridge regressor, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nRidgeRegressor = @load RidgeRegressor pkg=ScikitLearn\n```\n\nDo `model = RidgeRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`RidgeRegressor(alpha=...)`.\n# Hyper-parameters\n\n- `alpha = 1.0`\n\n- `fit_intercept = true`\n\n- `normalize = false`\n\n- `copy_X = true`\n\n- `max_iter = 1000`\n\n- `tol = 0.0001`\n\n- `solver = auto`\n\n- `random_state = nothing`\n\n"
":name" = "RidgeRegressor"
":human_name" = "ridge regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:alpha, :fit_intercept, :normalize, :copy_X, :max_iter, :tol, :solver, :random_state)`"
":hyperparameter_types" = "`(\"Union{Float64, Vector{Float64}}\", \"Bool\", \"Bool\", \"Bool\", \"Int64\", \"Float64\", \"String\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.LassoLarsICRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.LassoLarsICRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLassoLarsICRegressor\n```\n\nA model type for constructing a Lasso model with LARS using BIC or AIC for model selection, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nLassoLarsICRegressor = @load LassoLarsICRegressor pkg=ScikitLearn\n```\n\nDo `model = LassoLarsICRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LassoLarsICRegressor(criterion=...)`.\n# Hyper-parameters\n\n- `criterion = aic`\n\n- `fit_intercept = true`\n\n- `verbose = false`\n\n- `normalize = true`\n\n- `precompute = auto`\n\n- `max_iter = 500`\n\n- `eps = 2.220446049250313e-16`\n\n- `copy_X = true`\n\n- `positive = false`\n\n"
":name" = "LassoLarsICRegressor"
":human_name" = "Lasso model with LARS using BIC or AIC for model selection"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:criterion, :fit_intercept, :verbose, :normalize, :precompute, :max_iter, :eps, :copy_X, :positive)`"
":hyperparameter_types" = "`(\"String\", \"Bool\", \"Union{Bool, Int64}\", \"Bool\", \"Union{Bool, String, AbstractMatrix}\", \"Int64\", \"Float64\", \"Bool\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.ARDRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.ARDRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nARDRegressor\n```\n\nA model type for constructing a Bayesian ARD regressor, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nARDRegressor = @load ARDRegressor pkg=ScikitLearn\n```\n\nDo `model = ARDRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`ARDRegressor(n_iter=...)`.\n# Hyper-parameters\n\n- `n_iter = 300`\n\n- `tol = 0.001`\n\n- `alpha_1 = 1.0e-6`\n\n- `alpha_2 = 1.0e-6`\n\n- `lambda_1 = 1.0e-6`\n\n- `lambda_2 = 1.0e-6`\n\n- `compute_score = false`\n\n- `threshold_lambda = 10000.0`\n\n- `fit_intercept = true`\n\n- `normalize = false`\n\n- `copy_X = true`\n\n- `verbose = false`\n\n"
":name" = "ARDRegressor"
":human_name" = "Bayesian ARD regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:n_iter, :tol, :alpha_1, :alpha_2, :lambda_1, :lambda_2, :compute_score, :threshold_lambda, :fit_intercept, :normalize, :copy_X, :verbose)`"
":hyperparameter_types" = "`(\"Int64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Bool\", \"Float64\", \"Bool\", \"Bool\", \"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.SVMNuRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.SVMNuRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nSVMNuRegressor\n```\n\nA model type for constructing a nu-support vector regressor, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nSVMNuRegressor = @load SVMNuRegressor pkg=ScikitLearn\n```\n\nDo `model = SVMNuRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`SVMNuRegressor(nu=...)`.\n# Hyper-parameters\n\n- `nu = 0.5`\n\n- `C = 1.0`\n\n- `kernel = rbf`\n\n- `degree = 3`\n\n- `gamma = auto`\n\n- `coef0 = 0.0`\n\n- `shrinking = true`\n\n- `tol = 0.001`\n\n- `cache_size = 200`\n\n- `max_iter = -1`\n\n"
":name" = "SVMNuRegressor"
":human_name" = "nu-support vector regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:nu, :C, :kernel, :degree, :gamma, :coef0, :shrinking, :tol, :cache_size, :max_iter)`"
":hyperparameter_types" = "`(\"Float64\", \"Float64\", \"Union{Function, String}\", \"Int64\", \"Union{Float64, String}\", \"Float64\", \"Any\", \"Float64\", \"Int64\", \"Int64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.RidgeClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.RidgeClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nRidgeClassifier\n```\n\nA model type for constructing a ridge regression classifier, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nRidgeClassifier = @load RidgeClassifier pkg=ScikitLearn\n```\n\nDo `model = RidgeClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`RidgeClassifier(alpha=...)`.\n# Hyper-parameters\n\n- `alpha = 1.0`\n\n- `fit_intercept = true`\n\n- `normalize = false`\n\n- `copy_X = true`\n\n- `max_iter = nothing`\n\n- `tol = 0.001`\n\n- `class_weight = nothing`\n\n- `solver = auto`\n\n- `random_state = nothing`\n\n"
":name" = "RidgeClassifier"
":human_name" = "ridge regression classifier"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:alpha, :fit_intercept, :normalize, :copy_X, :max_iter, :tol, :class_weight, :solver, :random_state)`"
":hyperparameter_types" = "`(\"Float64\", \"Bool\", \"Bool\", \"Bool\", \"Union{Nothing, Int64}\", \"Float64\", \"Any\", \"String\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.SGDRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.SGDRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nSGDRegressor\n```\n\nA model type for constructing a stochastic gradient descent-based regressor, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nSGDRegressor = @load SGDRegressor pkg=ScikitLearn\n```\n\nDo `model = SGDRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`SGDRegressor(loss=...)`.\n# Hyper-parameters\n\n- `loss = squared_loss`\n\n- `penalty = l2`\n\n- `alpha = 0.0001`\n\n- `l1_ratio = 0.15`\n\n- `fit_intercept = true`\n\n- `max_iter = 1000`\n\n- `tol = 0.001`\n\n- `shuffle = true`\n\n- `verbose = 0`\n\n- `epsilon = 0.1`\n\n- `random_state = nothing`\n\n- `learning_rate = invscaling`\n\n- `eta0 = 0.01`\n\n- `power_t = 0.25`\n\n- `early_stopping = false`\n\n- `validation_fraction = 0.1`\n\n- `n_iter_no_change = 5`\n\n- `warm_start = false`\n\n- `average = false`\n\n"
":name" = "SGDRegressor"
":human_name" = "stochastic gradient descent-based regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:loss, :penalty, :alpha, :l1_ratio, :fit_intercept, :max_iter, :tol, :shuffle, :verbose, :epsilon, :random_state, :learning_rate, :eta0, :power_t, :early_stopping, :validation_fraction, :n_iter_no_change, :warm_start, :average)`"
":hyperparameter_types" = "`(\"String\", \"String\", \"Float64\", \"Float64\", \"Bool\", \"Int64\", \"Float64\", \"Bool\", \"Union{Bool, Int64}\", \"Float64\", \"Any\", \"String\", \"Float64\", \"Float64\", \"Bool\", \"Float64\", \"Int64\", \"Bool\", \"Union{Bool, Int64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.ComplementNBClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Count}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Count}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.ComplementNBClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nComplementNBClassifier\n```\n\nA model type for constructing a Complement naive Bayes classifier, based on [ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nComplementNBClassifier = @load ComplementNBClassifier pkg=ScikitLearn\n```\n\nDo `model = ComplementNBClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `ComplementNBClassifier(alpha=...)`.\n\nSimilar to [`MultinomialNBClassifier`](@ref) but with more robust assumptions. Suited for imbalanced datasets.\n"
":name" = "ComplementNBClassifier"
":human_name" = "Complement naive Bayes classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:alpha, :fit_prior, :class_prior, :norm)`"
":hyperparameter_types" = "`(\"Float64\", \"Bool\", \"Union{Nothing, AbstractVector}\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.HuberRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.HuberRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nHuberRegressor\n```\n\nA model type for constructing a Huber regressor, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nHuberRegressor = @load HuberRegressor pkg=ScikitLearn\n```\n\nDo `model = HuberRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`HuberRegressor(epsilon=...)`.\n# Hyper-parameters\n\n- `epsilon = 1.35`\n\n- `max_iter = 100`\n\n- `alpha = 0.0001`\n\n- `warm_start = false`\n\n- `fit_intercept = true`\n\n- `tol = 1.0e-5`\n\n"
":name" = "HuberRegressor"
":human_name" = "Huber regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:epsilon, :max_iter, :alpha, :warm_start, :fit_intercept, :tol)`"
":hyperparameter_types" = "`(\"Float64\", \"Int64\", \"Float64\", \"Bool\", \"Bool\", \"Float64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.SVMNuClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.SVMNuClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nSVMNuClassifier\n```\n\nA model type for constructing a nu-support vector classifier, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nSVMNuClassifier = @load SVMNuClassifier pkg=ScikitLearn\n```\n\nDo `model = SVMNuClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`SVMNuClassifier(nu=...)`.\n# Hyper-parameters\n\n- `nu = 0.5`\n\n- `kernel = rbf`\n\n- `degree = 3`\n\n- `gamma = auto`\n\n- `coef0 = 0.0`\n\n- `shrinking = true`\n\n- `tol = 0.001`\n\n- `cache_size = 200`\n\n- `max_iter = -1`\n\n- `decision_function_shape = ovr`\n\n- `random_state = nothing`\n\n"
":name" = "SVMNuClassifier"
":human_name" = "nu-support vector classifier"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:nu, :kernel, :degree, :gamma, :coef0, :shrinking, :tol, :cache_size, :max_iter, :decision_function_shape, :random_state)`"
":hyperparameter_types" = "`(\"Float64\", \"Union{Function, String}\", \"Int64\", \"Union{Float64, String}\", \"Float64\", \"Bool\", \"Float64\", \"Int64\", \"Int64\", \"String\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.GradientBoostingClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.GradientBoostingClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nGradientBoostingClassifier\n```\n\nA model type for constructing a gradient boosting classifier, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nGradientBoostingClassifier = @load GradientBoostingClassifier pkg=ScikitLearn\n```\n\nDo `model = GradientBoostingClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`GradientBoostingClassifier(loss=...)`.\n# Hyper-parameters\n\n- `loss = deviance`\n\n- `learning_rate = 0.1`\n\n- `n_estimators = 100`\n\n- `subsample = 1.0`\n\n- `criterion = friedman_mse`\n\n- `min_samples_split = 2`\n\n- `min_samples_leaf = 1`\n\n- `min_weight_fraction_leaf = 0.0`\n\n- `max_depth = 3`\n\n- `min_impurity_decrease = 0.0`\n\n- `init = nothing`\n\n- `random_state = nothing`\n\n- `max_features = nothing`\n\n- `verbose = 0`\n\n- `max_leaf_nodes = nothing`\n\n- `warm_start = false`\n\n- `validation_fraction = 0.1`\n\n- `n_iter_no_change = nothing`\n\n- `tol = 0.0001`\n\n"
":name" = "GradientBoostingClassifier"
":human_name" = "gradient boosting classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:loss, :learning_rate, :n_estimators, :subsample, :criterion, :min_samples_split, :min_samples_leaf, :min_weight_fraction_leaf, :max_depth, :min_impurity_decrease, :init, :random_state, :max_features, :verbose, :max_leaf_nodes, :warm_start, :validation_fraction, :n_iter_no_change, :tol)`"
":hyperparameter_types" = "`(\"String\", \"Float64\", \"Int64\", \"Float64\", \"String\", \"Union{Float64, Int64}\", \"Union{Float64, Int64}\", \"Float64\", \"Int64\", \"Float64\", \"Any\", \"Any\", \"Union{Nothing, Float64, Int64, String}\", \"Int64\", \"Union{Nothing, Int64}\", \"Bool\", \"Float64\", \"Union{Nothing, Int64}\", \"Float64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.GaussianProcessRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.GaussianProcessRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nGaussianProcessRegressor\n```\n\nA model type for constructing a Gaussian process regressor, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nGaussianProcessRegressor = @load GaussianProcessRegressor pkg=ScikitLearn\n```\n\nDo `model = GaussianProcessRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`GaussianProcessRegressor(kernel=...)`.\n# Hyper-parameters\n\n- `kernel = nothing`\n\n- `alpha = 1.0e-10`\n\n- `optimizer = fmin_l_bfgs_b`\n\n- `n_restarts_optimizer = 0`\n\n- `normalize_y = false`\n\n- `copy_X_train = true`\n\n- `random_state = nothing`\n\n"
":name" = "GaussianProcessRegressor"
":human_name" = "Gaussian process regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:kernel, :alpha, :optimizer, :n_restarts_optimizer, :normalize_y, :copy_X_train, :random_state)`"
":hyperparameter_types" = "`(\"Any\", \"Union{Float64, AbstractArray}\", \"Any\", \"Int64\", \"Bool\", \"Bool\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.SVMLinearRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.SVMLinearRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nSVMLinearRegressor\n```\n\nA model type for constructing a linear support vector regressor, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nSVMLinearRegressor = @load SVMLinearRegressor pkg=ScikitLearn\n```\n\nDo `model = SVMLinearRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`SVMLinearRegressor(epsilon=...)`.\n# Hyper-parameters\n\n- `epsilon = 0.0`\n\n- `tol = 0.0001`\n\n- `C = 1.0`\n\n- `loss = epsilon_insensitive`\n\n- `fit_intercept = true`\n\n- `intercept_scaling = 1.0`\n\n- `dual = true`\n\n- `random_state = nothing`\n\n- `max_iter = 1000`\n\n"
":name" = "SVMLinearRegressor"
":human_name" = "linear support vector regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:epsilon, :tol, :C, :loss, :fit_intercept, :intercept_scaling, :dual, :random_state, :max_iter)`"
":hyperparameter_types" = "`(\"Float64\", \"Float64\", \"Float64\", \"String\", \"Bool\", \"Float64\", \"Bool\", \"Any\", \"Int64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.LarsRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.LarsRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLarsRegressor\n```\n\nA model type for constructing a least angle regressor (LARS), based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nLarsRegressor = @load LarsRegressor pkg=ScikitLearn\n```\n\nDo `model = LarsRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LarsRegressor(fit_intercept=...)`.\n# Hyper-parameters\n\n- `fit_intercept = true`\n\n- `verbose = false`\n\n- `normalize = true`\n\n- `precompute = auto`\n\n- `n_nonzero_coefs = 500`\n\n- `eps = 2.220446049250313e-16`\n\n- `copy_X = true`\n\n- `fit_path = true`\n\n"
":name" = "LarsRegressor"
":human_name" = "least angle regressor (LARS)"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:fit_intercept, :verbose, :normalize, :precompute, :n_nonzero_coefs, :eps, :copy_X, :fit_path)`"
":hyperparameter_types" = "`(\"Bool\", \"Union{Bool, Int64}\", \"Bool\", \"Union{Bool, String, AbstractMatrix}\", \"Int64\", \"Float64\", \"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.MeanShift]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Multiclass}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.MeanShift"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nMeanShift\n```\n\nA model type for constructing a mean shift, based on [ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nMeanShift = @load MeanShift pkg=ScikitLearn\n```\n\nDo `model = MeanShift()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `MeanShift(bandwidth=...)`.\n\nMean shift clustering using a flat kernel. Mean shift clustering aims to discover \"blobs\" in a smooth density of samples. It is a centroid-based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids.\"\n"
":name" = "MeanShift"
":human_name" = "mean shift"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:bandwidth, :seeds, :bin_seeding, :min_bin_freq, :cluster_all, :n_jobs)`"
":hyperparameter_types" = "`(\"Union{Nothing, Float64}\", \"Union{Nothing, AbstractArray}\", \"Bool\", \"Int64\", \"Bool\", \"Union{Nothing, Int64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.AdaBoostRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.AdaBoostRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nAdaBoostRegressor\n```\n\nA model type for constructing a AdaBoost ensemble regression, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nAdaBoostRegressor = @load AdaBoostRegressor pkg=ScikitLearn\n```\n\nDo `model = AdaBoostRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`AdaBoostRegressor(base_estimator=...)`.\n# Hyper-parameters\n\n- `base_estimator = nothing`\n\n- `n_estimators = 50`\n\n- `learning_rate = 1.0`\n\n- `loss = linear`\n\n- `random_state = nothing`\n\n"
":name" = "AdaBoostRegressor"
":human_name" = "AdaBoost ensemble regression"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:base_estimator, :n_estimators, :learning_rate, :loss, :random_state)`"
":hyperparameter_types" = "`(\"Any\", \"Int64\", \"Float64\", \"String\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.AffinityPropagation]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Multiclass}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.AffinityPropagation"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nAffinityPropagation\n```\n\nA model type for constructing a Affinity Propagation Clustering of data, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nAffinityPropagation = @load AffinityPropagation pkg=ScikitLearn\n```\n\nDo `model = AffinityPropagation()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`AffinityPropagation(damping=...)`.\n# Hyper-parameters\n\n- `damping = 0.5`\n\n- `max_iter = 200`\n\n- `convergence_iter = 15`\n\n- `copy = true`\n\n- `preference = nothing`\n\n- `affinity = euclidean`\n\n- `verbose = false`\n\n"
":name" = "AffinityPropagation"
":human_name" = "Affinity Propagation Clustering of data"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:damping, :max_iter, :convergence_iter, :copy, :preference, :affinity, :verbose)`"
":hyperparameter_types" = "`(\"Float64\", \"Int64\", \"Int64\", \"Bool\", \"Any\", \"String\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.MultiTaskLassoCVRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.MultiTaskLassoCVRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nMultiTaskLassoCVRegressor\n```\n\nA model type for constructing a multi-target lasso regressor with built-in cross-validation, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nMultiTaskLassoCVRegressor = @load MultiTaskLassoCVRegressor pkg=ScikitLearn\n```\n\nDo `model = MultiTaskLassoCVRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`MultiTaskLassoCVRegressor(eps=...)`.\n# Hyper-parameters\n\n- `eps = 0.001`\n\n- `n_alphas = 100`\n\n- `alphas = nothing`\n\n- `fit_intercept = true`\n\n- `normalize = false`\n\n- `max_iter = 300`\n\n- `tol = 0.0001`\n\n- `copy_X = true`\n\n- `cv = 5`\n\n- `verbose = false`\n\n- `n_jobs = 1`\n\n- `random_state = nothing`\n\n- `selection = cyclic`\n\n"
":name" = "MultiTaskLassoCVRegressor"
":human_name" = "multi-target lasso regressor with built-in cross-validation"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:eps, :n_alphas, :alphas, :fit_intercept, :normalize, :max_iter, :tol, :copy_X, :cv, :verbose, :n_jobs, :random_state, :selection)`"
":hyperparameter_types" = "`(\"Float64\", \"Int64\", \"Any\", \"Bool\", \"Bool\", \"Int64\", \"Float64\", \"Bool\", \"Any\", \"Union{Bool, Int64}\", \"Union{Nothing, Int64}\", \"Any\", \"String\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.OrthogonalMatchingPursuitRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.OrthogonalMatchingPursuitRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nOrthogonalMatchingPursuitRegressor\n```\n\nA model type for constructing a orthogonal matching pursuit regressor, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nOrthogonalMatchingPursuitRegressor = @load OrthogonalMatchingPursuitRegressor pkg=ScikitLearn\n```\n\nDo `model = OrthogonalMatchingPursuitRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`OrthogonalMatchingPursuitRegressor(n_nonzero_coefs=...)`.\n# Hyper-parameters\n\n- `n_nonzero_coefs = nothing`\n\n- `tol = nothing`\n\n- `fit_intercept = true`\n\n- `normalize = true`\n\n- `precompute = auto`\n\n"
":name" = "OrthogonalMatchingPursuitRegressor"
":human_name" = "orthogonal matching pursuit regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:n_nonzero_coefs, :tol, :fit_intercept, :normalize, :precompute)`"
":hyperparameter_types" = "`(\"Union{Nothing, Int64}\", \"Union{Nothing, Float64}\", \"Bool\", \"Bool\", \"Union{Bool, String, AbstractMatrix}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.RidgeCVRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.RidgeCVRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nRidgeCVRegressor\n```\n\nA model type for constructing a ridge regressor with built-in cross-validation, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nRidgeCVRegressor = @load RidgeCVRegressor pkg=ScikitLearn\n```\n\nDo `model = RidgeCVRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`RidgeCVRegressor(alphas=...)`.\n# Hyper-parameters\n\n- `alphas = (0.1, 1.0, 10.0)`\n\n- `fit_intercept = true`\n\n- `normalize = false`\n\n- `scoring = nothing`\n\n- `cv = 5`\n\n- `gcv_mode = nothing`\n\n- `store_cv_values = false`\n\n"
":name" = "RidgeCVRegressor"
":human_name" = "ridge regressor with built-in cross-validation"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:alphas, :fit_intercept, :normalize, :scoring, :cv, :gcv_mode, :store_cv_values)`"
":hyperparameter_types" = "`(\"Any\", \"Bool\", \"Bool\", \"Any\", \"Any\", \"Union{Nothing, String}\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.PassiveAggressiveClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.PassiveAggressiveClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nPassiveAggressiveClassifier\n```\n\nA model type for constructing a passive aggressive classifier, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nPassiveAggressiveClassifier = @load PassiveAggressiveClassifier pkg=ScikitLearn\n```\n\nDo `model = PassiveAggressiveClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`PassiveAggressiveClassifier(C=...)`.\n# Hyper-parameters\n\n- `C = 1.0`\n\n- `fit_intercept = true`\n\n- `max_iter = 100`\n\n- `tol = 0.001`\n\n- `early_stopping = false`\n\n- `validation_fraction = 0.1`\n\n- `n_iter_no_change = 5`\n\n- `shuffle = true`\n\n- `verbose = 0`\n\n- `loss = hinge`\n\n- `n_jobs = nothing`\n\n- `random_state = 0`\n\n- `warm_start = false`\n\n- `class_weight = nothing`\n\n- `average = false`\n\n"
":name" = "PassiveAggressiveClassifier"
":human_name" = "passive aggressive classifier"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:C, :fit_intercept, :max_iter, :tol, :early_stopping, :validation_fraction, :n_iter_no_change, :shuffle, :verbose, :loss, :n_jobs, :random_state, :warm_start, :class_weight, :average)`"
":hyperparameter_types" = "`(\"Float64\", \"Bool\", \"Int64\", \"Float64\", \"Bool\", \"Float64\", \"Int64\", \"Bool\", \"Int64\", \"String\", \"Union{Nothing, Int64}\", \"Any\", \"Bool\", \"Any\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.SVMRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.SVMRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nSVMRegressor\n```\n\nA model type for constructing a epsilon-support vector regressor, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nSVMRegressor = @load SVMRegressor pkg=ScikitLearn\n```\n\nDo `model = SVMRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`SVMRegressor(kernel=...)`.\n# Hyper-parameters\n\n- `kernel = rbf`\n\n- `degree = 3`\n\n- `gamma = auto`\n\n- `coef0 = 0.0`\n\n- `tol = 0.001`\n\n- `C = 1.0`\n\n- `epsilon = 0.1`\n\n- `shrinking = true`\n\n- `cache_size = 200`\n\n- `max_iter = -1`\n\n"
":name" = "SVMRegressor"
":human_name" = "epsilon-support vector regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:kernel, :degree, :gamma, :coef0, :tol, :C, :epsilon, :shrinking, :cache_size, :max_iter)`"
":hyperparameter_types" = "`(\"Union{Function, String}\", \"Int64\", \"Union{Float64, String}\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Any\", \"Int64\", \"Int64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.BernoulliNBClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Count}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Count}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.BernoulliNBClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nBernoulliNBClassifier\n```\n\nA model type for constructing a Bernoulli naive Bayes classifier, based on [ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nBernoulliNBClassifier = @load BernoulliNBClassifier pkg=ScikitLearn\n```\n\nDo `model = BernoulliNBClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `BernoulliNBClassifier(alpha=...)`.\n\nBinomial naive bayes classifier. It is suitable for classification with binary features; features will be binarized based on the `binarize` keyword (unless it's `nothing` in which case the features are assumed to be binary).\n"
":name" = "BernoulliNBClassifier"
":human_name" = "Bernoulli naive Bayes classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:alpha, :binarize, :fit_prior, :class_prior)`"
":hyperparameter_types" = "`(\"Float64\", \"Union{Nothing, Float64}\", \"Bool\", \"Union{Nothing, AbstractVector}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.GaussianNBClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.GaussianNBClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nGaussianNBClassifier\n```\n\nA model type for constructing a Gaussian naive Bayes classifier, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nGaussianNBClassifier = @load GaussianNBClassifier pkg=ScikitLearn\n```\n\nDo `model = GaussianNBClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`GaussianNBClassifier(priors=...)`.\n# Hyper-parameters\n\n- `priors = nothing`\n\n- `var_smoothing = 1.0e-9`\n\n"
":name" = "GaussianNBClassifier"
":human_name" = "Gaussian naive Bayes classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:priors, :var_smoothing)`"
":hyperparameter_types" = "`(\"Union{Nothing, AbstractVector{Float64}}\", \"Float64\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.ExtraTreesClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.ExtraTreesClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nExtraTreesClassifier\n```\n\nA model type for constructing a extra trees classifier, based on [ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nExtraTreesClassifier = @load ExtraTreesClassifier pkg=ScikitLearn\n```\n\nDo `model = ExtraTreesClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `ExtraTreesClassifier(n_estimators=...)`.\n\nExtra trees classifier, fits a number of randomized decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n"
":name" = "ExtraTreesClassifier"
":human_name" = "extra trees classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:n_estimators, :criterion, :max_depth, :min_samples_split, :min_samples_leaf, :min_weight_fraction_leaf, :max_features, :max_leaf_nodes, :min_impurity_decrease, :bootstrap, :oob_score, :n_jobs, :random_state, :verbose, :warm_start, :class_weight)`"
":hyperparameter_types" = "`(\"Int64\", \"String\", \"Union{Nothing, Int64}\", \"Union{Float64, Int64}\", \"Union{Float64, Int64}\", \"Float64\", \"Union{Nothing, Float64, Int64, String}\", \"Union{Nothing, Int64}\", \"Float64\", \"Bool\", \"Bool\", \"Union{Nothing, Int64}\", \"Any\", \"Int64\", \"Bool\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.KMeans]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Multiclass}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.KMeans"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nKMeans\n```\n\nA model type for constructing a k means, based on [ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nKMeans = @load KMeans pkg=ScikitLearn\n```\n\nDo `model = KMeans()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `KMeans(n_clusters=...)`.\n\nK-Means algorithm: find K centroids corresponding to K clusters in the data.\n"
":name" = "KMeans"
":human_name" = "k means"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":transform"]
":hyperparameters" = "`(:n_clusters, :n_init, :max_iter, :tol, :verbose, :random_state, :copy_x, :algorithm, :init)`"
":hyperparameter_types" = "`(\"Int64\", \"Int64\", \"Int64\", \"Float64\", \"Int64\", \"Any\", \"Bool\", \"String\", \"Union{String, AbstractArray}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.MultiTaskElasticNetCVRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.MultiTaskElasticNetCVRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nMultiTaskElasticNetCVRegressor\n```\n\nA model type for constructing a multi-target elastic net regressor with built-in cross-validation, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nMultiTaskElasticNetCVRegressor = @load MultiTaskElasticNetCVRegressor pkg=ScikitLearn\n```\n\nDo `model = MultiTaskElasticNetCVRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`MultiTaskElasticNetCVRegressor(l1_ratio=...)`.\n# Hyper-parameters\n\n- `l1_ratio = 0.5`\n\n- `eps = 0.001`\n\n- `n_alphas = 100`\n\n- `alphas = nothing`\n\n- `fit_intercept = true`\n\n- `normalize = false`\n\n- `max_iter = 1000`\n\n- `tol = 0.0001`\n\n- `cv = 5`\n\n- `copy_X = true`\n\n- `verbose = 0`\n\n- `n_jobs = nothing`\n\n- `random_state = nothing`\n\n- `selection = cyclic`\n\n"
":name" = "MultiTaskElasticNetCVRegressor"
":human_name" = "multi-target elastic net regressor with built-in cross-validation"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:l1_ratio, :eps, :n_alphas, :alphas, :fit_intercept, :normalize, :max_iter, :tol, :cv, :copy_X, :verbose, :n_jobs, :random_state, :selection)`"
":hyperparameter_types" = "`(\"Union{Float64, Vector{Float64}}\", \"Float64\", \"Int64\", \"Any\", \"Bool\", \"Bool\", \"Int64\", \"Float64\", \"Any\", \"Bool\", \"Union{Bool, Int64}\", \"Union{Nothing, Int64}\", \"Any\", \"String\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.LassoLarsCVRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.LassoLarsCVRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLassoLarsCVRegressor\n```\n\nA model type for constructing a Lasso model fit with least angle regression (LARS) with built-in cross-validation, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nLassoLarsCVRegressor = @load LassoLarsCVRegressor pkg=ScikitLearn\n```\n\nDo `model = LassoLarsCVRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LassoLarsCVRegressor(fit_intercept=...)`.\n# Hyper-parameters\n\n- `fit_intercept = true`\n\n- `verbose = false`\n\n- `max_iter = 500`\n\n- `normalize = true`\n\n- `precompute = auto`\n\n- `cv = 5`\n\n- `max_n_alphas = 1000`\n\n- `n_jobs = nothing`\n\n- `eps = 2.220446049250313e-16`\n\n- `copy_X = true`\n\n- `positive = false`\n\n"
":name" = "LassoLarsCVRegressor"
":human_name" = "Lasso model fit with least angle regression (LARS) with built-in cross-validation"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:fit_intercept, :verbose, :max_iter, :normalize, :precompute, :cv, :max_n_alphas, :n_jobs, :eps, :copy_X, :positive)`"
":hyperparameter_types" = "`(\"Bool\", \"Union{Bool, Int64}\", \"Int64\", \"Bool\", \"Union{Bool, String, AbstractMatrix}\", \"Any\", \"Int64\", \"Union{Nothing, Int64}\", \"Float64\", \"Bool\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.OrthogonalMatchingPursuitCVRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.OrthogonalMatchingPursuitCVRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nOrthogonalMatchingPursuitCVRegressor\n```\n\nA model type for constructing a orthogonal ,atching pursuit (OMP) model with built-in cross-validation, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nOrthogonalMatchingPursuitCVRegressor = @load OrthogonalMatchingPursuitCVRegressor pkg=ScikitLearn\n```\n\nDo `model = OrthogonalMatchingPursuitCVRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`OrthogonalMatchingPursuitCVRegressor(copy=...)`.\n# Hyper-parameters\n\n- `copy = true`\n\n- `fit_intercept = true`\n\n- `normalize = false`\n\n- `max_iter = nothing`\n\n- `cv = 5`\n\n- `n_jobs = 1`\n\n- `verbose = false`\n\n"
":name" = "OrthogonalMatchingPursuitCVRegressor"
":human_name" = "orthogonal ,atching pursuit (OMP) model with built-in cross-validation"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:copy, :fit_intercept, :normalize, :max_iter, :cv, :n_jobs, :verbose)`"
":hyperparameter_types" = "`(\"Bool\", \"Bool\", \"Bool\", \"Union{Nothing, Int64}\", \"Any\", \"Union{Nothing, Int64}\", \"Union{Bool, Int64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.AdaBoostClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.AdaBoostClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nAdaBoostClassifier\n```\n\nA model type for constructing a ada boost classifier, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nAdaBoostClassifier = @load AdaBoostClassifier pkg=ScikitLearn\n```\n\nDo `model = AdaBoostClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`AdaBoostClassifier(base_estimator=...)`.\n# Hyper-parameters\n\n- `base_estimator = nothing`\n\n- `n_estimators = 50`\n\n- `learning_rate = 1.0`\n\n- `algorithm = SAMME.R`\n\n- `random_state = nothing`\n\n"
":name" = "AdaBoostClassifier"
":human_name" = "ada boost classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:base_estimator, :n_estimators, :learning_rate, :algorithm, :random_state)`"
":hyperparameter_types" = "`(\"Any\", \"Int64\", \"Float64\", \"String\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.PassiveAggressiveRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.PassiveAggressiveRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nPassiveAggressiveRegressor\n```\n\nA model type for constructing a passive aggressive regressor, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nPassiveAggressiveRegressor = @load PassiveAggressiveRegressor pkg=ScikitLearn\n```\n\nDo `model = PassiveAggressiveRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`PassiveAggressiveRegressor(C=...)`.\n# Hyper-parameters\n\n- `C = 1.0`\n\n- `fit_intercept = true`\n\n- `max_iter = 1000`\n\n- `tol = 0.0001`\n\n- `early_stopping = false`\n\n- `validation_fraction = 0.1`\n\n- `n_iter_no_change = 5`\n\n- `shuffle = true`\n\n- `verbose = 0`\n\n- `loss = epsilon_insensitive`\n\n- `epsilon = 0.1`\n\n- `random_state = nothing`\n\n- `warm_start = false`\n\n- `average = false`\n\n"
":name" = "PassiveAggressiveRegressor"
":human_name" = "passive aggressive regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:C, :fit_intercept, :max_iter, :tol, :early_stopping, :validation_fraction, :n_iter_no_change, :shuffle, :verbose, :loss, :epsilon, :random_state, :warm_start, :average)`"
":hyperparameter_types" = "`(\"Float64\", \"Bool\", \"Int64\", \"Float64\", \"Bool\", \"Float64\", \"Int64\", \"Bool\", \"Union{Bool, Int64}\", \"String\", \"Float64\", \"Any\", \"Bool\", \"Union{Bool, Int64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.BayesianRidgeRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.BayesianRidgeRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nBayesianRidgeRegressor\n```\n\nA model type for constructing a Bayesian ridge regressor, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nBayesianRidgeRegressor = @load BayesianRidgeRegressor pkg=ScikitLearn\n```\n\nDo `model = BayesianRidgeRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`BayesianRidgeRegressor(n_iter=...)`.\n# Hyper-parameters\n\n- `n_iter = 300`\n\n- `tol = 0.001`\n\n- `alpha_1 = 1.0e-6`\n\n- `alpha_2 = 1.0e-6`\n\n- `lambda_1 = 1.0e-6`\n\n- `lambda_2 = 1.0e-6`\n\n- `compute_score = false`\n\n- `fit_intercept = true`\n\n- `normalize = false`\n\n- `copy_X = true`\n\n- `verbose = false`\n\n"
":name" = "BayesianRidgeRegressor"
":human_name" = "Bayesian ridge regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:n_iter, :tol, :alpha_1, :alpha_2, :lambda_1, :lambda_2, :compute_score, :fit_intercept, :normalize, :copy_X, :verbose)`"
":hyperparameter_types" = "`(\"Int64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Bool\", \"Bool\", \"Bool\", \"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.RANSACRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.RANSACRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nRANSACRegressor\n```\n\nA model type for constructing a ransac regressor, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nRANSACRegressor = @load RANSACRegressor pkg=ScikitLearn\n```\n\nDo `model = RANSACRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`RANSACRegressor(base_estimator=...)`.\n# Hyper-parameters\n\n- `base_estimator = nothing`\n\n- `min_samples = 5`\n\n- `residual_threshold = nothing`\n\n- `is_data_valid = nothing`\n\n- `is_model_valid = nothing`\n\n- `max_trials = 100`\n\n- `max_skips = 9223372036854775807`\n\n- `stop_n_inliers = 9223372036854775807`\n\n- `stop_score = Inf`\n\n- `stop_probability = 0.99`\n\n- `loss = absolute_loss`\n\n- `random_state = nothing`\n\n"
":name" = "RANSACRegressor"
":human_name" = "ransac regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:base_estimator, :min_samples, :residual_threshold, :is_data_valid, :is_model_valid, :max_trials, :max_skips, :stop_n_inliers, :stop_score, :stop_probability, :loss, :random_state)`"
":hyperparameter_types" = "`(\"Any\", \"Union{Float64, Int64}\", \"Union{Nothing, Float64}\", \"Any\", \"Any\", \"Int64\", \"Int64\", \"Int64\", \"Float64\", \"Float64\", \"Union{Function, String}\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.BaggingClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.BaggingClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nBaggingClassifier\n```\n\nA model type for constructing a bagging ensemble classifier, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nBaggingClassifier = @load BaggingClassifier pkg=ScikitLearn\n```\n\nDo `model = BaggingClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`BaggingClassifier(base_estimator=...)`.\n# Hyper-parameters\n\n- `base_estimator = nothing`\n\n- `n_estimators = 10`\n\n- `max_samples = 1.0`\n\n- `max_features = 1.0`\n\n- `bootstrap = true`\n\n- `bootstrap_features = false`\n\n- `oob_score = false`\n\n- `warm_start = false`\n\n- `n_jobs = nothing`\n\n- `random_state = nothing`\n\n- `verbose = 0`\n\n"
":name" = "BaggingClassifier"
":human_name" = "bagging ensemble classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:base_estimator, :n_estimators, :max_samples, :max_features, :bootstrap, :bootstrap_features, :oob_score, :warm_start, :n_jobs, :random_state, :verbose)`"
":hyperparameter_types" = "`(\"Any\", \"Int64\", \"Union{Float64, Int64}\", \"Union{Float64, Int64}\", \"Bool\", \"Bool\", \"Bool\", \"Bool\", \"Union{Nothing, Int64}\", \"Any\", \"Int64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.GaussianProcessClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.GaussianProcessClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nGaussianProcessClassifier\n```\n\nA model type for constructing a Gaussian process classifier, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nGaussianProcessClassifier = @load GaussianProcessClassifier pkg=ScikitLearn\n```\n\nDo `model = GaussianProcessClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`GaussianProcessClassifier(kernel=...)`.\n# Hyper-parameters\n\n- `kernel = nothing`\n\n- `optimizer = fmin_l_bfgs_b`\n\n- `n_restarts_optimizer = 0`\n\n- `copy_X_train = true`\n\n- `random_state = nothing`\n\n- `max_iter_predict = 100`\n\n- `warm_start = false`\n\n- `multi_class = one_vs_rest`\n\n"
":name" = "GaussianProcessClassifier"
":human_name" = "Gaussian process classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:kernel, :optimizer, :n_restarts_optimizer, :copy_X_train, :random_state, :max_iter_predict, :warm_start, :multi_class)`"
":hyperparameter_types" = "`(\"Any\", \"Any\", \"Int64\", \"Bool\", \"Any\", \"Int64\", \"Bool\", \"String\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.OPTICS]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.OPTICS"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nOPTICS\n```\n\nA model type for constructing a optics, based on [ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nOPTICS = @load OPTICS pkg=ScikitLearn\n```\n\nDo `model = OPTICS()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `OPTICS(min_samples=...)`.\n\nOPTICS (Ordering Points To Identify the Clustering Structure), closely related to [`DBSCAN'](@ref), finds core sample of high density and expands clusters from them. Unlike DBSCAN, keeps cluster hierarchy for a variable neighborhood radius. Better suited for usage on large datasets than the current sklearn implementation of DBSCAN.\n"
":name" = "OPTICS"
":human_name" = "optics"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params"]
":hyperparameters" = "`(:min_samples, :max_eps, :metric, :p, :cluster_method, :eps, :xi, :predecessor_correction, :min_cluster_size, :algorithm, :leaf_size, :n_jobs)`"
":hyperparameter_types" = "`(\"Union{Float64, Int64}\", \"Float64\", \"String\", \"Int64\", \"String\", \"Union{Nothing, Float64}\", \"Float64\", \"Bool\", \"Union{Nothing, Float64, Int64}\", \"String\", \"Int64\", \"Union{Nothing, Int64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.KNeighborsRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.KNeighborsRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nKNeighborsRegressor\n```\n\nA model type for constructing a K-nearest neighbors regressor, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nKNeighborsRegressor = @load KNeighborsRegressor pkg=ScikitLearn\n```\n\nDo `model = KNeighborsRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`KNeighborsRegressor(n_neighbors=...)`.\n# Hyper-parameters\n\n- `n_neighbors = 5`\n\n- `weights = uniform`\n\n- `algorithm = auto`\n\n- `leaf_size = 30`\n\n- `p = 2`\n\n- `metric = minkowski`\n\n- `metric_params = nothing`\n\n- `n_jobs = nothing`\n\n"
":name" = "KNeighborsRegressor"
":human_name" = "K-nearest neighbors regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:n_neighbors, :weights, :algorithm, :leaf_size, :p, :metric, :metric_params, :n_jobs)`"
":hyperparameter_types" = "`(\"Int64\", \"Union{Function, String}\", \"String\", \"Int64\", \"Int64\", \"Any\", \"Any\", \"Union{Nothing, Int64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.MiniBatchKMeans]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Multiclass}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.MiniBatchKMeans"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nMiniBatchKMeans\n```\n\nA model type for constructing a Mini-Batch K-Means clustering., based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nMiniBatchKMeans = @load MiniBatchKMeans pkg=ScikitLearn\n```\n\nDo `model = MiniBatchKMeans()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`MiniBatchKMeans(n_clusters=...)`.\n# Hyper-parameters\n\n- `n_clusters = 8`\n\n- `max_iter = 100`\n\n- `batch_size = 100`\n\n- `verbose = 0`\n\n- `compute_labels = true`\n\n- `random_state = nothing`\n\n- `tol = 0.0`\n\n- `max_no_improvement = 10`\n\n- `init_size = nothing`\n\n- `n_init = 3`\n\n- `init = k-means++`\n\n- `reassignment_ratio = 0.01`\n\n"
":name" = "MiniBatchKMeans"
":human_name" = "Mini-Batch K-Means clustering."
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":transform"]
":hyperparameters" = "`(:n_clusters, :max_iter, :batch_size, :verbose, :compute_labels, :random_state, :tol, :max_no_improvement, :init_size, :n_init, :init, :reassignment_ratio)`"
":hyperparameter_types" = "`(\"Int64\", \"Int64\", \"Int64\", \"Int64\", \"Bool\", \"Any\", \"Float64\", \"Int64\", \"Union{Nothing, Int64}\", \"Int64\", \"Union{String, AbstractArray}\", \"Float64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.LassoCVRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.LassoCVRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLassoCVRegressor\n```\n\nA model type for constructing a lasso regressor with built-in cross-validation, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nLassoCVRegressor = @load LassoCVRegressor pkg=ScikitLearn\n```\n\nDo `model = LassoCVRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LassoCVRegressor(eps=...)`.\n# Hyper-parameters\n\n- `eps = 0.001`\n\n- `n_alphas = 100`\n\n- `alphas = nothing`\n\n- `fit_intercept = true`\n\n- `normalize = false`\n\n- `precompute = auto`\n\n- `max_iter = 1000`\n\n- `tol = 0.0001`\n\n- `copy_X = true`\n\n- `cv = 5`\n\n- `verbose = false`\n\n- `n_jobs = nothing`\n\n- `positive = false`\n\n- `random_state = nothing`\n\n- `selection = cyclic`\n\n"
":name" = "LassoCVRegressor"
":human_name" = "lasso regressor with built-in cross-validation"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:eps, :n_alphas, :alphas, :fit_intercept, :normalize, :precompute, :max_iter, :tol, :copy_X, :cv, :verbose, :n_jobs, :positive, :random_state, :selection)`"
":hyperparameter_types" = "`(\"Float64\", \"Int64\", \"Any\", \"Bool\", \"Bool\", \"Union{Bool, String, AbstractMatrix}\", \"Int64\", \"Float64\", \"Bool\", \"Any\", \"Union{Bool, Int64}\", \"Union{Nothing, Int64}\", \"Bool\", \"Any\", \"String\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.DummyRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.DummyRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nDummyRegressor\n```\n\nA model type for constructing a dummy regressor, based on [ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nDummyRegressor = @load DummyRegressor pkg=ScikitLearn\n```\n\nDo `model = DummyRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `DummyRegressor(strategy=...)`.\n\nDummyRegressor is a regressor that makes predictions using simple rules.\n"
":name" = "DummyRegressor"
":human_name" = "dummy regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:strategy, :constant, :quantile)`"
":hyperparameter_types" = "`(\"String\", \"Any\", \"Float64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.LassoLarsRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.LassoLarsRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLassoLarsRegressor\n```\n\nA model type for constructing a Lasso model fit with least angle regression (LARS), based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nLassoLarsRegressor = @load LassoLarsRegressor pkg=ScikitLearn\n```\n\nDo `model = LassoLarsRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LassoLarsRegressor(alpha=...)`.\n# Hyper-parameters\n\n- `alpha = 1.0`\n\n- `fit_intercept = true`\n\n- `verbose = false`\n\n- `normalize = true`\n\n- `precompute = auto`\n\n- `max_iter = 500`\n\n- `eps = 2.220446049250313e-16`\n\n- `copy_X = true`\n\n- `fit_path = true`\n\n- `positive = false`\n\n"
":name" = "LassoLarsRegressor"
":human_name" = "Lasso model fit with least angle regression (LARS)"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:alpha, :fit_intercept, :verbose, :normalize, :precompute, :max_iter, :eps, :copy_X, :fit_path, :positive)`"
":hyperparameter_types" = "`(\"Float64\", \"Bool\", \"Union{Bool, Int64}\", \"Bool\", \"Union{Bool, String, AbstractMatrix}\", \"Int64\", \"Float64\", \"Bool\", \"Bool\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.LarsCVRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.LarsCVRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLarsCVRegressor\n```\n\nA model type for constructing a least angle regressor with built-in cross-validation, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nLarsCVRegressor = @load LarsCVRegressor pkg=ScikitLearn\n```\n\nDo `model = LarsCVRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LarsCVRegressor(fit_intercept=...)`.\n# Hyper-parameters\n\n- `fit_intercept = true`\n\n- `verbose = false`\n\n- `max_iter = 500`\n\n- `normalize = true`\n\n- `precompute = auto`\n\n- `cv = 5`\n\n- `max_n_alphas = 1000`\n\n- `n_jobs = nothing`\n\n- `eps = 2.220446049250313e-16`\n\n- `copy_X = true`\n\n"
":name" = "LarsCVRegressor"
":human_name" = "least angle regressor with built-in cross-validation"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:fit_intercept, :verbose, :max_iter, :normalize, :precompute, :cv, :max_n_alphas, :n_jobs, :eps, :copy_X)`"
":hyperparameter_types" = "`(\"Bool\", \"Union{Bool, Int64}\", \"Int64\", \"Bool\", \"Union{Bool, String, AbstractMatrix}\", \"Any\", \"Int64\", \"Union{Nothing, Int64}\", \"Float64\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.KNeighborsClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.KNeighborsClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nKNeighborsClassifier\n```\n\nA model type for constructing a K-nearest neighbors classifier, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nKNeighborsClassifier = @load KNeighborsClassifier pkg=ScikitLearn\n```\n\nDo `model = KNeighborsClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`KNeighborsClassifier(n_neighbors=...)`.\n# Hyper-parameters\n\n- `n_neighbors = 5`\n\n- `weights = uniform`\n\n- `algorithm = auto`\n\n- `leaf_size = 30`\n\n- `p = 2`\n\n- `metric = minkowski`\n\n- `metric_params = nothing`\n\n- `n_jobs = nothing`\n\n"
":name" = "KNeighborsClassifier"
":human_name" = "K-nearest neighbors classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:n_neighbors, :weights, :algorithm, :leaf_size, :p, :metric, :metric_params, :n_jobs)`"
":hyperparameter_types" = "`(\"Int64\", \"Union{Function, String}\", \"String\", \"Int64\", \"Int64\", \"Any\", \"Any\", \"Union{Nothing, Int64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.SVMLinearClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.SVMLinearClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nSVMLinearClassifier\n```\n\nA model type for constructing a linear support vector classifier, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nSVMLinearClassifier = @load SVMLinearClassifier pkg=ScikitLearn\n```\n\nDo `model = SVMLinearClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`SVMLinearClassifier(penalty=...)`.\n# Hyper-parameters\n\n- `penalty = l2`\n\n- `loss = squared_hinge`\n\n- `dual = true`\n\n- `tol = 0.0001`\n\n- `C = 1.0`\n\n- `multi_class = ovr`\n\n- `fit_intercept = true`\n\n- `intercept_scaling = 1.0`\n\n- `random_state = nothing`\n\n- `max_iter = 1000`\n\n"
":name" = "SVMLinearClassifier"
":human_name" = "linear support vector classifier"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:penalty, :loss, :dual, :tol, :C, :multi_class, :fit_intercept, :intercept_scaling, :random_state, :max_iter)`"
":hyperparameter_types" = "`(\"String\", \"String\", \"Bool\", \"Float64\", \"Float64\", \"String\", \"Bool\", \"Float64\", \"Any\", \"Int64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.FeatureAgglomeration]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.FeatureAgglomeration"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nFeatureAgglomeration\n```\n\nA model type for constructing a feature agglomeration, based on [ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nFeatureAgglomeration = @load FeatureAgglomeration pkg=ScikitLearn\n```\n\nDo `model = FeatureAgglomeration()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `FeatureAgglomeration(n_clusters=...)`.\n\nSimilar to [`AgglomerativeClustering`](@ref), but recursively merges features instead of samples.\"\n"
":name" = "FeatureAgglomeration"
":human_name" = "feature agglomeration"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":inverse_transform", ":transform"]
":hyperparameters" = "`(:n_clusters, :memory, :connectivity, :affinity, :compute_full_tree, :linkage, :distance_threshold)`"
":hyperparameter_types" = "`(\"Int64\", \"Any\", \"Any\", \"Any\", \"Union{Bool, String}\", \"String\", \"Union{Nothing, Float64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.DummyClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.DummyClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nDummyClassifier\n```\n\nA model type for constructing a dummy classifier, based on [ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nDummyClassifier = @load DummyClassifier pkg=ScikitLearn\n```\n\nDo `model = DummyClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `DummyClassifier(strategy=...)`.\n\nDummyClassifier is a classifier that makes predictions using simple rules.\n"
":name" = "DummyClassifier"
":human_name" = "dummy classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:strategy, :constant, :random_state)`"
":hyperparameter_types" = "`(\"String\", \"Any\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.BaggingRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.BaggingRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nBaggingRegressor\n```\n\nA model type for constructing a bagging ensemble regressor, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nBaggingRegressor = @load BaggingRegressor pkg=ScikitLearn\n```\n\nDo `model = BaggingRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`BaggingRegressor(base_estimator=...)`.\n# Hyper-parameters\n\n- `base_estimator = nothing`\n\n- `n_estimators = 10`\n\n- `max_samples = 1.0`\n\n- `max_features = 1.0`\n\n- `bootstrap = true`\n\n- `bootstrap_features = false`\n\n- `oob_score = false`\n\n- `warm_start = false`\n\n- `n_jobs = nothing`\n\n- `random_state = nothing`\n\n- `verbose = 0`\n\n"
":name" = "BaggingRegressor"
":human_name" = "bagging ensemble regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:base_estimator, :n_estimators, :max_samples, :max_features, :bootstrap, :bootstrap_features, :oob_score, :warm_start, :n_jobs, :random_state, :verbose)`"
":hyperparameter_types" = "`(\"Any\", \"Int64\", \"Union{Float64, Int64}\", \"Union{Float64, Int64}\", \"Bool\", \"Bool\", \"Bool\", \"Bool\", \"Union{Nothing, Int64}\", \"Any\", \"Int64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.BayesianQDA]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.BayesianQDA"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nBayesianQDA\n```\n\nA model type for constructing a Bayesian quadratic discriminant analysis, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nBayesianQDA = @load BayesianQDA pkg=ScikitLearn\n```\n\nDo `model = BayesianQDA()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`BayesianQDA(priors=...)`.\n# Hyper-parameters\n\n- `priors = nothing`\n\n- `reg_param = 0.0`\n\n- `store_covariance = false`\n\n- `tol = 0.0001`\n\n"
":name" = "BayesianQDA"
":human_name" = "Bayesian quadratic discriminant analysis"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:priors, :reg_param, :store_covariance, :tol)`"
":hyperparameter_types" = "`(\"Union{Nothing, AbstractVector}\", \"Float64\", \"Bool\", \"Float64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.BayesianLDA]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.BayesianLDA"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nBayesianLDA\n```\n\nA model type for constructing a Bayesian linear discriminant analysis, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nBayesianLDA = @load BayesianLDA pkg=ScikitLearn\n```\n\nDo `model = BayesianLDA()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`BayesianLDA(solver=...)`.\n# Hyper-parameters\n\n- `solver = svd`\n\n- `shrinkage = nothing`\n\n- `priors = nothing`\n\n- `n_components = nothing`\n\n- `store_covariance = false`\n\n- `tol = 0.0001`\n\n"
":name" = "BayesianLDA"
":human_name" = "Bayesian linear discriminant analysis"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:solver, :shrinkage, :priors, :n_components, :store_covariance, :tol)`"
":hyperparameter_types" = "`(\"String\", \"Union{Nothing, Float64, String}\", \"Union{Nothing, AbstractVector}\", \"Union{Nothing, Int64}\", \"Bool\", \"Float64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.SGDClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.SGDClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nSGDClassifier\n```\n\nA model type for constructing a sgd classifier, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nSGDClassifier = @load SGDClassifier pkg=ScikitLearn\n```\n\nDo `model = SGDClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`SGDClassifier(loss=...)`.\n# Hyper-parameters\n\n- `loss = hinge`\n\n- `penalty = l2`\n\n- `alpha = 0.0001`\n\n- `l1_ratio = 0.15`\n\n- `fit_intercept = true`\n\n- `max_iter = 1000`\n\n- `tol = 0.001`\n\n- `shuffle = true`\n\n- `verbose = 0`\n\n- `epsilon = 0.1`\n\n- `n_jobs = nothing`\n\n- `random_state = nothing`\n\n- `learning_rate = optimal`\n\n- `eta0 = 0.0`\n\n- `power_t = 0.5`\n\n- `early_stopping = false`\n\n- `validation_fraction = 0.1`\n\n- `n_iter_no_change = 5`\n\n- `class_weight = nothing`\n\n- `warm_start = false`\n\n- `average = false`\n\n"
":name" = "SGDClassifier"
":human_name" = "sgd classifier"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:loss, :penalty, :alpha, :l1_ratio, :fit_intercept, :max_iter, :tol, :shuffle, :verbose, :epsilon, :n_jobs, :random_state, :learning_rate, :eta0, :power_t, :early_stopping, :validation_fraction, :n_iter_no_change, :class_weight, :warm_start, :average)`"
":hyperparameter_types" = "`(\"String\", \"String\", \"Float64\", \"Float64\", \"Bool\", \"Int64\", \"Union{Nothing, Float64}\", \"Bool\", \"Int64\", \"Float64\", \"Union{Nothing, Int64}\", \"Any\", \"String\", \"Float64\", \"Float64\", \"Bool\", \"Float64\", \"Int64\", \"Any\", \"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.TheilSenRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.TheilSenRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nTheilSenRegressor\n```\n\nA model type for constructing a Theil-Sen regressor, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nTheilSenRegressor = @load TheilSenRegressor pkg=ScikitLearn\n```\n\nDo `model = TheilSenRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`TheilSenRegressor(fit_intercept=...)`.\n# Hyper-parameters\n\n- `fit_intercept = true`\n\n- `copy_X = true`\n\n- `max_subpopulation = 10000`\n\n- `n_subsamples = nothing`\n\n- `max_iter = 300`\n\n- `tol = 0.001`\n\n- `random_state = nothing`\n\n- `n_jobs = nothing`\n\n- `verbose = false`\n\n"
":name" = "TheilSenRegressor"
":human_name" = "Theil-Sen regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:fit_intercept, :copy_X, :max_subpopulation, :n_subsamples, :max_iter, :tol, :random_state, :n_jobs, :verbose)`"
":hyperparameter_types" = "`(\"Bool\", \"Bool\", \"Int64\", \"Union{Nothing, Int64}\", \"Int64\", \"Float64\", \"Any\", \"Union{Nothing, Int64}\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.SpectralClustering]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.SpectralClustering"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nSpectralClustering\n```\n\nA model type for constructing a spectral clustering, based on [ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nSpectralClustering = @load SpectralClustering pkg=ScikitLearn\n```\n\nDo `model = SpectralClustering()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `SpectralClustering(n_clusters=...)`.\n\nApply clustering to a projection of the normalized Laplacian.  In practice spectral clustering is very useful when the structure of the individual clusters is highly non-convex or more generally when a measure of the center and spread of the cluster is not a suitable description of the complete cluster. For instance when clusters are nested circles on the 2D plane.\n"
":name" = "SpectralClustering"
":human_name" = "spectral clustering"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params"]
":hyperparameters" = "`(:n_clusters, :eigen_solver, :random_state, :n_init, :gamma, :affinity, :n_neighbors, :eigen_tol, :assign_labels, :n_jobs)`"
":hyperparameter_types" = "`(\"Int64\", \"Union{Nothing, String}\", \"Any\", \"Int64\", \"Float64\", \"String\", \"Int64\", \"Float64\", \"String\", \"Union{Nothing, Int64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.Birch]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Multiclass}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.Birch"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nBirch\n```\n\nA model type for constructing a birch, based on [ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nBirch = @load Birch pkg=ScikitLearn\n```\n\nDo `model = Birch()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `Birch(threshold=...)`.\n\nMemory-efficient, online-learning algorithm provided as an alternative to MiniBatchKMeans. Note: noisy samples are given the label -1.\n"
":name" = "Birch"
":human_name" = "birch"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":transform"]
":hyperparameters" = "`(:threshold, :branching_factor, :n_clusters, :compute_labels, :copy)`"
":hyperparameter_types" = "`(\"Float64\", \"Int64\", \"Int64\", \"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.AgglomerativeClustering]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.AgglomerativeClustering"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nAgglomerativeClustering\n```\n\nA model type for constructing a agglomerative clustering, based on [ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nAgglomerativeClustering = @load AgglomerativeClustering pkg=ScikitLearn\n```\n\nDo `model = AgglomerativeClustering()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `AgglomerativeClustering(n_clusters=...)`.\n\nRecursively merges the pair of clusters that minimally increases a given linkage distance. Note: there is no `predict` or `transform`. Instead, inspect the `fitted_params`.\n"
":name" = "AgglomerativeClustering"
":human_name" = "agglomerative clustering"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params"]
":hyperparameters" = "`(:n_clusters, :affinity, :memory, :connectivity, :compute_full_tree, :linkage, :distance_threshold)`"
":hyperparameter_types" = "`(\"Int64\", \"String\", \"Any\", \"Any\", \"Union{Bool, String}\", \"String\", \"Union{Nothing, Float64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.ElasticNetRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.ElasticNetRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nElasticNetRegressor\n```\n\nA model type for constructing a elastic net regressor, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nElasticNetRegressor = @load ElasticNetRegressor pkg=ScikitLearn\n```\n\nDo `model = ElasticNetRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`ElasticNetRegressor(alpha=...)`.\n# Hyper-parameters\n\n- `alpha = 1.0`\n\n- `l1_ratio = 0.5`\n\n- `fit_intercept = true`\n\n- `normalize = false`\n\n- `precompute = false`\n\n- `max_iter = 1000`\n\n- `copy_X = true`\n\n- `tol = 0.0001`\n\n- `warm_start = false`\n\n- `positive = false`\n\n- `random_state = nothing`\n\n- `selection = cyclic`\n\n"
":name" = "ElasticNetRegressor"
":human_name" = "elastic net regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:alpha, :l1_ratio, :fit_intercept, :normalize, :precompute, :max_iter, :copy_X, :tol, :warm_start, :positive, :random_state, :selection)`"
":hyperparameter_types" = "`(\"Float64\", \"Float64\", \"Bool\", \"Bool\", \"Union{Bool, AbstractMatrix}\", \"Int64\", \"Bool\", \"Float64\", \"Bool\", \"Bool\", \"Any\", \"String\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.RandomForestClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.Continuous}}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.RandomForestClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nRandomForestClassifier\n```\n\nA model type for constructing a random forest classifier, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nRandomForestClassifier = @load RandomForestClassifier pkg=ScikitLearn\n```\n\nDo `model = RandomForestClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`RandomForestClassifier(n_estimators=...)`.\n# Hyper-parameters\n\n- `n_estimators = 100`\n\n- `criterion = gini`\n\n- `max_depth = nothing`\n\n- `min_samples_split = 2`\n\n- `min_samples_leaf = 1`\n\n- `min_weight_fraction_leaf = 0.0`\n\n- `max_features = auto`\n\n- `max_leaf_nodes = nothing`\n\n- `min_impurity_decrease = 0.0`\n\n- `bootstrap = true`\n\n- `oob_score = false`\n\n- `n_jobs = nothing`\n\n- `random_state = nothing`\n\n- `verbose = 0`\n\n- `warm_start = false`\n\n- `class_weight = nothing`\n\n- `ccp_alpha = 0.0`\n\n- `max_samples = nothing`\n\n"
":name" = "RandomForestClassifier"
":human_name" = "random forest classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:n_estimators, :criterion, :max_depth, :min_samples_split, :min_samples_leaf, :min_weight_fraction_leaf, :max_features, :max_leaf_nodes, :min_impurity_decrease, :bootstrap, :oob_score, :n_jobs, :random_state, :verbose, :warm_start, :class_weight, :ccp_alpha, :max_samples)`"
":hyperparameter_types" = "`(\"Int64\", \"String\", \"Union{Nothing, Int64}\", \"Union{Float64, Int64}\", \"Union{Float64, Int64}\", \"Float64\", \"Union{Nothing, Float64, Int64, String}\", \"Union{Nothing, Int64}\", \"Float64\", \"Bool\", \"Bool\", \"Union{Nothing, Int64}\", \"Any\", \"Int64\", \"Bool\", \"Any\", \"Float64\", \"Union{Nothing, Float64, Int64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.LogisticCVClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.LogisticCVClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLogisticCVClassifier\n```\n\nA model type for constructing a logistic regression classifier with built-in cross-validation, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nLogisticCVClassifier = @load LogisticCVClassifier pkg=ScikitLearn\n```\n\nDo `model = LogisticCVClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LogisticCVClassifier(Cs=...)`.\n# Hyper-parameters\n\n- `Cs = 10`\n\n- `fit_intercept = true`\n\n- `cv = 5`\n\n- `dual = false`\n\n- `penalty = l2`\n\n- `scoring = nothing`\n\n- `solver = lbfgs`\n\n- `tol = 0.0001`\n\n- `max_iter = 100`\n\n- `class_weight = nothing`\n\n- `n_jobs = nothing`\n\n- `verbose = 0`\n\n- `refit = true`\n\n- `intercept_scaling = 1.0`\n\n- `multi_class = auto`\n\n- `random_state = nothing`\n\n- `l1_ratios = nothing`\n\n"
":name" = "LogisticCVClassifier"
":human_name" = "logistic regression classifier with built-in cross-validation"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:Cs, :fit_intercept, :cv, :dual, :penalty, :scoring, :solver, :tol, :max_iter, :class_weight, :n_jobs, :verbose, :refit, :intercept_scaling, :multi_class, :random_state, :l1_ratios)`"
":hyperparameter_types" = "`(\"Union{Int64, AbstractVector{Float64}}\", \"Bool\", \"Any\", \"Bool\", \"String\", \"Any\", \"String\", \"Float64\", \"Int64\", \"Any\", \"Union{Nothing, Int64}\", \"Int64\", \"Bool\", \"Float64\", \"String\", \"Any\", \"Union{Nothing, AbstractVector{Float64}}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.MultiTaskElasticNetRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.MultiTaskElasticNetRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nMultiTaskElasticNetRegressor\n```\n\nA model type for constructing a multi-target elastic net regressor, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nMultiTaskElasticNetRegressor = @load MultiTaskElasticNetRegressor pkg=ScikitLearn\n```\n\nDo `model = MultiTaskElasticNetRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`MultiTaskElasticNetRegressor(alpha=...)`.\n# Hyper-parameters\n\n- `alpha = 1.0`\n\n- `l1_ratio = 0.5`\n\n- `fit_intercept = true`\n\n- `normalize = true`\n\n- `copy_X = true`\n\n- `max_iter = 1000`\n\n- `tol = 0.0001`\n\n- `warm_start = false`\n\n- `random_state = nothing`\n\n- `selection = cyclic`\n\n"
":name" = "MultiTaskElasticNetRegressor"
":human_name" = "multi-target elastic net regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:alpha, :l1_ratio, :fit_intercept, :normalize, :copy_X, :max_iter, :tol, :warm_start, :random_state, :selection)`"
":hyperparameter_types" = "`(\"Float64\", \"Union{Float64, Vector{Float64}}\", \"Bool\", \"Bool\", \"Bool\", \"Int64\", \"Float64\", \"Bool\", \"Any\", \"String\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.ExtraTreesRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.ExtraTreesRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nExtraTreesRegressor\n```\n\nA model type for constructing a extra trees regressor, based on [ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nExtraTreesRegressor = @load ExtraTreesRegressor pkg=ScikitLearn\n```\n\nDo `model = ExtraTreesRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `ExtraTreesRegressor(n_estimators=...)`.\n\nExtra trees regressor, fits a number of randomized decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n"
":name" = "ExtraTreesRegressor"
":human_name" = "extra trees regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:n_estimators, :criterion, :max_depth, :min_samples_split, :min_samples_leaf, :min_weight_fraction_leaf, :max_features, :max_leaf_nodes, :min_impurity_decrease, :bootstrap, :oob_score, :n_jobs, :random_state, :verbose, :warm_start)`"
":hyperparameter_types" = "`(\"Int64\", \"String\", \"Union{Nothing, Int64}\", \"Union{Float64, Int64}\", \"Union{Float64, Int64}\", \"Float64\", \"Union{Nothing, Float64, Int64, String}\", \"Union{Nothing, Int64}\", \"Float64\", \"Bool\", \"Bool\", \"Union{Nothing, Int64}\", \"Any\", \"Int64\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.LassoRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.LassoRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLassoRegressor\n```\n\nA model type for constructing a lasso regressor, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nLassoRegressor = @load LassoRegressor pkg=ScikitLearn\n```\n\nDo `model = LassoRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LassoRegressor(alpha=...)`.\n# Hyper-parameters\n\n- `alpha = 1.0`\n\n- `fit_intercept = true`\n\n- `normalize = false`\n\n- `precompute = false`\n\n- `copy_X = true`\n\n- `max_iter = 1000`\n\n- `tol = 0.0001`\n\n- `warm_start = false`\n\n- `positive = false`\n\n- `random_state = nothing`\n\n- `selection = cyclic`\n\n"
":name" = "LassoRegressor"
":human_name" = "lasso regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:alpha, :fit_intercept, :normalize, :precompute, :copy_X, :max_iter, :tol, :warm_start, :positive, :random_state, :selection)`"
":hyperparameter_types" = "`(\"Float64\", \"Bool\", \"Bool\", \"Union{Bool, AbstractMatrix}\", \"Bool\", \"Int64\", \"Float64\", \"Bool\", \"Bool\", \"Any\", \"String\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.MultinomialNBClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Count}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Count}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.MultinomialNBClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nMultinomialNBClassifier\n```\n\nA model type for constructing a multinomial naive Bayes classifier, based on [ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nMultinomialNBClassifier = @load MultinomialNBClassifier pkg=ScikitLearn\n```\n\nDo `model = MultinomialNBClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `MultinomialNBClassifier(alpha=...)`.\n\nMultinomial naive bayes classifier. It is suitable for classification with discrete features (e.g. word counts for text classification).\n"
":name" = "MultinomialNBClassifier"
":human_name" = "multinomial naive Bayes classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:alpha, :fit_prior, :class_prior)`"
":hyperparameter_types" = "`(\"Float64\", \"Bool\", \"Union{Nothing, AbstractVector}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.GradientBoostingRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.GradientBoostingRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nGradientBoostingRegressor\n```\n\nA model type for constructing a gradient boosting ensemble regression, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nGradientBoostingRegressor = @load GradientBoostingRegressor pkg=ScikitLearn\n```\n\nDo `model = GradientBoostingRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`GradientBoostingRegressor(loss=...)`.\n# Hyper-parameters\n\n- `loss = ls`\n\n- `learning_rate = 0.1`\n\n- `n_estimators = 100`\n\n- `subsample = 1.0`\n\n- `criterion = friedman_mse`\n\n- `min_samples_split = 2`\n\n- `min_samples_leaf = 1`\n\n- `min_weight_fraction_leaf = 0.0`\n\n- `max_depth = 3`\n\n- `min_impurity_decrease = 0.0`\n\n- `init = nothing`\n\n- `random_state = nothing`\n\n- `max_features = nothing`\n\n- `alpha = 0.9`\n\n- `verbose = 0`\n\n- `max_leaf_nodes = nothing`\n\n- `warm_start = false`\n\n- `validation_fraction = 0.1`\n\n- `n_iter_no_change = nothing`\n\n- `tol = 0.0001`\n\n"
":name" = "GradientBoostingRegressor"
":human_name" = "gradient boosting ensemble regression"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:loss, :learning_rate, :n_estimators, :subsample, :criterion, :min_samples_split, :min_samples_leaf, :min_weight_fraction_leaf, :max_depth, :min_impurity_decrease, :init, :random_state, :max_features, :alpha, :verbose, :max_leaf_nodes, :warm_start, :validation_fraction, :n_iter_no_change, :tol)`"
":hyperparameter_types" = "`(\"String\", \"Float64\", \"Int64\", \"Float64\", \"String\", \"Union{Float64, Int64}\", \"Union{Float64, Int64}\", \"Float64\", \"Int64\", \"Float64\", \"Any\", \"Any\", \"Union{Nothing, Float64, Int64, String}\", \"Float64\", \"Int64\", \"Union{Nothing, Int64}\", \"Bool\", \"Float64\", \"Union{Nothing, Int64}\", \"Float64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ScikitLearn.SVMClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "ScikitLearn"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.SVMClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/cstjean/ScikitLearn.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nSVMClassifier\n```\n\nA model type for constructing a C-support vector classifier, based on\n[ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n\n```\nSVMClassifier = @load SVMClassifier pkg=ScikitLearn\n```\n\nDo `model = SVMClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`SVMClassifier(C=...)`.\n# Hyper-parameters\n\n- `C = 1.0`\n\n- `kernel = rbf`\n\n- `degree = 3`\n\n- `gamma = auto`\n\n- `coef0 = 0.0`\n\n- `shrinking = true`\n\n- `tol = 0.001`\n\n- `cache_size = 200`\n\n- `max_iter = -1`\n\n- `decision_function_shape = ovr`\n\n- `random_state = nothing`\n\n"
":name" = "SVMClassifier"
":human_name" = "C-support vector classifier"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:C, :kernel, :degree, :gamma, :coef0, :shrinking, :tol, :cache_size, :max_iter, :decision_function_shape, :random_state)`"
":hyperparameter_types" = "`(\"Float64\", \"Union{Function, String}\", \"Int64\", \"Union{Float64, String}\", \"Float64\", \"Bool\", \"Float64\", \"Int64\", \"Int64\", \"String\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ParallelKMeans.KMeans]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`AbstractArray{<:ScientificTypesBase.Multiclass}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "ParallelKMeans"
":package_license" = "MIT"
":load_path" = "ParallelKMeans.KMeans"
":package_uuid" = "42b8e9d4-006b-409a-8472-7f34b3fb58af"
":package_url" = "https://github.com/PyDataBlog/ParallelKMeans.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Parallel & lightning fast implementation of all available variants of the KMeans clustering algorithm\n                             in native Julia. Compatible with Julia 1.3+"
":name" = "KMeans"
":human_name" = "k means"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":transform"]
":hyperparameters" = "`(:algo, :k_init, :k, :tol, :max_iters, :copy, :threads, :rng, :weights, :init)`"
":hyperparameter_types" = "`(\"Union{Symbol, ParallelKMeans.AbstractKMeansAlg}\", \"String\", \"Int64\", \"Float64\", \"Int64\", \"Bool\", \"Int64\", \"Union{Int64, Random.AbstractRNG}\", \"Any\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[NaiveBayes.GaussianNBClassifier]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "NaiveBayes"
":package_license" = "unknown"
":load_path" = "MLJNaiveBayesInterface.GaussianNBClassifier"
":package_uuid" = "9bbee03b-0db5-5f46-924f-b5c9c21b8c60"
":package_url" = "https://github.com/dfdx/NaiveBayes.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nGaussianNBClassifier\n```\n\nA model type for constructing a Gaussian naive Bayes classifier, based on [NaiveBayes.jl](https://github.com/dfdx/NaiveBayes.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nGaussianNBClassifier = @load GaussianNBClassifier pkg=NaiveBayes\n```\n\nDo `model = GaussianNBClassifier()` to construct an instance with default hyper-parameters. \n\nGiven each class taken on by the target variable `y`, it is supposed that the conditional probability distribution for the input variables `X` is a multivariate Gaussian. The mean and covariance of these Gaussian distributions are estimated using maximum likelihood, and a probability distribution for `y` given `X` is deduced by applying Bayes' rule. The required marginal for `y` is estimated using class frequency in the training data.\n\n**Important.** The name \"naive Bayes classifier\" is perhaps misleading. Since we are learning the full multivariate Gaussian distributions for `X` given `y`, we are not applying the usual naive Bayes independence condition, which would amount to forcing the covariance matrix to be diagonal.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check the column scitypes with `schema(X)`\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `Finite`; check the scitype with `schema(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given new features `Xnew`, which should have the same scitype as `X` above. Predictions are probabilistic.\n  * `predict_mode(mach, Xnew)`: Return the mode of above predictions.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `c_counts`: A dictionary containing the observed count of each input class.\n  * `c_stats`: A dictionary containing observed statistics on each input class. Each class is represented by a `DataStats` object, with the following fields:\n\n      * `n_vars`: The number of variables used to describe the class's behavior.\n      * `n_obs`: The number of times the class is observed.\n      * `obs_axis`: The axis along which the observations were computed.\n  * `gaussians`: A per class dictionary of Gaussians, each representing the distribution of the class. Represented with type `Distributions.MvNormal` from the Distributions.jl package.\n  * `n_obs`: The total number of observations in the training data.\n\n# Examples\n\n```\nusing MLJ\nGaussianNB = @load GaussianNBClassifier pkg=NaiveBayes\n\nX, y = @load_iris\nclf = GaussianNB()\nmach = machine(clf, X, y) |> fit!\n\nfitted_params(mach)\n\npreds = predict(mach, X) # probabilistic predictions\npreds[1]\npredict_mode(mach, X) # point predictions\n```\n\nSee also [`MultinomialNBClassifier`](@ref)\n"
":name" = "GaussianNBClassifier"
":human_name" = "Gaussian naive Bayes classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`()`"
":hyperparameter_types" = "`()`"
":hyperparameter_ranges" = "`()`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[NaiveBayes.MultinomialNBClassifier]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Count}}, AbstractMatrix{<:ScientificTypesBase.Count}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Count}}, AbstractMatrix{<:ScientificTypesBase.Count}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "NaiveBayes"
":package_license" = "unknown"
":load_path" = "MLJNaiveBayesInterface.MultinomialNBClassifier"
":package_uuid" = "9bbee03b-0db5-5f46-924f-b5c9c21b8c60"
":package_url" = "https://github.com/dfdx/NaiveBayes.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nMultinomialNBClassifier\n```\n\nA model type for constructing a multinomial naive Bayes classifier, based on [NaiveBayes.jl](https://github.com/dfdx/NaiveBayes.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nMultinomialNBClassifier = @load MultinomialNBClassifier pkg=NaiveBayes\n```\n\nDo `model = MultinomialNBClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `MultinomialNBClassifier(alpha=...)`.\n\nThe [multinomial naive Bayes classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Multinomial_naive_Bayes) is often applied when input features consist of a counts (scitype `Count`) and when observations for a fixed target class are generated from a multinomial distribution with fixed probability vector, but whose sample length varies from observation to observation. For example, features might represent word counts in text documents being classified by sentiment.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Count`; check the column scitypes with `schema(X)`.\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `Finite`; check the scitype with `schema(y)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `alpha=1`: Lindstone smoothing in estimation of multinomial probability vectors from training histograms (default corresponds to Laplacian smoothing).\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given new features `Xnew`, which should have the same scitype as `X` above.\n  * `predict_mode(mach, Xnew)`: Return the mode of above predictions.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `c_counts`: A dictionary containing the observed count of each input class.\n  * `x_counts`: A dictionary containing the categorical counts of each input class.\n  * `x_totals`: The sum of each count (input feature), ungrouped.\n  * `n_obs`: The total number of observations in the training data.\n\n# Examples\n\n```\nusing MLJ\nimport TextAnalysis\n\nCountTransformer = @load CountTransformer pkg=MLJText\nMultinomialNBClassifier = @load MultinomialNBClassifier pkg=NaiveBayes\n\ntokenized_docs = TextAnalysis.tokenize.([\n    \"I am very mad. You never listen.\",\n    \"You seem to be having trouble? Can I help you?\",\n    \"Our boss is mad at me. I hope he dies.\",\n    \"His boss wants to help me. She is nice.\",\n    \"Thank you for your help. It is nice working with you.\",\n    \"Never do that again! I am so mad. \",\n])\n\nsentiment = [\n    \"negative\",\n    \"positive\",\n    \"negative\",\n    \"positive\",\n    \"positive\",\n    \"negative\",\n]\n\nmach1 = machine(CountTransformer(), tokenized_docs) |> fit!\n\n# matrix of counts:\nX = transform(mach1, tokenized_docs)\n\n# to ensure scitype(y) <: AbstractVector{<:OrderedFactor}:\ny = coerce(sentiment, OrderedFactor)\n\nclassifier = MultinomialNBClassifier()\nmach2 = machine(classifier, X, y)\nfit!(mach2, rows=1:4)\n\n# probabilistic predictions:\ny_prob = predict(mach2, rows=5:6) # distributions\npdf.(y_prob, \"positive\") # probabilities for \"positive\"\nlog_loss(y_prob, y[5:6])\n\n# point predictions:\nyhat = mode.(y_prob) # or `predict_mode(mach2, rows=5:6)`\n```\n\nSee also [`GaussianNBClassifier`](@ref)\n"
":name" = "MultinomialNBClassifier"
":human_name" = "multinomial naive Bayes classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:alpha,)`"
":hyperparameter_types" = "`(\"Int64\",)`"
":hyperparameter_ranges" = "`(nothing,)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MultivariateStats.LDA]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MultivariateStats"
":package_license" = "MIT"
":load_path" = "MLJMultivariateStatsInterface.LDA"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaStats/MultivariateStats.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLDA\n```\n\nA model type for constructing a linear discriminant analysis model, based on [unknown.jl](unknown), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nLDA = @load LDA pkg=unknown\n```\n\nDo `model = LDA()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `LDA(method=...)`.\n\n[Multiclass linear discriminant analysis](https://en.wikipedia.org/wiki/Linear_discriminant_analysis) learns a projection in a space of features to a lower dimensional space, in a way that attempts to preserve as much as possible the degree to which the classes of a discrete target variable can be discriminated. This can be used either for dimension reduction of the features (see `transform` below) or for probabilistic classification of the target (see `predict` below).\n\nIn the case of prediction, the class probability for a new observation reflects the proximity of that observation to training observations associated with that class, and how far away the observation is from observations associated with other classes. Specifically, the distances, in the transformed (projected) space, of a new observation, from the centroid of each target class, is computed; the resulting vector of distances, multiplied by minus one, is passed to a softmax function to obtain a class probability prediction. Here \"distance\" is computed using a user-specified distance function.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `OrderedFactor` or `Multiclass`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `method::Symbol=:gevd`: The solver, one of `:gevd` or `:whiten` methods.\n  * `cov_w::StatsBase.SimpleCovariance()`: An estimator for the within-class covariance (used in computing the within-class scatter matrix, `Sw`). Any robust estimator from `CovarianceEstimation.jl` can be used.\n  * `cov_b::StatsBase.SimpleCovariance()`: The same as `cov_w` but for the between-class covariance (used in computing the between-class scatter matrix, `Sb`).\n  * `outdim::Int=0`: The output dimension, i.e dimension of the transformed space, automatically set to `min(indim, nclasses-1)` if equal to 0.\n  * `regcoef::Float64=1e-6`: The regularization coefficient. A positive value `regcoef*eigmax(Sw)` where `Sw` is the within-class scatter matrix, is added to the diagonal of `Sw` to improve numerical stability. This can be useful if using the standard covariance estimator.\n  * `dist=Distances.SqEuclidean()`: The distance metric to use when performing classification (to compare the distance between a new point and centroids in the transformed space); must be a subtype of `Distances.SemiMetric` from Distances.jl, e.g., `Distances.CosineDist`.\n\n# Operations\n\n  * `transform(mach, Xnew)`: Return a lower dimensional projection of the input `Xnew`, which should have the same scitype as `X` above.\n  * `predict(mach, Xnew)`: Return predictions of the target given features `Xnew` having the same scitype as `X` above. Predictions are probabilistic but uncalibrated.\n  * `predict_mode(mach, Xnew)`: Return the modes of the probabilistic predictions returned above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `classes`: The classes seen during model fitting.\n  * `projection_matrix`: The learned projection matrix, of size `(indim, outdim)`, where `indim` and `outdim` are the input and output dimensions respectively (See Report section below).\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `indim`: The dimension of the input space i.e the number of training features.\n  * `outdim`: The dimension of the transformed space the model is projected to.\n  * `mean`: The mean of the untransformed training data. A vector of length `indim`.\n  * `nclasses`: The number of classes directly observed in the training data (which can be less than the total number of classes in the class pool).\n  * `class_means`: The class-specific means of the training data. A matrix of size `(indim, nclasses)` with the ith column being the class-mean of the ith class in `classes` (See fitted params section above).\n  * `class_weights`: The weights (class counts) of each class. A vector of length `nclasses` with the ith element being the class weight of the ith class in `classes`. (See fitted params section above.)\n  * `Sb`: The between class scatter matrix.\n  * `Sw`: The within class scatter matrix.\n\n# Examples\n\n```\nusing MLJ\n\nLDA = @load LDA pkg=MultivariateStats\n\nX, y = @load_iris # a table and a vector\n\nmodel = LDA()\nmach = machine(model, X, y) |> fit!\n\nXproj = transform(mach, X)\ny_hat = predict(mach, X)\nlabels = predict_mode(mach, X)\n\n```\n\nSee also [`BayesianLDA`](@ref), [`SubspaceLDA`](@ref), [`BayesianSubspaceLDA`](@ref)\n"
":name" = "LDA"
":human_name" = "linear discriminant analysis model"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":transform"]
":hyperparameters" = "`(:method, :cov_w, :cov_b, :outdim, :regcoef, :dist)`"
":hyperparameter_types" = "`(\"Symbol\", \"StatsBase.CovarianceEstimator\", \"StatsBase.CovarianceEstimator\", \"Int64\", \"Float64\", \"Distances.SemiMetric\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MultivariateStats.MultitargetLinearRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MultivariateStats"
":package_license" = "MIT"
":load_path" = "MLJMultivariateStatsInterface.MultitargetLinearRegressor"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaStats/MultivariateStats.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nMultitargetLinearRegressor\n```\n\nA model type for constructing a multitarget linear regressor, based on [unknown.jl](unknown), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nMultitargetLinearRegressor = @load MultitargetLinearRegressor pkg=unknown\n```\n\nDo `model = MultitargetLinearRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `MultitargetLinearRegressor(bias=...)`.\n\n`MultitargetLinearRegressor` assumes the target variable is vector-valued with continuous components.  It trains a linear prediction function using the least squares algorithm. Options exist to specify a bias term.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype    `Continuous`; check column scitypes with `schema(X)`.\n  * `y` is the target, which can be any table of responses whose element scitype is    `Continuous`; check the scitype with `scitype(y)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `bias=true`: Include the bias term if true, otherwise fit without bias term.\n\n# Operations\n\n  * `predict(mach, Xnew)`: Return predictions of the target given new features `Xnew`,    which should have the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `coefficients`: The linear coefficients determined by the model.\n  * `intercept`: The intercept determined by the model.\n\n# Examples\n\n```\nusing MLJ\nusing DataFrames\n\nLinearRegressor = @load MultitargetLinearRegressor pkg=MultivariateStats\nlinear_regressor = LinearRegressor()\n\nX, y = make_regression(100, 9; n_targets = 2) # a table and a table (synthetic data)\n\nmach = machine(linear_regressor, X, y) |> fit!\n\nXnew, _ = make_regression(3, 9)\nyhat = predict(mach, Xnew) # new predictions\n```\n\nSee also [`LinearRegressor`](@ref), [`RidgeRegressor`](@ref), [`MultitargetRidgeRegressor`](@ref)\n"
":name" = "MultitargetLinearRegressor"
":human_name" = "multitarget linear regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:bias,)`"
":hyperparameter_types" = "`(\"Bool\",)`"
":hyperparameter_ranges" = "`(nothing,)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MultivariateStats.BayesianSubspaceLDA]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MultivariateStats"
":package_license" = "MIT"
":load_path" = "MLJMultivariateStatsInterface.BayesianSubspaceLDA"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaStats/MultivariateStats.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nBayesianSubspaceLDA\n```\n\nA model type for constructing a Bayesian subspace LDA model, based on [unknown.jl](unknown), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nBayesianSubspaceLDA = @load BayesianSubspaceLDA pkg=unknown\n```\n\nDo `model = BayesianSubspaceLDA()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `BayesianSubspaceLDA(normalize=...)`.\n\nThe Bayesian multiclass subspace linear discriminant analysis algorithm learns a projection matrix as described in [`SubspaceLDA`](@ref). The posterior class probability distribution is derived as in [`BayesianLDA`](@ref).\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `OrderedFactor` or `Multiclass`; check the scitype with `scitype(y)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `normalize=true`: Option to normalize the between class variance for the number of observations in each class, one of `true` or `false`.\n\n`outdim`: the ouput dimension, automatically set to `min(indim, nclasses-1)` if equal   to `0`. If a non-zero `outdim` is passed, then the actual output dimension used is   `min(rank, outdim)` where `rank` is the rank of the within-class covariance matrix.\n\n  * `priors::Union{Nothing, UnivariateFinite{<:Any, <:Any, <:Any, <:Real}, Dict{<:Any, <:Real}} = nothing`: For use in prediction with Bayes rule. If `priors = nothing` then `priors` are estimated from the class proportions in the training data. Otherwise it requires a `Dict` or `UnivariateFinite` object specifying the classes with non-zero probabilities in the training target.\n\n# Operations\n\n  * `transform(mach, Xnew)`: Return a lower dimensional projection of the input `Xnew`, which should have the same scitype as `X` above.\n  * `predict(mach, Xnew)`: Return predictions of the target given features `Xnew`, which should have same scitype as `X` above. Predictions are probabilistic but uncalibrated.\n  * `predict_mode(mach, Xnew)`: Return the modes of the probabilistic predictions returned above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `classes`: The classes seen during model fitting.\n  * `projection_matrix`: The learned projection matrix, of size `(indim, outdim)`, where `indim` and `outdim` are the input and output dimensions respectively (See Report section below).\n  * `priors`: The class priors for classification. As inferred from training target `y`, if not user-specified. A `UnivariateFinite` object with levels consistent with `levels(y)`.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `indim`: The dimension of the input space i.e the number of training features.\n  * `outdim`: The dimension of the transformed space the model is projected to.\n  * `mean`: The overall mean of the training data.\n  * `nclasses`: The number of classes directly observed in the training data (which can be less than the total number of classes in the class pool).\n\n`class_means`: The class-specific means of the training data. A matrix of size   `(indim, nclasses)` with the ith column being the class-mean of the ith class in   `classes` (See fitted params section above).\n\n  * `class_weights`: The weights (class counts) of each class. A vector of length `nclasses` with the ith element being the class weight of the ith class in `classes`. (See fitted params section above.)\n  * `explained_variance_ratio`: The ratio of explained variance to total variance. Each dimension corresponds to an eigenvalue.\n\n# Examples\n\n```\nusing MLJ\n\nBayesianSubspaceLDA = @load BayesianSubspaceLDA pkg=MultivariateStats\n\nX, y = @load_iris # a table and a vector\n\nmodel = BayesianSubspaceLDA()\nmach = machine(model, X, y) |> fit!\n\nXproj = transform(mach, X)\ny_hat = predict(mach, X)\nlabels = predict_mode(mach, X)\n```\n\nSee also [`LDA`](@ref), [`BayesianLDA`](@ref), [`SubspaceLDA`](@ref)\n"
":name" = "BayesianSubspaceLDA"
":human_name" = "Bayesian subspace LDA model"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":transform"]
":hyperparameters" = "`(:normalize, :outdim, :priors)`"
":hyperparameter_types" = "`(\"Bool\", \"Int64\", \"Union{Nothing, Dict{<:Any, <:Real}, CategoricalDistributions.UnivariateFinite{<:Any, <:Any, <:Any, <:Real}}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MultivariateStats.FactorAnalysis]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "MultivariateStats"
":package_license" = "MIT"
":load_path" = "MLJMultivariateStatsInterface.FactorAnalysis"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaStats/MultivariateStats.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nFactorAnalysis\n```\n\nA model type for constructing a factor analysis model, based on [unknown.jl](unknown), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nFactorAnalysis = @load FactorAnalysis pkg=unknown\n```\n\nDo `model = FactorAnalysis()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `FactorAnalysis(method=...)`.\n\nFactor analysis is a linear-Gaussian latent variable model that is closely related to probabilistic PCA. In contrast to the probabilistic PCA model, the covariance of  conditional distribution of the observed variable given the latent variable is diagonal  rather than isotropic.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns   are of scitype `Continuous`; check column scitypes with `schema(X)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `method::Symbol=:cm`: Method to use to solve the problem, one of `:ml`, `:em`, `:bayes`.\n  * `maxoutdim=0`: Controls the the dimension (number of columns) of the output,   `outdim`. Specifically, `outdim = min(n, indim, maxoutdim)`, where `n` is the number of   observations and `indim` the input dimension.\n  * `maxiter::Int=1000`: Maximum number of iterations.\n  * `tol::Real=1e-6`: Convergence tolerance.\n  * `eta::Real=tol`: Variance lower bound.\n  * `mean::Union{Nothing, Real, Vector{Float64}}=nothing`: If `nothing`, centering will be   computed and applied; if set to `0` no centering is applied (data is assumed   pre-centered); if a vector, the centering is done with that vector.\n\n# Operations\n\n  * `transform(mach, Xnew)`: Return a lower dimensional projection of the input `Xnew`, which should have the same scitype as `X` above.\n  * `inverse_transform(mach, Xsmall)`: For a dimension-reduced table `Xsmall`, such as returned by `transform`, reconstruct a table, having same the number of columns as the original training data `X`, that transforms to `Xsmall`. Mathematically, `inverse_transform` is a right-inverse for the PCA projection map, whose image is orthogonal to the kernel of that map. In particular, if `Xsmall = transform(mach, Xnew)`, then `inverse_transform(Xsmall)` is only an approximation to `Xnew`.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `projection`: Returns the projection matrix, which has size `(indim, outdim)`, where `indim` and `outdim` are the number of features of the input and ouput respectively. Each column of the projection matrix corresponds to a factor.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `indim`: Dimension (number of columns) of the training data and new data to be  transformed.\n  * `outdim`: Dimension of transformed data (number of factors).\n  * `variance`: The variance of the factors.\n  * `covariance_matrix`: The estimated covariance matrix.\n  * `mean`: The mean of the untransformed training data, of length `indim`.\n  * `loadings`: The factor loadings.\n\n# Examples\n\n```\nusing MLJ\n\nFactorAnalysis = @load FactorAnalysis pkg=MultivariateStats\n\nX, y = @load_iris # a table and a vector\n\nmodel = FactorAnalysis(maxoutdim=2)\nmach = machine(model, X) |> fit!\n\nXproj = transform(mach, X)\n```\n\nSee also [`KernelPCA`](@ref), [`ICA`](@ref), [`PPCA`](@ref), [`PCA`](@ref)\n"
":name" = "FactorAnalysis"
":human_name" = "factor analysis model"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":inverse_transform", ":transform"]
":hyperparameters" = "`(:method, :maxoutdim, :maxiter, :tol, :eta, :mean)`"
":hyperparameter_types" = "`(\"Symbol\", \"Int64\", \"Int64\", \"Real\", \"Real\", \"Union{Nothing, Real, Vector{Float64}}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MultivariateStats.LinearRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MultivariateStats"
":package_license" = "MIT"
":load_path" = "MLJMultivariateStatsInterface.LinearRegressor"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaStats/MultivariateStats.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLinearRegressor\n```\n\nA model type for constructing a linear regressor, based on [unknown.jl](unknown), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nLinearRegressor = @load LinearRegressor pkg=unknown\n```\n\nDo `model = LinearRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `LinearRegressor(bias=...)`.\n\n`LinearRegressor` assumes the target is a `Continuous` variable and trains a linear prediction function using the least squares algorithm. Options exist to specify a bias  term.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype    `Continuous`; check the column scitypes with `schema(X)`.\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is    `Continuous`; check the scitype with `scitype(y)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `bias=true`: Include the bias term if true, otherwise fit without bias term.\n\n# Operations\n\n  * `predict(mach, Xnew)`: Return predictions of the target given new features `Xnew`, which    should have the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `coefficients`: The linear coefficients determined by the model.\n  * `intercept`: The intercept determined by the model.\n\n# Examples\n\n```\nusing MLJ\n\nLinearRegressor = @load LinearRegressor pkg=MultivariateStats\nlinear_regressor = LinearRegressor()\n\nX, y = make_regression(100, 2) # a table and a vector (synthetic data)\nmach = machine(linear_regressor, X, y) |> fit!\n\nXnew, _ = make_regression(3, 2)\nyhat = predict(mach, Xnew) # new predictions\n```\n\nSee also [`MultitargetLinearRegressor`](@ref), [`RidgeRegressor`](@ref), [`MultitargetRidgeRegressor`](@ref)\n"
":name" = "LinearRegressor"
":human_name" = "linear regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:bias,)`"
":hyperparameter_types" = "`(\"Bool\",)`"
":hyperparameter_ranges" = "`(nothing,)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MultivariateStats.ICA]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "MultivariateStats"
":package_license" = "MIT"
":load_path" = "MLJMultivariateStatsInterface.ICA"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaStats/MultivariateStats.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nICA\n```\n\nA model type for constructing a independent component analysis model, based on [unknown.jl](unknown), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nICA = @load ICA pkg=unknown\n```\n\nDo `model = ICA()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `ICA(outdim=...)`.\n\nIndependent component analysis is a computational technique for separating a multivariate signal into additive subcomponents, with the assumption that the subcomponents are non-Gaussian and independent from each other.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `outdim::Int=0`: The number of independent components to recover, set automatically  if `0`.\n  * `alg::Symbol=:fastica`: The algorithm to use (only `:fastica` is supported at the  moment).\n  * `fun::Symbol=:tanh`: The approximate neg-entropy function, one of `:tanh`, `:gaus`.\n  * `do_whiten::Bool=true`: Whether or not to perform pre-whitening.\n  * `maxiter::Int=100`: The maximum number of iterations.\n  * `tol::Real=1e-6`: The convergence tolerance for change in the unmixing matrix W.\n  * `mean::Union{Nothing, Real, Vector{Float64}}=nothing`: mean to use, if nothing (default) centering is computed and applied, if zero, no centering; otherwise a vector of means  can be passed.\n  * `winit::Union{Nothing,Matrix{<:Real}}=nothing`: Initial guess for the unmixing matrix  `W`: either an empty matrix (for random initialization of `W`), a matrix of size  `m × k` (if `do_whiten` is true), or a matrix of size `m × k`. Here `m` is the number  of components (columns) of the input.\n\n# Operations\n\n  * `transform(mach, Xnew)`: Return the component-separated version of input `Xnew`, which  should have the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `projection`: The estimated component matrix.\n  * `mean`: The estimated mean vector.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `indim`: Dimension (number of columns) of the training data and new data to be  transformed.\n  * `outdim`: Dimension of transformed data.\n  * `mean`: The mean of the untransformed training data, of length `indim`.\n\n# Examples\n\n```\nusing MLJ\n\nICA = @load ICA pkg=MultivariateStats\n\ntimes = range(0, 8, length=2000)\n\nsine_wave = sin.(2*times)\nsquare_wave = sign.(sin.(3*times))\nsawtooth_wave = map(t -> mod(2t, 2) - 1, times)\nsignals = hcat(sine_wave, square_wave, sawtooth_wave)\nnoisy_signals = signals + 0.2*randn(size(signals))\n\nmixing_matrix = [ 1 1 1; 0.5 2 1; 1.5 1 2]\nX = MLJ.table(noisy_signals*mixing_matrix)\n\nmodel = ICA(outdim = 3, tol=0.1)\nmach = machine(model, X) |> fit!\n\nX_unmixed = transform(mach, X)\n\nusing Plots\n\nplot(X.x2)\nplot(X.x2)\nplot(X.x3)\n\nplot(X_unmixed.x1)\nplot(X_unmixed.x2)\nplot(X_unmixed.x3)\n\n```\n\nSee also [`PCA`](@ref), [`KernelPCA`](@ref), [`FactorAnalysis`](@ref), [`PPCA`](@ref)\n"
":name" = "ICA"
":human_name" = "independent component analysis model"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":inverse_transform", ":transform"]
":hyperparameters" = "`(:outdim, :alg, :fun, :do_whiten, :maxiter, :tol, :winit, :mean)`"
":hyperparameter_types" = "`(\"Int64\", \"Symbol\", \"Symbol\", \"Bool\", \"Int64\", \"Real\", \"Union{Nothing, Matrix{<:Real}}\", \"Union{Nothing, Real, Vector{Float64}}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MultivariateStats.PPCA]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "MultivariateStats"
":package_license" = "MIT"
":load_path" = "MLJMultivariateStatsInterface.PPCA"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaStats/MultivariateStats.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nPPCA\n```\n\nA model type for constructing a probabilistic PCA model, based on [unknown.jl](unknown), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nPPCA = @load PPCA pkg=unknown\n```\n\nDo `model = PPCA()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `PPCA(maxoutdim=...)`.\n\nProbabilistic principal component analysis is a dimension-reduction algorithm which represents a constrained form of the Gaussian distribution in which the number of free parameters can be restricted while still allowing the model to capture the dominant correlations in a data set. It is expressed as the maximum likelihood solution of a probabilistic latent variable model. For details, see Bishop (2006): C. M. Pattern Recognition and Machine Learning.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `maxoutdim=0`: Controls the the dimension (number of columns) of the output, `outdim`. Specifically, `outdim = min(n, indim, maxoutdim)`, where `n` is the number of observations and `indim` the input dimension.\n  * `method::Symbol=:ml`: The method to use to solve the problem, one of `:ml`, `:em`, `:bayes`.\n  * `maxiter::Int=1000`: The maximum number of iterations.\n  * `tol::Real=1e-6`: The convergence tolerance.\n  * `mean::Union{Nothing, Real, Vector{Float64}}=nothing`: If `nothing`, centering will be computed and applied; if set to `0` no centering is applied (data is assumed pre-centered); if a vector, the centering is done with that vector.\n\n# Operations\n\n  * `transform(mach, Xnew)`: Return a lower dimensional projection of the input `Xnew`, which should have the same scitype as `X` above.\n  * `inverse_transform(mach, Xsmall)`: For a dimension-reduced table `Xsmall`, such as returned by `transform`, reconstruct a table, having same the number of columns as the original training data `X`, that transforms to `Xsmall`. Mathematically, `inverse_transform` is a right-inverse for the PCA projection map, whose image is orthogonal to the kernel of that map. In particular, if `Xsmall = transform(mach, Xnew)`, then `inverse_transform(Xsmall)` is only an  approximation to `Xnew`.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `projection`: Returns the projection matrix, which has size `(indim, outdim)`, where `indim` and `outdim` are the number of features of the input and ouput respectively. Each column of the projection matrix corresponds to a principal component.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `indim`: Dimension (number of columns) of the training data and new data to be  transformed.\n  * `outdim`: Dimension of transformed data.\n  * `tvat`: The variance of the components.\n  * `loadings`: The models loadings, weights for each variable used when calculating  principal components.\n\n# Examples\n\n```\nusing MLJ\n\nPPCA = @load PPCA pkg=MultivariateStats\n\nX, y = @load_iris # a table and a vector\n\nmodel = PPCA(maxoutdim=2)\nmach = machine(model, X) |> fit!\n\nXproj = transform(mach, X)\n```\n\nSee also [`KernelPCA`](@ref), [`ICA`](@ref), [`FactorAnalysis`](@ref), [`PCA`](@ref)\n"
":name" = "PPCA"
":human_name" = "probabilistic PCA model"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":inverse_transform", ":transform"]
":hyperparameters" = "`(:maxoutdim, :method, :maxiter, :tol, :mean)`"
":hyperparameter_types" = "`(\"Int64\", \"Symbol\", \"Int64\", \"Real\", \"Union{Nothing, Real, Vector{Float64}}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MultivariateStats.RidgeRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MultivariateStats"
":package_license" = "MIT"
":load_path" = "MLJMultivariateStatsInterface.RidgeRegressor"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaStats/MultivariateStats.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nRidgeRegressor\n```\n\nA model type for constructing a ridge regressor, based on [unknown.jl](unknown), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nRidgeRegressor = @load RidgeRegressor pkg=unknown\n```\n\nDo `model = RidgeRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `RidgeRegressor(lambda=...)`.\n\n`RidgeRegressor` adds a quadratic penalty term to least squares regression, for regularization. Ridge regression is particularly useful in the case of multicollinearity. Options exist to specify a bias term, and to adjust the strength of the penalty term.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype    `Continuous`; check column scitypes with `schema(X)`.\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is    `Continuous`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `lambda=1.0`: Is the non-negative parameter for the regularization strength. If lambda    is 0, ridge regression is equivalent to linear least squares regression, and as lambda    approaches infinity, all the linear coefficients approach 0.\n  * `bias=true`: Include the bias term if true, otherwise fit without bias term.\n\n# Operations\n\n  * `predict(mach, Xnew)`: Return predictions of the target given new features `Xnew`, which    should have the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `coefficients`: The linear coefficients determined by the model.\n  * `intercept`: The intercept determined by the model.\n\n# Examples\n\n```\nusing MLJ\n\nRidgeRegressor = @load RidgeRegressor pkg=MultivariateStats\npipe = Standardizer() |> RidgeRegressor(lambda=10)\n\nX, y = @load_boston\n\nmach = machine(pipe, X, y) |> fit!\nyhat = predict(mach, X)\ntraining_error = l1(yhat, y) |> mean\n```\n\nSee also [`LinearRegressor`](@ref), [`MultitargetLinearRegressor`](@ref), [`MultitargetRidgeRegressor`](@ref)\n"
":name" = "RidgeRegressor"
":human_name" = "ridge regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:lambda, :bias)`"
":hyperparameter_types" = "`(\"Union{Real, AbstractVecOrMat}\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MultivariateStats.KernelPCA]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "MultivariateStats"
":package_license" = "MIT"
":load_path" = "MLJMultivariateStatsInterface.KernelPCA"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaStats/MultivariateStats.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nKernelPCA\n```\n\nA model type for constructing a kernel prinicipal component analysis model, based on [unknown.jl](unknown), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nKernelPCA = @load KernelPCA pkg=unknown\n```\n\nDo `model = KernelPCA()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `KernelPCA(maxoutdim=...)`.\n\nIn kernel PCA the linear operations of ordinary principal component analysis are performed in a [reproducing Hilbert space](https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space).\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `maxoutdim=0`: Controls the the dimension (number of columns) of the output, `outdim`. Specifically, `outdim = min(n, indim, maxoutdim)`, where `n` is the number of observations and `indim` the input dimension.\n  * `kernel::Function=(x,y)->x'y`: The kernel function, takes in 2 vector arguments x and y, returns a scalar value. Defaults to the dot product of `x` and `y`.\n  * `solver::Symbol=:eig`: solver to use for the eigenvalues, one of `:eig`(default, uses `LinearAlgebra.eigen`), `:eigs`(uses `Arpack.eigs`).\n  * `inverse::Bool=true`: perform calculations needed for inverse transform\n  * `beta::Real=1.0`: strength of the ridge regression that learns the inverse transform when inverse is true.\n  * `tol::Real=0.0`: Convergence tolerance for eigenvalue solver.\n  * `maxiter::Int=300`: maximum number of iterations for eigenvalue solver.\n\n# Operations\n\n  * `transform(mach, Xnew)`: Return a lower dimensional projection of the input `Xnew`, which   should have the same scitype as `X` above.\n  * `inverse_transform(mach, Xsmall)`: For a dimension-reduced table `Xsmall`, such as returned by `transform`, reconstruct a table, having same the number of columns as the original training data `X`, that transforms to `Xsmall`.  Mathematically, `inverse_transform` is a right-inverse for the PCA projection map, whose image is orthogonal to the kernel of that map. In particular, if  `Xsmall = transform(mach, Xnew)`, then `inverse_transform(Xsmall)` is only an  approximation to `Xnew`.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `projection`: Returns the projection matrix, which has size `(indim, outdim)`, where `indim` and `outdim` are the number of features of the input and ouput respectively.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `indim`: Dimension (number of columns) of the training data and new data to be transformed.\n  * `outdim`: Dimension of transformed data.\n  * `principalvars`: The variance of the principal components.\n\n# Examples\n\n```\nusing MLJ\nusing LinearAlgebra\n\nKernelPCA = @load KernelPCA pkg=MultivariateStats\n\nX, y = @load_iris # a table and a vector\n\nfunction rbf_kernel(length_scale)\n    return (x,y) -> norm(x-y)^2 / ((2 * length_scale)^2)\nend\n\nmodel = KernelPCA(maxoutdim=2, kernel=rbf_kernel(1))\nmach = machine(model, X) |> fit!\n\nXproj = transform(mach, X)\n```\n\nSee also [`PCA`](@ref), [`ICA`](@ref), [`FactorAnalysis`](@ref), [`PPCA`](@ref)\n"
":name" = "KernelPCA"
":human_name" = "kernel prinicipal component analysis model"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":inverse_transform", ":transform"]
":hyperparameters" = "`(:maxoutdim, :kernel, :solver, :inverse, :beta, :tol, :maxiter)`"
":hyperparameter_types" = "`(\"Int64\", \"Union{Nothing, Function}\", \"Symbol\", \"Bool\", \"Real\", \"Real\", \"Int64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MultivariateStats.MultitargetRidgeRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MultivariateStats"
":package_license" = "MIT"
":load_path" = "MLJMultivariateStatsInterface.MultitargetRidgeRegressor"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaStats/MultivariateStats.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nMultitargetRidgeRegressor\n```\n\nA model type for constructing a multitarget ridge regressor, based on [unknown.jl](unknown), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nMultitargetRidgeRegressor = @load MultitargetRidgeRegressor pkg=unknown\n```\n\nDo `model = MultitargetRidgeRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `MultitargetRidgeRegressor(lambda=...)`.\n\nMulti-target ridge regression adds a quadratic penalty term to multi-target least squares regression, for regularization. Ridge regression is particularly useful in the case of multicollinearity. In this case, the output represents a response vector. Options exist to specify a bias term, and to adjust the strength of the penalty term.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype    `Continuous`; check column scitypes with `schema(X)`.\n  * `y` is the target, which can be any table of responses whose element scitype is    `Continuous`; check the scitype with `scitype(y)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `lambda=1.0`: Is the non-negative parameter for the regularization strength. If lambda    is 0, ridge regression is equivalent to linear least squares regression, and as lambda    approaches infinity, all the linear coefficients approach 0.\n  * `bias=true`: Include the bias term if true, otherwise fit without bias term.\n\n# Operations\n\n  * `predict(mach, Xnew)`: Return predictions of the target given new features `Xnew`, which    should have the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `coefficients`: The linear coefficients determined by the model.\n  * `intercept`: The intercept determined by the model.\n\n# Examples\n\n```\nusing MLJ\nusing DataFrames\n\nRidgeRegressor = @load MultitargetRidgeRegressor pkg=MultivariateStats\n\nX, y = make_regression(100, 6; n_targets = 2)  # a table and a table (synthetic data)\n\nridge_regressor = RidgeRegressor(lambda=1.5)\nmach = machine(ridge_regressor, X, y) |> fit!\n\nXnew, _ = make_regression(3, 6)\nyhat = predict(mach, Xnew) # new predictions\n```\n\nSee also [`LinearRegressor`](@ref), [`MultitargetLinearRegressor`](@ref), [`RidgeRegressor`](@ref)\n"
":name" = "MultitargetRidgeRegressor"
":human_name" = "multitarget ridge regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:lambda, :bias)`"
":hyperparameter_types" = "`(\"Union{Real, AbstractVecOrMat}\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MultivariateStats.SubspaceLDA]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MultivariateStats"
":package_license" = "MIT"
":load_path" = "MLJMultivariateStatsInterface.SubspaceLDA"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaStats/MultivariateStats.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nSubspaceLDA\n```\n\nA model type for constructing a subpace LDA model, based on [unknown.jl](unknown), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nSubspaceLDA = @load SubspaceLDA pkg=unknown\n```\n\nDo `model = SubspaceLDA()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `SubspaceLDA(normalize=...)`.\n\nMulticlass subspace linear discriminant analysis (LDA) is a variation on ordinary [`LDA`](@ref) suitable for high dimensional data, as it avoids storing scatter matrices. For details, refer the [MultivariateStats.jl documentation](https://juliastats.org/MultivariateStats.jl/stable/).\n\nIn addition to dimension reduction (using `transform`) probabilistic classification is provided (using `predict`).  In the case of classification, the class probability for a new observation reflects the proximity of that observation to training observations associated with that class, and how far away the observation is from observations associated with other classes. Specifically, the distances, in the transformed (projected) space, of a new observation, from the centroid of each target class, is computed; the resulting vector of distances, multiplied by minus one, is passed to a softmax function to obtain a class probability prediction. Here \"distance\" is computed using a user-specified distance function.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `OrderedFactor` or `Multiclass`; check the scitype with `scitype(y)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `normalize=true`: Option to normalize the between class variance for the number of observations in each class, one of `true` or `false`.\n  * `outdim`: the ouput dimension, automatically set to `min(indim, nclasses-1)` if equal to `0`. If a non-zero `outdim` is passed, then the actual output dimension used is `min(rank, outdim)` where `rank` is the rank of the within-class covariance matrix.\n  * `dist=Distances.SqEuclidean()`: The distance metric to use when performing classification (to compare the distance between a new point and centroids in the transformed space); must be a subtype of `Distances.SemiMetric` from Distances.jl, e.g., `Distances.CosineDist`.\n\n# Operations\n\n  * `transform(mach, Xnew)`: Return a lower dimensional projection of the input `Xnew`, which should have the same scitype as `X` above.\n  * `predict(mach, Xnew)`: Return predictions of the target given features `Xnew`, which should have same scitype as `X` above. Predictions are probabilistic but uncalibrated.\n  * `predict_mode(mach, Xnew)`: Return the modes of the probabilistic predictions returned above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `classes`: The classes seen during model fitting.\n  * `projection_matrix`: The learned projection matrix, of size `(indim, outdim)`, where `indim` and `outdim` are the input and output dimensions respectively (See Report section below).\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `indim`: The dimension of the input space i.e the number of training features.\n  * `outdim`: The dimension of the transformed space the model is projected to.\n  * `mean`: The mean of the untransformed training data. A vector of length `indim`.\n  * `nclasses`: The number of classes directly observed in the training data (which can be less than the total number of classes in the class pool)\n\n`class_means`: The class-specific means of the training data. A matrix of size   `(indim, nclasses)` with the ith column being the class-mean of the ith class in   `classes` (See fitted params section above).\n\n  * `class_weights`: The weights (class counts) of each class. A vector of length `nclasses` with the ith element being the class weight of the ith class in `classes`. (See fitted params section above.)\n  * `explained_variance_ratio`: The ratio of explained variance to total variance. Each dimension corresponds to an eigenvalue.\n\n# Examples\n\n```\nusing MLJ\n\nSubspaceLDA = @load SubspaceLDA pkg=MultivariateStats\n\nX, y = @load_iris # a table and a vector\n\nmodel = SubspaceLDA()\nmach = machine(model, X, y) |> fit!\n\nXproj = transform(mach, X)\ny_hat = predict(mach, X)\nlabels = predict_mode(mach, X)\n```\n\nSee also [`LDA`](@ref), [`BayesianLDA`](@ref), [`BayesianSubspaceLDA`](@ref)\n"
":name" = "SubspaceLDA"
":human_name" = "subpace LDA model"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":transform"]
":hyperparameters" = "`(:normalize, :outdim, :dist)`"
":hyperparameter_types" = "`(\"Bool\", \"Int64\", \"Distances.SemiMetric\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MultivariateStats.BayesianLDA]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MultivariateStats"
":package_license" = "MIT"
":load_path" = "MLJMultivariateStatsInterface.BayesianLDA"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaStats/MultivariateStats.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nBayesianLDA\n```\n\nA model type for constructing a Bayesian LDA model, based on [unknown.jl](unknown), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nBayesianLDA = @load BayesianLDA pkg=unknown\n```\n\nDo `model = BayesianLDA()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `BayesianLDA(method=...)`.\n\nThe Bayesian multiclass LDA algorithm learns a projection matrix as described in ordinary [`LDA`](@ref).  Predicted class posterior probability distributions are derived by applying Bayes' rule with a multivariate Gaussian class-conditional distribution. A prior class distribution can be specified by the user or inferred from training data class frequency.\n\nSee also the [package documentation](https://multivariatestatsjl.readthedocs.io/en/latest/lda.html).  For more information about the algorithm, see [Li, Zhu and Ogihara (2006): Using Discriminant Analysis for Multi-class Classification: An Experimental Investigation](https://doi.org/10.1007/s10115-006-0013-y).\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `OrderedFactor` or `Multiclass`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `method::Symbol=:gevd`: choice of solver, one of `:gevd` or `:whiten` methods.\n  * `cov_w::StatsBase.SimpleCovariance()`: An estimator for the within-class covariance (used in computing the within-class scatter matrix, `Sw`). Any robust estimator from `CovarianceEstimation.jl` can be used.\n  * `cov_b::StatsBase.SimpleCovariance()`: The same as `cov_w` but for the between-class covariance (used in computing the between-class scatter matrix, `Sb`).\n  * `outdim::Int=0`: The output dimension, i.e., dimension of the transformed space, automatically set to `min(indim, nclasses-1)` if equal to 0.\n  * `regcoef::Float64=1e-6`: The regularization coefficient. A positive value `regcoef*eigmax(Sw)` where `Sw` is the within-class scatter matrix, is added to the diagonal of `Sw` to improve numerical stability. This can be useful if using the standard covariance estimator.\n  * `priors::Union{Nothing, UnivariateFinite{<:Any, <:Any, <:Any, <:Real}, Dict{<:Any, <:Real}} = nothing`: For use in prediction with Bayes rule. If `priors = nothing` then `priors` are estimated from the class proportions in the training data. Otherwise it requires a `Dict` or `UnivariateFinite` object specifying the classes with non-zero probabilities in the training target.\n\n# Operations\n\n  * `transform(mach, Xnew)`: Return a lower dimensional projection of the input `Xnew`, which should have the same scitype as `X` above.\n  * `predict(mach, Xnew)`: Return predictions of the target given features `Xnew`, which should have the same scitype as `X` above. Predictions are probabilistic but uncalibrated.\n  * `predict_mode(mach, Xnew)`: Return the modes of the probabilistic predictions returned above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `classes`: The classes seen during model fitting.\n  * `projection_matrix`: The learned projection matrix, of size `(indim, outdim)`, where `indim` and `outdim` are the input and output dimensions respectively (See Report section below).\n  * `priors`: The class priors for classification. As inferred from training target `y`, if not user-specified. A `UnivariateFinite` object with levels consistent with `levels(y)`.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `indim`: The dimension of the input space i.e the number of training features.\n  * `outdim`: The dimension of the transformed space the model is projected to.\n  * `mean`: The mean of the untransformed training data. A vector of length `indim`.\n  * `nclasses`: The number of classes directly observed in the training data (which can be less than the total number of classes in the class pool).\n  * `class_means`: The class-specific means of the training data. A matrix of size `(indim, nclasses)` with the ith column being the class-mean of the ith class in `classes` (See fitted params section above).\n  * `class_weights`: The weights (class counts) of each class. A vector of length `nclasses` with the ith element being the class weight of the ith class in `classes`. (See fitted params section above.)\n  * `Sb`: The between class scatter matrix.\n  * `Sw`: The within class scatter matrix.\n\n# Examples\n\n```\nusing MLJ\n\nBayesianLDA = @load BayesianLDA pkg=MultivariateStats\n\nX, y = @load_iris # a table and a vector\n\nmodel = BayesianLDA()\nmach = machine(model, X, y) |> fit!\n\nXproj = transform(mach, X)\ny_hat = predict(mach, X)\nlabels = predict_mode(mach, X)\n```\n\nSee also [`LDA`](@ref), [`SubspaceLDA`](@ref), [`BayesianSubspaceLDA`](@ref)\n"
":name" = "BayesianLDA"
":human_name" = "Bayesian LDA model"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":transform"]
":hyperparameters" = "`(:method, :cov_w, :cov_b, :outdim, :regcoef, :priors)`"
":hyperparameter_types" = "`(\"Symbol\", \"StatsBase.CovarianceEstimator\", \"StatsBase.CovarianceEstimator\", \"Int64\", \"Float64\", \"Union{Nothing, Dict{<:Any, <:Real}, CategoricalDistributions.UnivariateFinite{<:Any, <:Any, <:Any, <:Real}}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MultivariateStats.PCA]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "MultivariateStats"
":package_license" = "MIT"
":load_path" = "MLJMultivariateStatsInterface.PCA"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaStats/MultivariateStats.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nPCA\n```\n\nA model type for constructing a pca, based on [unknown.jl](unknown), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nPCA = @load PCA pkg=unknown\n```\n\nDo `model = PCA()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `PCA(maxoutdim=...)`.\n\nPrincipal component analysis learns a linear projection onto a lower dimensional space  while preserving most of the initial variance seen in the training data.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `maxoutdim=0`: Together with `variance_ratio`, controls the output dimension `outdim` chosen by the model. Specifically, suppose that `k` is the smallest integer such that retaining the `k` most significant principal components accounts for `variance_ratio` of the total variance in the training data. Then `outdim = min(outdim, maxoutdim)`. If `maxoutdim=0` (default) then the effective `maxoutdim` is `min(n, indim - 1)` where `n` is the number of observations and `indim` the number of features in the training data.\n  * `variance_ratio::Float64=0.99`: The ratio of variance preserved after the transformation\n  * `method=:auto`: The method to use to solve the problem. Choices are\n\n      * `:svd`: Support Vector Decomposition of the matrix.\n      * `:cov`: Covariance matrix decomposition.\n      * `:auto`: Use `:cov` if the matrices first dimension is smaller than its second dimension and otherwise use `:svd`\n  * `mean=nothing`: if `nothing`, centering will be computed and applied, if set to `0` no centering (data is assumed pre-centered); if a vector is passed, the centering is done with that vector.\n\n# Operations\n\n  * `transform(mach, Xnew)`: Return a lower dimensional projection of the input `Xnew`, which should have the same scitype as `X` above.\n  * `inverse_transform(mach, Xsmall)`: For a dimension-reduced table `Xsmall`, such as returned by `transform`, reconstruct a table, having same the number of columns as the original training data `X`, that transforms to `Xsmall`. Mathematically, `inverse_transform` is a right-inverse for the PCA projection map, whose image is orthogonal to the kernel of that map. In particular, if `Xsmall = transform(mach, Xnew)`, then `inverse_transform(Xsmall)` is only an approximation to `Xnew`.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `projection`: Returns the projection matrix, which has size `(indim, outdim)`, where `indim` and `outdim` are the number of features of the input and output respectively.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `indim`: Dimension (number of columns) of the training data and new data to be  transformed.\n  * `outdim = min(n, indim, maxoutdim)` is the output dimension; here `n` is the number of observations.\n  * `tprincipalvar`: Total variance of the principal components.\n  * `tresidualvar`: Total residual variance.\n  * `tvar`: Total observation variance (principal + residual variance).\n  * `mean`: The mean of the untransformed training data, of length `indim`.\n  * `principalvars`: The variance of the principal components.\n\n# Examples\n\n```\nusing MLJ\n\nPCA = @load PCA pkg=MultivariateStats\n\nX, y = @load_iris # a table and a vector\n\nmodel = PCA(maxoutdim=2)\nmach = machine(model, X) |> fit!\n\nXproj = transform(mach, X)\n```\n\nSee also [`KernelPCA`](@ref), [`ICA`](@ref), [`FactorAnalysis`](@ref), [`PPCA`](@ref)\n"
":name" = "PCA"
":human_name" = "pca"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":inverse_transform", ":transform"]
":hyperparameters" = "`(:maxoutdim, :method, :variance_ratio, :mean)`"
":hyperparameter_types" = "`(\"Int64\", \"Symbol\", \"Float64\", \"Union{Nothing, Real, Vector{Float64}}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[DecisionTree.AdaBoostStumpClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "DecisionTree"
":package_license" = "MIT"
":load_path" = "MLJDecisionTreeInterface.AdaBoostStumpClassifier"
":package_uuid" = "7806a523-6efd-50cb-b5f6-3fa6f1930dbb"
":package_url" = "https://github.com/bensadeghi/DecisionTree.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nAdaBoostStumpClassifier\n```\n\nA model type for constructing a Ada-boosted stump classifier, based on [DecisionTree.jl](https://github.com/bensadeghi/DecisionTree.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nAdaBoostStumpClassifier = @load AdaBoostStumpClassifier pkg=DecisionTree\n```\n\nDo `model = AdaBoostStumpClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `AdaBoostStumpClassifier(n_iter=...)`.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere:\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have one of the following element scitypes: `Continuous`, `Count`, or `<:OrderedFactor`; check column scitypes with `schema(X)`\n  * `y`: the target, which can be any `AbstractVector` whose element scitype is `<:OrderedFactor` or `<:Multiclass`; check the scitype with `scitype(y)`\n\nTrain the machine with `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `n_iter=10`:   number of iterations of AdaBoost\n  * `feature_importance`: method to use for computing feature importances. One of `(:impurity, :split)`\n  * `rng=Random.GLOBAL_RNG`: random number generator or seed\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given features `Xnew` having the same scitype as `X` above. Predictions are probabilistic, but uncalibrated.\n  * `predict_mode(mach, Xnew)`: instead return the mode of each prediction above.\n\n# Fitted Parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `stumps`: the `Ensemble` object returned by the core DecisionTree.jl algorithm.\n  * `coefficients`: the stump coefficients (one per stump)\n\n# Report\n\n  * `features`: the names of the features encountered in training\n\n```\nusing MLJ\nBooster = @load AdaBoostStumpClassifier pkg=DecisionTree\nbooster = Booster(n_iter=15)\n\nX, y = @load_iris\nmach = machine(booster, X, y) |> fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\nyhat = predict(mach, Xnew) # probabilistic predictions\npredict_mode(mach, Xnew)   # point predictions\npdf.(yhat, \"virginica\")    # probabilities for the \"verginica\" class\n\nfitted_params(mach).stumps # raw `Ensemble` object from DecisionTree.jl\nfitted_params(mach).coefs  # coefficient associated with each stump\n```\n\nSee also [DecisionTree.jl](https://github.com/bensadeghi/DecisionTree.jl) and the unwrapped model type [`MLJDecisionTreeInterface.DecisionTree.AdaBoostStumpClassifier`](@ref).\n"
":name" = "AdaBoostStumpClassifier"
":human_name" = "Ada-boosted stump classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":feature_importances"]
":hyperparameters" = "`(:n_iter, :feature_importance, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Symbol\", \"Union{Integer, Random.AbstractRNG}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`true`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[DecisionTree.DecisionTreeRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "DecisionTree"
":package_license" = "MIT"
":load_path" = "MLJDecisionTreeInterface.DecisionTreeRegressor"
":package_uuid" = "7806a523-6efd-50cb-b5f6-3fa6f1930dbb"
":package_url" = "https://github.com/bensadeghi/DecisionTree.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nDecisionTreeRegressor\n```\n\nA model type for constructing a CART decision tree regressor, based on [DecisionTree.jl](https://github.com/bensadeghi/DecisionTree.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nDecisionTreeRegressor = @load DecisionTreeRegressor pkg=DecisionTree\n```\n\nDo `model = DecisionTreeRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `DecisionTreeRegressor(max_depth=...)`.\n\n`DecisionTreeRegressor` implements the [CART algorithm](https://en.wikipedia.org/wiki/Decision_tree_learning), originally published in Breiman, Leo; Friedman, J. H.; Olshen, R. A.; Stone, C. J. (1984): \"Classification and regression trees\". *Monterey, CA: Wadsworth & Brooks/Cole Advanced Books & Software.*.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have one of the following element scitypes: `Continuous`, `Count`, or `<:OrderedFactor`; check column scitypes with `schema(X)`\n  * `y`: the target, which can be any `AbstractVector` whose element scitype is `Continuous`; check the scitype with `scitype(y)`\n\nTrain the machine with `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `max_depth=-1`:          max depth of the decision tree (-1=any)\n  * `min_samples_leaf=1`:    max number of samples each leaf needs to have\n  * `min_samples_split=2`:   min number of samples needed for a split\n  * `min_purity_increase=0`: min purity needed for a split\n  * `n_subfeatures=0`: number of features to select at random (0 for all, -1 for square root of number of features)\n  * `post_prune=false`:      set to `true` for post-fit pruning\n  * `merge_purity_threshold=1.0`: (post-pruning) merge leaves having                          combined purity `>= merge_purity_threshold`\n  * `feature_importance`:    method to use for computing feature importances. One of `(:impurity, :split)`\n  * `rng=Random.GLOBAL_RNG`: random number generator or seed\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given new features `Xnew` having the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `tree`: the tree or stump object returned by the core DecisionTree.jl algorithm\n\n# Report\n\n  * `features`: the names of the features encountered in training\n\n# Examples\n\n```\nusing MLJ\nTree = @load DecisionTreeRegressor pkg=DecisionTree\ntree = Tree(max_depth=4, min_samples_split=3)\n\nX, y = make_regression(100, 2) # synthetic data\nmach = machine(tree, X, y) |> fit!\n\nXnew, _ = make_regression(3, 2)\nyhat = predict(mach, Xnew) # new predictions\n\nfitted_params(mach).tree # raw tree or stump object from DecisionTree.jl\n```\n\nSee also [DecisionTree.jl](https://github.com/bensadeghi/DecisionTree.jl) and the unwrapped model type [`MLJDecisionTreeInterface.DecisionTree.DecisionTreeRegressor`](@ref).\n"
":name" = "DecisionTreeRegressor"
":human_name" = "CART decision tree regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":feature_importances"]
":hyperparameters" = "`(:max_depth, :min_samples_leaf, :min_samples_split, :min_purity_increase, :n_subfeatures, :post_prune, :merge_purity_threshold, :feature_importance, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Int64\", \"Int64\", \"Float64\", \"Int64\", \"Bool\", \"Float64\", \"Symbol\", \"Union{Integer, Random.AbstractRNG}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`true`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[DecisionTree.DecisionTreeClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "DecisionTree"
":package_license" = "MIT"
":load_path" = "MLJDecisionTreeInterface.DecisionTreeClassifier"
":package_uuid" = "7806a523-6efd-50cb-b5f6-3fa6f1930dbb"
":package_url" = "https://github.com/bensadeghi/DecisionTree.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nDecisionTreeClassifier\n```\n\nA model type for constructing a CART decision tree classifier, based on [DecisionTree.jl](https://github.com/bensadeghi/DecisionTree.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nDecisionTreeClassifier = @load DecisionTreeClassifier pkg=DecisionTree\n```\n\nDo `model = DecisionTreeClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `DecisionTreeClassifier(max_depth=...)`.\n\n`DecisionTreeClassifier` implements the [CART algorithm](https://en.wikipedia.org/wiki/Decision_tree_learning), originally published in Breiman, Leo; Friedman, J. H.; Olshen, R. A.; Stone, C. J. (1984): \"Classification and regression trees\". *Monterey, CA: Wadsworth & Brooks/Cole Advanced Books & Software.*.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have one of the following element scitypes: `Continuous`, `Count`, or `<:OrderedFactor`; check column scitypes with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `<:OrderedFactor` or `<:Multiclass`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `max_depth=-1`:          max depth of the decision tree (-1=any)\n  * `min_samples_leaf=1`:    max number of samples each leaf needs to have\n  * `min_samples_split=2`:   min number of samples needed for a split\n  * `min_purity_increase=0`: min purity needed for a split\n  * `n_subfeatures=0`: number of features to select at random (0 for all, -1 for square root of number of features)\n  * `post_prune=false`:      set to `true` for post-fit pruning\n  * `merge_purity_threshold=1.0`: (post-pruning) merge leaves having                          combined purity `>= merge_purity_threshold`\n  * `display_depth=5`:       max depth to show when displaying the tree\n  * `feature_importance`: method to use for computing feature importances. One of `(:impurity, :split)`\n  * `rng=Random.GLOBAL_RNG`: random number generator or seed\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given features `Xnew` having the same scitype as `X` above. Predictions are probabilistic, but uncalibrated.\n  * `predict_mode(mach, Xnew)`: instead return the mode of each prediction above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `tree`: the tree or stump object returned by the core DecisionTree.jl algorithm\n  * `encoding`: dictionary of target classes keyed on integers used internally by DecisionTree.jl; needed to interpret pretty printing of tree (obtained by calling `fit!(mach, verbosity=2)` or from report - see below)\n  * `features`: the names of the features encountered in training, in an order consistent with the output of `print_tree` (see below)\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `classes_seen`: list of target classes actually observed in training\n  * `print_tree`: method to print a pretty representation of the fitted tree, with single argument the tree depth; interpretation requires internal integer-class encoding (see \"Fitted parameters\" above).\n  * `features`: the names of the features encountered in training, in an order consistent with the output of `print_tree` (see below)\n\n# Examples\n\n```\nusing MLJ\nTree = @load DecisionTreeClassifier pkg=DecisionTree\ntree = Tree(max_depth=4, min_samples_split=3)\n\nX, y = @load_iris\nmach = machine(tree, X, y) |> fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\nyhat = predict(mach, Xnew) # probabilistic predictions\npredict_mode(mach, Xnew)   # point predictions\npdf.(yhat, \"virginica\")    # probabilities for the \"verginica\" class\n\nfitted_params(mach).tree # raw tree or stump object from DecisionTrees.jl\n\njulia> report(mach).print_tree(3)\nFeature 4, Threshold 0.8\nL-> 1 : 50/50\nR-> Feature 4, Threshold 1.75\n    L-> Feature 3, Threshold 4.95\n        L->\n        R->\n    R-> Feature 3, Threshold 4.85\n        L->\n        R-> 3 : 43/43\n```\n\nTo interpret the internal class labelling:\n\n```\njulia> fitted_params(mach).encoding\nDict{CategoricalArrays.CategoricalValue{String, UInt32}, UInt32} with 3 entries:\n  \"virginica\"  => 0x00000003\n  \"setosa\"     => 0x00000001\n  \"versicolor\" => 0x00000002\n```\n\nSee also [DecisionTree.jl](https://github.com/bensadeghi/DecisionTree.jl) and the unwrapped model type [`MLJDecisionTreeInterface.DecisionTree.DecisionTreeClassifier`](@ref).\n"
":name" = "DecisionTreeClassifier"
":human_name" = "CART decision tree classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":feature_importances"]
":hyperparameters" = "`(:max_depth, :min_samples_leaf, :min_samples_split, :min_purity_increase, :n_subfeatures, :post_prune, :merge_purity_threshold, :display_depth, :feature_importance, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Int64\", \"Int64\", \"Float64\", \"Int64\", \"Bool\", \"Float64\", \"Int64\", \"Symbol\", \"Union{Integer, Random.AbstractRNG}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`true`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[DecisionTree.RandomForestRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "DecisionTree"
":package_license" = "MIT"
":load_path" = "MLJDecisionTreeInterface.RandomForestRegressor"
":package_uuid" = "7806a523-6efd-50cb-b5f6-3fa6f1930dbb"
":package_url" = "https://github.com/bensadeghi/DecisionTree.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nRandomForestRegressor\n```\n\nA model type for constructing a CART random forest regressor, based on [DecisionTree.jl](https://github.com/bensadeghi/DecisionTree.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nRandomForestRegressor = @load RandomForestRegressor pkg=DecisionTree\n```\n\nDo `model = RandomForestRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `RandomForestRegressor(max_depth=...)`.\n\n`DecisionTreeRegressor` implements the standard [Random Forest algorithm](https://en.wikipedia.org/wiki/Random_forest), originally published in Breiman, L. (2001): \"Random Forests.\", *Machine Learning*, vol. 45, pp. 5–32\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have one of the following element scitypes: `Continuous`, `Count`, or `<:OrderedFactor`; check column scitypes with `schema(X)`\n  * `y`: the target, which can be any `AbstractVector` whose element scitype is `Continuous`; check the scitype with `scitype(y)`\n\nTrain the machine with `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `max_depth=-1`:          max depth of the decision tree (-1=any)\n  * `min_samples_leaf=1`:    min number of samples each leaf needs to have\n  * `min_samples_split=2`:   min number of samples needed for a split\n  * `min_purity_increase=0`: min purity needed for a split\n  * `n_subfeatures=-1`: number of features to select at random (0 for all, -1 for square root of number of features)\n  * `n_trees=10`:            number of trees to train\n  * `sampling_fraction=0.7`  fraction of samples to train each tree on\n  * `feature_importance`:    method to use for computing feature importances. One of `(:impurity, :split)`\n  * `rng=Random.GLOBAL_RNG`: random number generator or seed\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given new features `Xnew` having the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `forest`: the `Ensemble` object returned by the core DecisionTree.jl algorithm\n\n# Report\n\n  * `features`: the names of the features encountered in training\n\n# Examples\n\n```\nusing MLJ\nForest = @load RandomForestRegressor pkg=DecisionTree\nforest = Forest(max_depth=4, min_samples_split=3)\n\nX, y = make_regression(100, 2) # synthetic data\nmach = machine(forest, X, y) |> fit!\n\nXnew, _ = make_regression(3, 2)\nyhat = predict(mach, Xnew) # new predictions\n\nfitted_params(mach).forest # raw `Ensemble` object from DecisionTree.jl\n```\n\nSee also [DecisionTree.jl](https://github.com/bensadeghi/DecisionTree.jl) and the unwrapped model type [`MLJDecisionTreeInterface.DecisionTree.RandomForestRegressor`](@ref).\n"
":name" = "RandomForestRegressor"
":human_name" = "CART random forest regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":feature_importances"]
":hyperparameters" = "`(:max_depth, :min_samples_leaf, :min_samples_split, :min_purity_increase, :n_subfeatures, :n_trees, :sampling_fraction, :feature_importance, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Int64\", \"Int64\", \"Float64\", \"Int64\", \"Int64\", \"Float64\", \"Symbol\", \"Union{Integer, Random.AbstractRNG}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`true`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[DecisionTree.RandomForestClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "DecisionTree"
":package_license" = "MIT"
":load_path" = "MLJDecisionTreeInterface.RandomForestClassifier"
":package_uuid" = "7806a523-6efd-50cb-b5f6-3fa6f1930dbb"
":package_url" = "https://github.com/bensadeghi/DecisionTree.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nRandomForestClassifier\n```\n\nA model type for constructing a CART random forest classifier, based on [DecisionTree.jl](https://github.com/bensadeghi/DecisionTree.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nRandomForestClassifier = @load RandomForestClassifier pkg=DecisionTree\n```\n\nDo `model = RandomForestClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `RandomForestClassifier(max_depth=...)`.\n\n`RandomForestClassifier` implements the standard [Random Forest algorithm](https://en.wikipedia.org/wiki/Random_forest), originally published in Breiman, L. (2001): \"Random Forests.\", *Machine Learning*, vol. 45, pp. 5–32.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have one of the following element scitypes: `Continuous`, `Count`, or `<:OrderedFactor`; check column scitypes with `schema(X)`\n  * `y`: the target, which can be any `AbstractVector` whose element scitype is `<:OrderedFactor` or `<:Multiclass`; check the scitype with `scitype(y)`\n\nTrain the machine with `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `max_depth=-1`:          max depth of the decision tree (-1=any)\n  * `min_samples_leaf=1`:    min number of samples each leaf needs to have\n  * `min_samples_split=2`:   min number of samples needed for a split\n  * `min_purity_increase=0`: min purity needed for a split\n  * `n_subfeatures=-1`: number of features to select at random (0 for all, -1 for square root of number of features)\n  * `n_trees=10`:            number of trees to train\n  * `sampling_fraction=0.7`  fraction of samples to train each tree on\n  * `feature_importance`: method to use for computing feature importances. One of `(:impurity, :split)`\n  * `rng=Random.GLOBAL_RNG`: random number generator or seed\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given features `Xnew` having the same scitype as `X` above. Predictions are probabilistic, but uncalibrated.\n  * `predict_mode(mach, Xnew)`: instead return the mode of each prediction above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `forest`: the `Ensemble` object returned by the core DecisionTree.jl algorithm\n\n# Report\n\n  * `features`: the names of the features encountered in training\n\n# Examples\n\n```\nusing MLJ\nForest = @load RandomForestClassifier pkg=DecisionTree\nforest = Forest(min_samples_split=6, n_subfeatures=3)\n\nX, y = @load_iris\nmach = machine(forest, X, y) |> fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\nyhat = predict(mach, Xnew) # probabilistic predictions\npredict_mode(mach, Xnew)   # point predictions\npdf.(yhat, \"virginica\")    # probabilities for the \"verginica\" class\n\nfitted_params(mach).forest # raw `Ensemble` object from DecisionTrees.jl\n\nfeature_importances(mach)  # `:impurity` feature importances\nforest.feature_importance = :split\nfeature_importance(mach)   # `:split` feature importances\n\n```\n\nSee also [DecisionTree.jl](https://github.com/bensadeghi/DecisionTree.jl) and the unwrapped model type [`MLJDecisionTreeInterface.DecisionTree.RandomForestClassifier`](@ref).\n"
":name" = "RandomForestClassifier"
":human_name" = "CART random forest classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":feature_importances"]
":hyperparameters" = "`(:max_depth, :min_samples_leaf, :min_samples_split, :min_purity_increase, :n_subfeatures, :n_trees, :sampling_fraction, :feature_importance, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Int64\", \"Int64\", \"Float64\", \"Int64\", \"Int64\", \"Float64\", \"Symbol\", \"Union{Integer, Random.AbstractRNG}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`true`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[Clustering.HierarchicalClustering]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "Clustering"
":package_license" = "MIT"
":load_path" = "MLJClusteringInterface.HierarchicalClustering"
":package_uuid" = "aaaa29a8-35af-508c-8bc3-b662a17a0fe5"
":package_url" = "https://github.com/JuliaStats/Clustering.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nHierarchicalClustering\n```\n\nA model type for constructing a hierarchical clusterer, based on [Clustering.jl](https://github.com/JuliaStats/Clustering.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nHierarchicalClustering = @load HierarchicalClustering pkg=Clustering\n```\n\nDo `model = HierarchicalClustering()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `HierarchicalClustering(linkage=...)`.\n\n[Hierarchical Clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering) is a clustering algorithm that organizes the data in a dendrogram based on distances between groups of points and computes cluster assignments by cutting the dendrogram at a given height. More information is available at the [Clustering.jl documentation](https://juliastats.org/Clustering.jl/stable/index.html). Use `predict` to get cluster assignments. The dendrogram and the dendrogram cutter are accessed from the machine report (see below).\n\nThis is a static implementation, i.e., it does not generalize to new data instances, and there is no training data. For clusterers that do generalize, see [`KMeans`](@ref) or [`KMedoids`](@ref).\n\nIn MLJ or MLJBase, create a machine with\n\n```\nmach = machine(model)\n```\n\n# Hyper-parameters\n\n  * `linkage = :single`: linkage method (:single, :average, :complete, :ward, :ward_presquared)\n  * `metric = SqEuclidean`: metric (see `Distances.jl` for available metrics)\n  * `branchorder = :r`: branchorder (:r, :barjoseph, :optimal)\n  * `h = nothing`: height at which the dendrogram is cut\n  * `k = 3`: number of clusters.\n\nIf both `k` and `h` are specified, it is guaranteed that the number of clusters is not less than `k` and their height is not above `h`.\n\n# Operations\n\n  * `predict(mach, X)`: return cluster label assignments, as an unordered `CategoricalVector`. Here `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n\n# Report\n\nAfter calling `predict(mach)`, the fields of `report(mach)`  are:\n\n  * `dendrogram`: the dendrogram that was computed when calling `predict`.\n  * `cutter`: a dendrogram cutter that can be called with a height `h` or a number of clusters `k`, to obtain a new assignment of the data points to clusters (see example below).\n\n# Examples\n\n```\nusing MLJ\n\nX, labels  = make_moons(400, noise=0.09, rng=1) # synthetic data with 2 clusters; X\n\nHierarchicalClustering = @load HierarchicalClustering pkg=Clustering\nmodel = HierarchicalClustering(linkage = :complete)\nmach = machine(model)\n\n# compute and output cluster assignments for observations in `X`:\nyhat = predict(mach, X)\n\n# plot dendrogram:\nusing StatsPlots\nplot(report(mach).dendrogram)\n\n# make new predictions by cutting the dendrogram at another height\nreport(mach).cutter(h = 2.5)\n```\n"
":name" = "HierarchicalClustering"
":human_name" = "hierarchical clusterer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Static`"
":implemented_methods" = [":clean!", ":predict"]
":hyperparameters" = "`(:linkage, :metric, :branchorder, :h, :k)`"
":hyperparameter_types" = "`(\"Symbol\", \"Distances.SemiMetric\", \"Symbol\", \"Union{Nothing, Float64}\", \"Int64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`(:predict,)`"

[Clustering.DBSCAN]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "Clustering"
":package_license" = "MIT"
":load_path" = "MLJClusteringInterface.DBSCAN"
":package_uuid" = "aaaa29a8-35af-508c-8bc3-b662a17a0fe5"
":package_url" = "https://github.com/JuliaStats/Clustering.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nDBSCAN\n```\n\nA model type for constructing a DBSCAN clusterer (density-based spatial clustering of applications with noise), based on [Clustering.jl](https://github.com/JuliaStats/Clustering.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nDBSCAN = @load DBSCAN pkg=Clustering\n```\n\nDo `model = DBSCAN()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `DBSCAN(radius=...)`.\n\n[DBSCAN](https://en.wikipedia.org/wiki/DBSCAN) is a clustering algorithm that groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away). More information is available at the [Clustering.jl documentation](https://juliastats.org/Clustering.jl/stable/index.html). Use `predict` to get cluster assignments. Point types - core, boundary or noise - are accessed from the machine report (see below).\n\nThis is a static implementation, i.e., it does not generalize to new data instances, and there is no training data. For clusterers that do generalize, see [`KMeans`](@ref) or [`KMedoids`](@ref).\n\nIn MLJ or MLJBase, create a machine with\n\n```\nmach = machine(model)\n```\n\n# Hyper-parameters\n\n  * `radius=1.0`: query radius.\n  * `leafsize=20`: number of points binned in each leaf node of the nearest neighbor k-d tree.\n  * `min_neighbors=1`: minimum number of a core point neighbors.\n  * `min_cluster_size=1`: minimum number of points in a valid cluster.\n\n# Operations\n\n  * `predict(mach, X)`: return cluster label assignments, as an unordered `CategoricalVector`. Here `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`. Note that points of type `noise` will always get a label of `0`.\n\n# Report\n\nAfter calling `predict(mach)`, the fields of `report(mach)`  are:\n\n  * `point_types`: A `CategoricalVector` with the DBSCAN point type classification, one element per row of `X`. Elements are either `'C'` (core), `'B'` (boundary), or `'N'` (noise).\n  * `nclusters`: The number of clusters (excluding the noise \"cluster\")\n  * `cluster_labels`: The unique list of cluster labels\n  * `clusters`: A vector of `Clustering.DbscanCluster` objects from Clustering.jl, which have these fields:\n\n      * `size`: number of points in a cluster (core + boundary)\n      * `core_indices`: indices of points in the cluster core\n      * `boundary_indices`: indices of points on the cluster boundary\n\n# Examples\n\n```\nusing MLJ\n\nX, labels  = make_moons(400, noise=0.09, rng=1) # synthetic data with 2 clusters; X\ny = map(labels) do label\n    label == 0 ? \"cookie\" : \"monster\"\nend;\ny = coerce(y, Multiclass);\n\nDBSCAN = @load DBSCAN pkg=Clustering\nmodel = DBSCAN(radius=0.13, min_cluster_size=5)\nmach = machine(model)\n\n# compute and output cluster assignments for observations in `X`:\nyhat = predict(mach, X)\n\n# get DBSCAN point types:\nreport(mach).point_types\nreport(mach).nclusters\n\n# compare cluster labels with actual labels:\ncompare = zip(yhat, y) |> collect;\ncompare[1:10] # clusters align with classes\n\n# visualize clusters, noise in red:\npoints = zip(X.x1, X.x2) |> collect\ncolors = map(yhat) do i\n   i == 0 ? :red :\n   i == 1 ? :blue :\n   i == 2 ? :green :\n   i == 3 ? :yellow :\n   :black\nend\nusing Plots\nscatter(points, color=colors)\n```\n"
":name" = "DBSCAN"
":human_name" = "DBSCAN clusterer (density-based spatial clustering of applications with noise)"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Static`"
":implemented_methods" = [":clean!", ":predict"]
":hyperparameters" = "`(:radius, :leafsize, :min_neighbors, :min_cluster_size)`"
":hyperparameter_types" = "`(\"Real\", \"Int64\", \"Int64\", \"Int64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`(:predict,)`"

[Clustering.KMeans]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "Clustering"
":package_license" = "MIT"
":load_path" = "MLJClusteringInterface.KMeans"
":package_uuid" = "aaaa29a8-35af-508c-8bc3-b662a17a0fe5"
":package_url" = "https://github.com/JuliaStats/Clustering.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nKMeans\n```\n\nA model type for constructing a K-means clusterer, based on [Clustering.jl](https://github.com/JuliaStats/Clustering.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nKMeans = @load KMeans pkg=Clustering\n```\n\nDo `model = KMeans()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `KMeans(k=...)`.\n\n[K-means](http://en.wikipedia.org/wiki/K_means) is a classical method for clustering or vector quantization. It produces a fixed number of clusters, each associated with a *center* (also known as a *prototype*), and each data point is assigned to a cluster with the nearest center.\n\nFrom a mathematical standpoint, K-means is a coordinate descent algorithm that solves the following optimization problem:\n\n$$\n\\text{minimize} \\ \\sum_{i=1}^n \\| \\mathbf{x}_i - \\boldsymbol{\\mu}_{z_i} \\|^2 \\ \\text{w.r.t.} \\ (\\boldsymbol{\\mu}, z)\n$$\n\nHere, $\\boldsymbol{\\mu}_k$ is the center of the $k$-th cluster, and $z_i$ is an index of the cluster for $i$-th point $\\mathbf{x}_i$.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column  scitypes with `schema(X)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `k=3`: The number of centroids to use in clustering.\n  * `metric::SemiMetric=Distances.SqEuclidean`: The metric used to calculate the clustering. Must have type `PreMetric` from Distances.jl.\n  * `init = :kmpp`: One of the following options to indicate how cluster seeds should be initialized:\n\n      * `:kmpp`: KMeans++\n      * `:kmenc`: K-medoids initialization based on centrality\n      * `:rand`: random\n      * an instance of `Clustering.SeedingAlgorithm` from Clustering.jl\n      * an integer vector of length `k` that provides the indices of points to use as initial cluster centers.\n\n    See [documentation of Clustering.jl](https://juliastats.org/Clustering.jl/stable/kmeans.html#Clustering.kmeans).\n\n# Operations\n\n  * `predict(mach, Xnew)`: return cluster label assignments, given new  features `Xnew` having the same Scitype as `X` above.\n  * `transform(mach, Xnew)`: instead return the mean pairwise distances from  new samples to the cluster centers.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `centers`: The coordinates of the cluster centers.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `assignments`: The cluster assignments of each point in the training data.\n  * `cluster_labels`: The labels assigned to each cluster.\n\n# Examples\n\n```\nusing MLJ\nKMeans = @load KMeans pkg=Clustering\n\ntable = load_iris()\ny, X = unpack(table, ==(:target), rng=123)\nmodel = KMeans(k=3)\nmach = machine(model, X) |> fit!\n\nyhat = predict(mach, X)\n@assert yhat == report(mach).assignments\n\ncompare = zip(yhat, y) |> collect;\ncompare[1:8] # clusters align with classes\n\ncenter_dists = transform(mach, fitted_params(mach).centers')\n\n@assert center_dists[1][1] == 0.0\n@assert center_dists[2][2] == 0.0\n@assert center_dists[3][3] == 0.0\n```\n\nSee also [`KMedoids`](@ref)\n"
":name" = "KMeans"
":human_name" = "K-means clusterer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":transform"]
":hyperparameters" = "`(:k, :metric, :init)`"
":hyperparameter_types" = "`(\"Int64\", \"Distances.SemiMetric\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[Clustering.KMedoids]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "Clustering"
":package_license" = "MIT"
":load_path" = "MLJClusteringInterface.KMedoids"
":package_uuid" = "aaaa29a8-35af-508c-8bc3-b662a17a0fe5"
":package_url" = "https://github.com/JuliaStats/Clustering.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nKMedoids\n```\n\nA model type for constructing a K-medoids clusterer, based on [Clustering.jl](https://github.com/JuliaStats/Clustering.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nKMedoids = @load KMedoids pkg=Clustering\n```\n\nDo `model = KMedoids()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `KMedoids(k=...)`.\n\n[K-medoids](http://en.wikipedia.org/wiki/K-medoids) is a clustering algorithm that works by finding $k$ data points (called *medoids*) such that the total distance between each data point and the closest *medoid* is minimal.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `k=3`: The number of centroids to use in clustering.\n  * `metric::SemiMetric=Distances.SqEuclidean`: The metric used to calculate the clustering. Must have type `PreMetric` from Distances.jl.\n  * `init` (defaults to `:kmpp`): how medoids should be initialized, could  be one of the following:\n\n      * `:kmpp`: KMeans++\n      * `:kmenc`: K-medoids initialization based on centrality\n      * `:rand`: random\n      * an instance of `Clustering.SeedingAlgorithm` from Clustering.jl\n      * an integer vector of length `k` that provides the indices of points to use as initial medoids.\n\n    See [documentation of Clustering.jl](https://juliastats.org/Clustering.jl/stable/kmedoids.html#Clustering.kmedoids).\n\n# Operations\n\n  * `predict(mach, Xnew)`: return cluster label assignments, given new  features `Xnew` having the same Scitype as `X` above.\n  * `transform(mach, Xnew)`: instead return the mean pairwise distances from  new samples to the cluster centers.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `medoids`: The coordinates of the cluster medoids.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `assignments`: The cluster assignments of each point in the training data.\n  * `cluster_labels`: The labels assigned to each cluster.\n\n# Examples\n\n```\nusing MLJ\nKMedoids = @load KMedoids pkg=Clustering\n\ntable = load_iris()\ny, X = unpack(table, ==(:target), rng=123)\nmodel = KMedoids(k=3)\nmach = machine(model, X) |> fit!\n\nyhat = predict(mach, X)\n@assert yhat == report(mach).assignments\n\ncompare = zip(yhat, y) |> collect;\ncompare[1:8] # clusters align with classes\n\ncenter_dists = transform(mach, fitted_params(mach).medoids')\n\n@assert center_dists[1][1] == 0.0\n@assert center_dists[2][2] == 0.0\n@assert center_dists[3][3] == 0.0\n```\n\nSee also [`KMeans`](@ref)\n"
":name" = "KMedoids"
":human_name" = "K-medoids clusterer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":transform"]
":hyperparameters" = "`(:k, :metric, :init)`"
":hyperparameter_types" = "`(\"Int64\", \"Distances.SemiMetric\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[EvoLinear.EvoLinearRegressor]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "EvoLinear"
":package_license" = "MIT"
":load_path" = "EvoLinear.EvoLinearRegressor"
":package_uuid" = "ab853011-1780-437f-b4b5-5de6f4777246"
":package_url" = "https://github.com/jeremiedb/EvoLinear.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Regression models for various taks: mse, logistic, poisson, gamma, tweedie."
":name" = "EvoLinearRegressor"
":human_name" = "evo linear regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":predict", ":reformat", ":selectrows", ":update"]
":hyperparameters" = "`(:loss, :updater, :nrounds, :eta, :L1, :L2, :rng, :device)`"
":hyperparameter_types" = "`(\"Symbol\", \"Symbol\", \"Int64\", \"AbstractFloat\", \"AbstractFloat\", \"AbstractFloat\", \"Any\", \"Symbol\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = ":nrounds"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[XGBoost.XGBoostCount]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Count}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Count}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Count}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "XGBoost"
":package_license" = "unknown"
":load_path" = "MLJXGBoostInterface.XGBoostCount"
":package_uuid" = "009559a3-9522-5dbb-924b-0b6ed2b22bb9"
":package_url" = "https://github.com/dmlc/XGBoost.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "The XGBoost gradient boosting method, for use with `Count` univariate targets, using a Poisson objective function. "
":name" = "XGBoostCount"
":human_name" = "xg boost count"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":predict"]
":hyperparameters" = "`(:num_round, :booster, :disable_default_eval_metric, :eta, :gamma, :max_depth, :min_child_weight, :max_delta_step, :subsample, :colsample_bytree, :colsample_bylevel, :lambda, :alpha, :tree_method, :sketch_eps, :scale_pos_weight, :updater, :refresh_leaf, :process_type, :grow_policy, :max_leaves, :max_bin, :predictor, :sample_type, :normalize_type, :rate_drop, :one_drop, :skip_drop, :feature_selector, :top_k, :tweedie_variance_power, :objective, :base_score, :eval_metric, :seed, :nthread)`"
":hyperparameter_types" = "`(\"Int64\", \"String\", \"Int64\", \"Float64\", \"Float64\", \"Int64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"String\", \"Float64\", \"Float64\", \"String\", \"Union{Bool, Int64}\", \"String\", \"String\", \"Int64\", \"Int64\", \"String\", \"String\", \"String\", \"Float64\", \"Any\", \"Float64\", \"String\", \"Int64\", \"Float64\", \"Any\", \"Float64\", \"Any\", \"Int64\", \"Int64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[XGBoost.XGBoostRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "XGBoost"
":package_license" = "unknown"
":load_path" = "MLJXGBoostInterface.XGBoostRegressor"
":package_uuid" = "009559a3-9522-5dbb-924b-0b6ed2b22bb9"
":package_url" = "https://github.com/dmlc/XGBoost.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "The XGBoost gradient boosting method, for use with `Continuous` univariate targets. "
":name" = "XGBoostRegressor"
":human_name" = "xg boost regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":predict"]
":hyperparameters" = "`(:num_round, :booster, :disable_default_eval_metric, :eta, :gamma, :max_depth, :min_child_weight, :max_delta_step, :subsample, :colsample_bytree, :colsample_bylevel, :lambda, :alpha, :tree_method, :sketch_eps, :scale_pos_weight, :updater, :refresh_leaf, :process_type, :grow_policy, :max_leaves, :max_bin, :predictor, :sample_type, :normalize_type, :rate_drop, :one_drop, :skip_drop, :feature_selector, :top_k, :tweedie_variance_power, :objective, :base_score, :eval_metric, :seed, :nthread)`"
":hyperparameter_types" = "`(\"Int64\", \"String\", \"Int64\", \"Float64\", \"Float64\", \"Int64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"String\", \"Float64\", \"Float64\", \"String\", \"Union{Bool, Int64}\", \"String\", \"String\", \"Int64\", \"Int64\", \"String\", \"String\", \"String\", \"Float64\", \"Any\", \"Float64\", \"String\", \"Int64\", \"Float64\", \"Any\", \"Float64\", \"Any\", \"Int64\", \"Int64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[XGBoost.XGBoostClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "XGBoost"
":package_license" = "unknown"
":load_path" = "MLJXGBoostInterface.XGBoostClassifier"
":package_uuid" = "009559a3-9522-5dbb-924b-0b6ed2b22bb9"
":package_url" = "https://github.com/dmlc/XGBoost.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "The XGBoost gradient boosting method, for use with `Finite` univariate targets (`Multiclass`, `OrderedFactor` and `Binary=Finite{2}`)."
":name" = "XGBoostClassifier"
":human_name" = "xg boost classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":predict"]
":hyperparameters" = "`(:num_round, :booster, :disable_default_eval_metric, :eta, :gamma, :max_depth, :min_child_weight, :max_delta_step, :subsample, :colsample_bytree, :colsample_bylevel, :lambda, :alpha, :tree_method, :sketch_eps, :scale_pos_weight, :updater, :refresh_leaf, :process_type, :grow_policy, :max_leaves, :max_bin, :predictor, :sample_type, :normalize_type, :rate_drop, :one_drop, :skip_drop, :feature_selector, :top_k, :tweedie_variance_power, :objective, :base_score, :eval_metric, :seed, :nthread)`"
":hyperparameter_types" = "`(\"Int64\", \"String\", \"Int64\", \"Float64\", \"Float64\", \"Int64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"String\", \"Float64\", \"Float64\", \"String\", \"Union{Bool, Int64}\", \"String\", \"String\", \"Int64\", \"Int64\", \"String\", \"String\", \"String\", \"Float64\", \"Any\", \"Float64\", \"String\", \"Int64\", \"Float64\", \"Any\", \"Float64\", \"Any\", \"Int64\", \"Int64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[LightGBM.LGBMClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "LightGBM"
":package_license" = "MIT Expat"
":load_path" = "LightGBM.MLJInterface.LGBMClassifier"
":package_uuid" = "7acf609c-83a4-11e9-1ffb-b912bcd3b04a"
":package_url" = "https://github.com/IQVIA-ML/LightGBM.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Microsoft LightGBM FFI wrapper: Classifier"
":name" = "LGBMClassifier"
":human_name" = "lgbm classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":predict", ":update"]
":hyperparameters" = "`(:boosting, :num_iterations, :learning_rate, :num_leaves, :max_depth, :tree_learner, :histogram_pool_size, :min_data_in_leaf, :min_sum_hessian_in_leaf, :max_delta_step, :lambda_l1, :lambda_l2, :min_gain_to_split, :feature_fraction, :feature_fraction_bynode, :feature_fraction_seed, :bagging_fraction, :pos_bagging_fraction, :neg_bagging_fraction, :bagging_freq, :bagging_seed, :early_stopping_round, :extra_trees, :extra_seed, :max_bin, :bin_construct_sample_cnt, :init_score, :drop_rate, :max_drop, :skip_drop, :xgboost_dart_mode, :uniform_drop, :drop_seed, :top_rate, :other_rate, :min_data_per_group, :max_cat_threshold, :cat_l2, :cat_smooth, :objective, :categorical_feature, :data_random_seed, :is_sparse, :is_unbalance, :boost_from_average, :scale_pos_weight, :use_missing, :feature_pre_filter, :metric, :metric_freq, :is_training_metric, :ndcg_at, :num_machines, :num_threads, :local_listen_port, :time_out, :machine_list_file, :save_binary, :device_type, :force_col_wise, :force_row_wise, :truncate_booster)`"
":hyperparameter_types" = "`(\"String\", \"Int64\", \"Float64\", \"Int64\", \"Int64\", \"String\", \"Float64\", \"Int64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Int64\", \"Float64\", \"Float64\", \"Float64\", \"Int64\", \"Int64\", \"Int64\", \"Bool\", \"Int64\", \"Int64\", \"Any\", \"String\", \"Float64\", \"Int64\", \"Float64\", \"Bool\", \"Bool\", \"Int64\", \"Float64\", \"Float64\", \"Int64\", \"Int64\", \"Float64\", \"Float64\", \"String\", \"Vector{Int64}\", \"Int64\", \"Bool\", \"Bool\", \"Bool\", \"Any\", \"Bool\", \"Bool\", \"Vector{String}\", \"Int64\", \"Bool\", \"Vector{Int64}\", \"Int64\", \"Int64\", \"Int64\", \"Int64\", \"String\", \"Bool\", \"String\", \"Bool\", \"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[LightGBM.LGBMRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "LightGBM"
":package_license" = "MIT Expat"
":load_path" = "LightGBM.MLJInterface.LGBMRegressor"
":package_uuid" = "7acf609c-83a4-11e9-1ffb-b912bcd3b04a"
":package_url" = "https://github.com/IQVIA-ML/LightGBM.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Microsoft LightGBM FFI wrapper: Regressor"
":name" = "LGBMRegressor"
":human_name" = "lgbm regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":predict", ":update"]
":hyperparameters" = "`(:boosting, :num_iterations, :learning_rate, :num_leaves, :max_depth, :tree_learner, :histogram_pool_size, :min_data_in_leaf, :min_sum_hessian_in_leaf, :max_delta_step, :lambda_l1, :lambda_l2, :min_gain_to_split, :feature_fraction, :feature_fraction_bynode, :feature_fraction_seed, :bagging_fraction, :bagging_freq, :bagging_seed, :early_stopping_round, :extra_trees, :extra_seed, :max_bin, :bin_construct_sample_cnt, :init_score, :drop_rate, :max_drop, :skip_drop, :xgboost_dart_mode, :uniform_drop, :drop_seed, :top_rate, :other_rate, :min_data_per_group, :max_cat_threshold, :cat_l2, :cat_smooth, :objective, :categorical_feature, :data_random_seed, :is_sparse, :is_unbalance, :boost_from_average, :use_missing, :feature_pre_filter, :alpha, :metric, :metric_freq, :is_training_metric, :ndcg_at, :num_machines, :num_threads, :local_listen_port, :time_out, :machine_list_file, :save_binary, :device_type, :force_col_wise, :force_row_wise, :truncate_booster)`"
":hyperparameter_types" = "`(\"String\", \"Int64\", \"Float64\", \"Int64\", \"Int64\", \"String\", \"Float64\", \"Int64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Int64\", \"Float64\", \"Int64\", \"Int64\", \"Int64\", \"Bool\", \"Int64\", \"Int64\", \"Any\", \"String\", \"Float64\", \"Int64\", \"Float64\", \"Bool\", \"Bool\", \"Int64\", \"Float64\", \"Float64\", \"Int64\", \"Int64\", \"Float64\", \"Float64\", \"String\", \"Vector{Int64}\", \"Int64\", \"Bool\", \"Bool\", \"Bool\", \"Bool\", \"Bool\", \"Float64\", \"Vector{String}\", \"Int64\", \"Bool\", \"Vector{Int64}\", \"Int64\", \"Int64\", \"Int64\", \"Int64\", \"String\", \"Bool\", \"String\", \"Bool\", \"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJText.TfidfTransformer]
":input_scitype" = "`Union{AbstractVector{<:AbstractVector{ScientificTypesBase.Textual}}, AbstractVector{<:ScientificTypesBase.Multiset{<:Tuple{Vararg{ScientificTypesBase.Textual, var\"_s2324\"}} where var\"_s2324\"}}, AbstractVector{<:ScientificTypesBase.Multiset{ScientificTypesBase.Textual}}}`"
":output_scitype" = "`AbstractMatrix{ScientificTypesBase.Continuous}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{Union{AbstractVector{<:AbstractVector{ScientificTypesBase.Textual}}, AbstractVector{<:ScientificTypesBase.Multiset{<:Tuple{Vararg{ScientificTypesBase.Textual, var\"_s2324\"}} where var\"_s2324\"}}, AbstractVector{<:ScientificTypesBase.Multiset{ScientificTypesBase.Textual}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractMatrix{ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`Union{AbstractVector{<:AbstractVector{ScientificTypesBase.Textual}}, AbstractVector{<:ScientificTypesBase.Multiset{<:Tuple{Vararg{ScientificTypesBase.Textual, var\"_s2324\"}} where var\"_s2324\"}}, AbstractVector{<:ScientificTypesBase.Multiset{ScientificTypesBase.Textual}}}`"
":is_pure_julia" = "`true`"
":package_name" = "MLJText"
":package_license" = "MIT"
":load_path" = "MLJText.TfidfTransformer"
":package_uuid" = "7876af07-990d-54b4-ab0e-23690620f79a"
":package_url" = "https://github.com/JuliaAI/MLJText.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Build TF-IDF matrix from raw documents"
":name" = "TfidfTransformer"
":human_name" = "tfidf transformer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fitted_params"]
":hyperparameters" = "`(:max_doc_freq, :min_doc_freq, :smooth_idf)`"
":hyperparameter_types" = "`(\"Float64\", \"Float64\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJText.CountTransformer]
":input_scitype" = "`Union{AbstractVector{<:AbstractVector{ScientificTypesBase.Textual}}, AbstractVector{<:ScientificTypesBase.Multiset{<:Tuple{Vararg{ScientificTypesBase.Textual, var\"_s2324\"}} where var\"_s2324\"}}, AbstractVector{<:ScientificTypesBase.Multiset{ScientificTypesBase.Textual}}}`"
":output_scitype" = "`AbstractMatrix{ScientificTypesBase.Continuous}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{Union{AbstractVector{<:AbstractVector{ScientificTypesBase.Textual}}, AbstractVector{<:ScientificTypesBase.Multiset{<:Tuple{Vararg{ScientificTypesBase.Textual, var\"_s2324\"}} where var\"_s2324\"}}, AbstractVector{<:ScientificTypesBase.Multiset{ScientificTypesBase.Textual}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractMatrix{ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`Union{AbstractVector{<:AbstractVector{ScientificTypesBase.Textual}}, AbstractVector{<:ScientificTypesBase.Multiset{<:Tuple{Vararg{ScientificTypesBase.Textual, var\"_s2324\"}} where var\"_s2324\"}}, AbstractVector{<:ScientificTypesBase.Multiset{ScientificTypesBase.Textual}}}`"
":is_pure_julia" = "`true`"
":package_name" = "MLJText"
":package_license" = "MIT"
":load_path" = "MLJText.CountTransformer"
":package_uuid" = "7876af07-990d-54b4-ab0e-23690620f79a"
":package_url" = "https://github.com/JuliaAI/MLJText.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Build Bag-of-Words matrix from word counts for corpus of documents"
":name" = "CountTransformer"
":human_name" = "count transformer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fitted_params"]
":hyperparameters" = "`(:max_doc_freq, :min_doc_freq)`"
":hyperparameter_types" = "`(\"Float64\", \"Float64\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJText.BM25Transformer]
":input_scitype" = "`Union{AbstractVector{<:AbstractVector{ScientificTypesBase.Textual}}, AbstractVector{<:ScientificTypesBase.Multiset{<:Tuple{Vararg{ScientificTypesBase.Textual, var\"_s2324\"}} where var\"_s2324\"}}, AbstractVector{<:ScientificTypesBase.Multiset{ScientificTypesBase.Textual}}}`"
":output_scitype" = "`AbstractMatrix{ScientificTypesBase.Continuous}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{Union{AbstractVector{<:AbstractVector{ScientificTypesBase.Textual}}, AbstractVector{<:ScientificTypesBase.Multiset{<:Tuple{Vararg{ScientificTypesBase.Textual, var\"_s2324\"}} where var\"_s2324\"}}, AbstractVector{<:ScientificTypesBase.Multiset{ScientificTypesBase.Textual}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractMatrix{ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`Union{AbstractVector{<:AbstractVector{ScientificTypesBase.Textual}}, AbstractVector{<:ScientificTypesBase.Multiset{<:Tuple{Vararg{ScientificTypesBase.Textual, var\"_s2324\"}} where var\"_s2324\"}}, AbstractVector{<:ScientificTypesBase.Multiset{ScientificTypesBase.Textual}}}`"
":is_pure_julia" = "`true`"
":package_name" = "MLJText"
":package_license" = "MIT"
":load_path" = "MLJText.BM25Transformer"
":package_uuid" = "7876af07-990d-54b4-ab0e-23690620f79a"
":package_url" = "https://github.com/JuliaAI/MLJText.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Build BM-25 matrix from raw documents"
":name" = "BM25Transformer"
":human_name" = "b m25 transformer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fitted_params"]
":hyperparameters" = "`(:max_doc_freq, :min_doc_freq, :κ, :β, :smooth_idf)`"
":hyperparameter_types" = "`(\"Float64\", \"Float64\", \"Int64\", \"Float64\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionNetworks.AEDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "OutlierDetectionNetworks"
":package_license" = "MIT"
":load_path" = "OutlierDetectionNetworks.AEDetector"
":package_uuid" = "c7f57e37-4fcb-4a0b-a36c-c2204bc839a7"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionNetworks.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nAEDetector(encoder= Chain(),\n           decoder = Chain(),\n           batchsize= 32,\n           epochs = 1,\n           shuffle = false,\n           partial = true,\n           opt = Adam(),\n           loss = mse)\n```\n\nCalculate the anomaly score of an instance based on the reconstruction loss of an autoencoder, see [1] for an explanation of auto encoders.\n\n## Parameters\n\n```\nencoder::Chain\n```\n\nTransforms the input data into a latent state with a fixed shape.\n\n```\ndecoder::Chain\n```\n\nTransforms the latent state back into the shape of the input data.\n\n```\nbatchsize::Integer\n```\n\nThe number of samples to work through before updating the internal model parameters.\n\n```\nepochs::Integer\n```\n\nThe number of passes of the entire training dataset the machine learning algorithm has completed. \n\n```\nshuffle::Bool\n```\n\nIf `shuffle=true`, shuffles the observations each time iterations are re-started, else no shuffling is performed.\n\n```\npartial::Bool\n```\n\nIf `partial=false`, drops the last mini-batch if it is smaller than the batchsize.\n\n```\nopt::Any\n```\n\nAny Flux-compatibale optimizer, typically a `struct`  that holds all the optimiser parameters along with a definition of `apply!` that defines how to apply the update rule associated with the optimizer.\n\n```\nloss::Function\n```\n\nThe loss function used to calculate the reconstruction error, see [https://fluxml.ai/Flux.jl/stable/models/losses/](https://fluxml.ai/Flux.jl/stable/models/losses/) for examples.\n\n## Examples\n\n```julia\nusing OutlierDetection: AEDetector, fit, score\ndetector = AEDetector()\nX = rand(10, 100)\nresult = fit(detector, X)\ntest_scores = transform(detector, result.model, X)\n```\n\n## References\n\n[1] Aggarwal, Charu C. (2017): Outlier Analysis.\n"
":name" = "AEDetector"
":human_name" = "ae detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:encoder, :decoder, :batchsize, :epochs, :shuffle, :partial, :opt, :loss)`"
":hyperparameter_types" = "`(\"Flux.Chain\", \"Flux.Chain\", \"Integer\", \"Integer\", \"Bool\", \"Bool\", \"Any\", \"Function\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionNetworks.DSADDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "OutlierDetectionNetworks"
":package_license" = "MIT"
":load_path" = "OutlierDetectionNetworks.DSADDetector"
":package_uuid" = "c7f57e37-4fcb-4a0b-a36c-c2204bc839a7"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionNetworks.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nDSADDetector(encoder = Chain(),\n                decoder = Chain(),\n                batchsize = 32,\n                epochs = 1,\n                shuffle = true,\n                partial = false,\n                opt = Adam(),\n                loss = mse,\n                eta = 1,\n                eps = 1e-6,\n                callback = _ -> () -> ())\n```\n\nDeep Semi-Supervised Anomaly detection technique based on the distance to a hypersphere center as described in [1].\n\n## Parameters\n\n```\nencoder::Chain\n```\n\nTransforms the input data into a latent state with a fixed shape.\n\n```\ndecoder::Chain\n```\n\nTransforms the latent state back into the shape of the input data.\n\n```\nbatchsize::Integer\n```\n\nThe number of samples to work through before updating the internal model parameters.\n\n```\nepochs::Integer\n```\n\nThe number of passes of the entire training dataset the machine learning algorithm has completed. \n\n```\nshuffle::Bool\n```\n\nIf `shuffle=true`, shuffles the observations each time iterations are re-started, else no shuffling is performed.\n\n```\npartial::Bool\n```\n\nIf `partial=false`, drops the last mini-batch if it is smaller than the batchsize.\n\n```\nopt::Any\n```\n\nAny Flux-compatibale optimizer, typically a `struct`  that holds all the optimiser parameters along with a definition of `apply!` that defines how to apply the update rule associated with the optimizer.\n\n```\nloss::Function\n```\n\nThe loss function used to calculate the reconstruction error, see [https://fluxml.ai/Flux.jl/stable/models/losses/](https://fluxml.ai/Flux.jl/stable/models/losses/) for examples.\n\n```\neta::Real\n```\n\nWeighting parameter for the labeled data; i.e. higher values of eta assign higher weight to labeled data in the svdd loss function. For a sensitivity analysis of this parameter, see [1].\n\n```\neps::Real\n```\n\nBecause the inverse distance used in the svdd loss can lead to division by zero, the parameters `eps` is added for numerical stability.\n\n```\ncallback::Function\n```\n\n*Experimental parameter that might change*. A function to be called after the model parameters have been updated that can call Flux's callback helpers, see [https://fluxml.ai/Flux.jl/stable/utilities/#Callback-Helpers-1](https://fluxml.ai/Flux.jl/stable/utilities/#Callback-Helpers-1).\n\n**Notice:** The parameters `batchsize`, `epochs`, `shuffle`, `partial`, `opt` and `callback` can also be tuples of size 2, specifying the corresponding values for (1) pretraining and (2) training; otherwise the same values are used for pretraining and training.\n\n## Examples\n\n```julia\nusing OutlierDetection: DSADDetector, fit, score\ndetector = DSADDetector()\nX = rand(10, 100)\ny = rand([-1,1], 100)\nmodel = fit(detector, X, y)\ntrain_scores, test_scores = score(detector, model, X)\n```\n\n## References\n\n[1] Ruff, Lukas; Vandermeulen, Robert A.; Görnitz, Nico; Binder, Alexander; Müller, Emmanuel; Müller, Klaus-Robert; Kloft, Marius (2019): Deep Semi-Supervised Anomaly Detection.\n"
":name" = "DSADDetector"
":human_name" = "dsad detector"
":is_supervised" = "`true`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.SupervisedDetector`"
":implemented_methods" = [":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:encoder, :decoder, :batchsize, :epochs, :shuffle, :partial, :opt, :loss, :eta, :eps, :callback)`"
":hyperparameter_types" = "`(\"Flux.Chain\", \"Flux.Chain\", \"Tuple{Integer, Integer}\", \"Tuple{Integer, Integer}\", \"Tuple{Bool, Bool}\", \"Tuple{Bool, Bool}\", \"Any\", \"Function\", \"Number\", \"Number\", \"Tuple{Function, Function}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionNetworks.ESADDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "OutlierDetectionNetworks"
":package_license" = "MIT"
":load_path" = "OutlierDetectionNetworks.ESADDetector"
":package_uuid" = "c7f57e37-4fcb-4a0b-a36c-c2204bc839a7"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionNetworks.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nESADDetector(encoder = Chain(),\n            decoder = Chain(),\n            batchsize = 32,\n            epochs = 1,\n            shuffle = false,\n            partial = true,\n            opt = Adam(),\n            λ1 = 1,\n            λ2 = 1,\n            noise = identity)\n```\n\nEnd-to-End semi-supervised anomaly detection algorithm similar to DeepSAD, but without the pretraining phase. The algorithm was published by Huang et al., see [1].\n\n## Parameters\n\n```\nencoder::Chain\n```\n\nTransforms the input data into a latent state with a fixed shape.\n\n```\ndecoder::Chain\n```\n\nTransforms the latent state back into the shape of the input data.\n\n```\nbatchsize::Integer\n```\n\nThe number of samples to work through before updating the internal model parameters.\n\n```\nepochs::Integer\n```\n\nThe number of passes of the entire training dataset the machine learning algorithm has completed. \n\n```\nshuffle::Bool\n```\n\nIf `shuffle=true`, shuffles the observations each time iterations are re-started, else no shuffling is performed.\n\n```\npartial::Bool\n```\n\nIf `partial=false`, drops the last mini-batch if it is smaller than the batchsize.\n\n```\nopt::Any\n```\n\nAny Flux-compatibale optimizer, typically a `struct`  that holds all the optimiser parameters along with a definition of `apply!` that defines how to apply the update rule associated with the optimizer.\n\n```\nλ1::Real\n```\n\nWeighting parameter of the norm loss, which minimizes the empirical variance and thus minimizes entropy.\n\n```\nλ2::Real\n```\n\nWeighting parameter of the assistent loss function to define the consistency between the two encoders.\n\n```\nnoise::Function (AbstractArray{T} -> AbstractArray{T})\n```\n\nA function to be applied to a batch of input data to add noise, see [1] for an explanation.\n\n## Examples\n\n```julia\nusing OutlierDetection: ESADDetector, fit, score\ndetector = ESADDetector()\nX = rand(10, 100)\ny = rand([-1,1], 100)\nmodel = fit(detector, X, y)\ntrain_scores, test_scores = score(detector, model, X)\n```\n\n## References\n\n[1] Huang, Chaoqin; Ye, Fei; Zhang, Ya; Wang, Yan-Feng; Tian, Qi (2020): ESAD: End-to-end Deep Semi-supervised Anomaly Detection.\n"
":name" = "ESADDetector"
":human_name" = "esad detector"
":is_supervised" = "`true`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.SupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:encoder, :decoder, :batchsize, :epochs, :shuffle, :partial, :opt, :λ1, :λ2, :noise)`"
":hyperparameter_types" = "`(\"Flux.Chain\", \"Flux.Chain\", \"Integer\", \"Integer\", \"Bool\", \"Bool\", \"Any\", \"Number\", \"Number\", \"Function\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[EvoTrees.EvoTreeClassifier]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "EvoTrees"
":package_license" = "Apache"
":load_path" = "EvoTrees.EvoTreeClassifier"
":package_uuid" = "f6006082-12f8-11e9-0c9c-0d5d367ab1e5"
":package_url" = "https://github.com/Evovest/EvoTrees.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Multi-classification with softmax and cross-entropy loss."
":name" = "EvoTreeClassifier"
":human_name" = "evo tree classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":predict"]
":hyperparameters" = "`(:loss, :nrounds, :lambda, :gamma, :eta, :max_depth, :min_weight, :rowsample, :colsample, :nbins, :alpha, :metric, :rng, :device)`"
":hyperparameter_types" = "`(\"EvoTrees.ModelType\", \"Int64\", \"AbstractFloat\", \"AbstractFloat\", \"AbstractFloat\", \"Int64\", \"AbstractFloat\", \"AbstractFloat\", \"AbstractFloat\", \"Int64\", \"AbstractFloat\", \"Symbol\", \"Any\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = ":nrounds"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[EvoTrees.EvoTreeGaussian]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "EvoTrees"
":package_license" = "Apache"
":load_path" = "EvoTrees.EvoTreeGaussian"
":package_uuid" = "f6006082-12f8-11e9-0c9c-0d5d367ab1e5"
":package_url" = "https://github.com/Evovest/EvoTrees.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Gaussian maximum likelihood of μ and σ."
":name" = "EvoTreeGaussian"
":human_name" = "evo tree gaussian"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":predict"]
":hyperparameters" = "`(:loss, :nrounds, :lambda, :gamma, :eta, :max_depth, :min_weight, :rowsample, :colsample, :nbins, :alpha, :monotone_constraints, :metric, :rng, :device)`"
":hyperparameter_types" = "`(\"EvoTrees.ModelType\", \"Int64\", \"AbstractFloat\", \"AbstractFloat\", \"AbstractFloat\", \"Int64\", \"AbstractFloat\", \"AbstractFloat\", \"AbstractFloat\", \"Int64\", \"AbstractFloat\", \"Any\", \"Symbol\", \"Any\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = ":nrounds"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[EvoTrees.EvoTreeRegressor]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "EvoTrees"
":package_license" = "Apache"
":load_path" = "EvoTrees.EvoTreeRegressor"
":package_uuid" = "f6006082-12f8-11e9-0c9c-0d5d367ab1e5"
":package_url" = "https://github.com/Evovest/EvoTrees.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Regression models with various underlying methods: least square, quantile, logistic, gamma, tweedie."
":name" = "EvoTreeRegressor"
":human_name" = "evo tree regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":predict"]
":hyperparameters" = "`(:loss, :nrounds, :lambda, :gamma, :eta, :max_depth, :min_weight, :rowsample, :colsample, :nbins, :alpha, :monotone_constraints, :metric, :rng, :device)`"
":hyperparameter_types" = "`(\"EvoTrees.ModelType\", \"Int64\", \"AbstractFloat\", \"AbstractFloat\", \"AbstractFloat\", \"Int64\", \"AbstractFloat\", \"AbstractFloat\", \"AbstractFloat\", \"Int64\", \"AbstractFloat\", \"Any\", \"Symbol\", \"Any\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = ":nrounds"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[EvoTrees.EvoTreeCount]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Count}`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Count}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Count}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "EvoTrees"
":package_license" = "Apache"
":load_path" = "EvoTrees.EvoTreeCount"
":package_uuid" = "f6006082-12f8-11e9-0c9c-0d5d367ab1e5"
":package_url" = "https://github.com/Evovest/EvoTrees.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Poisson regression fitting λ with max likelihood."
":name" = "EvoTreeCount"
":human_name" = "evo tree count"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":predict"]
":hyperparameters" = "`(:loss, :nrounds, :lambda, :gamma, :eta, :max_depth, :min_weight, :rowsample, :colsample, :nbins, :alpha, :monotone_constraints, :metric, :rng, :device)`"
":hyperparameter_types" = "`(\"EvoTrees.ModelType\", \"Int64\", \"AbstractFloat\", \"AbstractFloat\", \"AbstractFloat\", \"Int64\", \"AbstractFloat\", \"AbstractFloat\", \"AbstractFloat\", \"Int64\", \"AbstractFloat\", \"Any\", \"Symbol\", \"Any\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = ":nrounds"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.ConstantClassifier]
":input_scitype" = "`ScientificTypesBase.Table`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table, AbstractVector{<:ScientificTypesBase.Finite}}, Tuple{ScientificTypesBase.Table, AbstractVector{<:ScientificTypesBase.Finite}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.ConstantClassifier"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Constant classifier (Probabilistic)."
":name" = "ConstantClassifier"
":human_name" = "constant classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`()`"
":hyperparameter_types" = "`()`"
":hyperparameter_ranges" = "`()`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.Standardizer]
":input_scitype" = "`Union{ScientificTypesBase.Table, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`Union{ScientificTypesBase.Table, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table, AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`Union{ScientificTypesBase.Table, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`Union{ScientificTypesBase.Table, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.Standardizer"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nStandardizer\n```\n\nA model type for constructing a standardizer, based on [MLJModels.jl](https://github.com/alan-turing-institute/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nStandardizer = @load Standardizer pkg=MLJModels\n```\n\nDo `model = Standardizer()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `Standardizer(features=...)`.\n\nUse this model to standardize (whiten) a `Continuous` vector, or relevant columns of a table. The rescalings applied by this transformer to new data are always those learned during the training phase, which are generally different from what would actually standardize the new data.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nwhere\n\n  * `X`: any Tables.jl compatible table or any abstract vector with `Continuous` element scitype (any abstract float vector). Only features in a table with `Continuous` scitype can be standardized; check column scitypes with `schema(X)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `features`: one of the following, with the behavior indicated below:\n\n      * `[]` (empty, the default): standardize all features (columns) having `Continuous` element scitype\n      * non-empty vector of feature names (symbols): standardize only the `Continuous` features in the vector (if `ignore=false`) or `Continuous` features *not* named in the vector (`ignore=true`).\n      * function or other callable: standardize a feature if the callable returns `true` on its name. For example, `Standardizer(features = name -> name in [:x1, :x3], ignore = true, count=true)` has the same effect as `Standardizer(features = [:x1, :x3], ignore = true, count=true)`, namely to standardize all `Continuous` and `Count` features, with the exception of `:x1` and `:x3`.\n\n    Note this behavior is further modified if the `ordered_factor` or `count` flags are set to `true`; see below\n  * `ignore=false`: whether to ignore or standardize specified `features`, as explained above\n  * `ordered_factor=false`: if `true`, standardize any `OrderedFactor` feature wherever a `Continuous` feature would be standardized, as described above\n  * `count=false`: if `true`, standardize any `Count` feature wherever a `Continuous` feature would be standardized, as described above\n\n# Operations\n\n  * `transform(mach, Xnew)`: return `Xnew` with relevant features standardized according to the rescalings learned during fitting of `mach`.\n  * `inverse_transform(mach, Z)`: apply the inverse transformation to `Z`, so that `inverse_transform(mach, transform(mach, Xnew))` is approximately the same as `Xnew`; unavailable if `ordered_factor` or `count` flags were set to `true`.\n\n# Fitted parameters\n\n`fitted_params(mach)` is a dictionary of the rescaling parameters, keyed on feature name. In each value the first component is the training data mean, the second the standard deviation.\n\n**Warning:** This format for `fitted_params(mach)` is not standard and may change in the future.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `features_fit`: the names of features that will be standardized\n\n# Examples\n\n```\nusing MLJ\n\nX = (ordinal1 = [1, 2, 3],\n     ordinal2 = coerce([:x, :y, :x], OrderedFactor),\n     ordinal3 = [10.0, 20.0, 30.0],\n     ordinal4 = [-20.0, -30.0, -40.0],\n     nominal = coerce([\"Your father\", \"he\", \"is\"], Multiclass));\n\njulia> schema(X)\n┌──────────┬──────────────────┐\n│ names    │ scitypes         │\n├──────────┼──────────────────┤\n│ ordinal1 │ Count            │\n│ ordinal2 │ OrderedFactor{2} │\n│ ordinal3 │ Continuous       │\n│ ordinal4 │ Continuous       │\n│ nominal  │ Multiclass{3}    │\n└──────────┴──────────────────┘\n\nstand1 = Standardizer();\n\njulia> transform(fit!(machine(stand1, X)), X)\n(ordinal1 = [1, 2, 3],\n ordinal2 = CategoricalValue{Symbol,UInt32}[:x, :y, :x],\n ordinal3 = [-1.0, 0.0, 1.0],\n ordinal4 = [1.0, 0.0, -1.0],\n nominal = CategoricalValue{String,UInt32}[\"Your father\", \"he\", \"is\"],)\n\nstand2 = Standardizer(features=[:ordinal3, ], ignore=true, count=true);\n\njulia> transform(fit!(machine(stand2, X)), X)\n(ordinal1 = [-1.0, 0.0, 1.0],\n ordinal2 = CategoricalValue{Symbol,UInt32}[:x, :y, :x],\n ordinal3 = [10.0, 20.0, 30.0],\n ordinal4 = [1.0, 0.0, -1.0],\n nominal = CategoricalValue{String,UInt32}[\"Your father\", \"he\", \"is\"],)\n```\n\nSee also [`OneHotEncoder`](@ref), [`ContinuousEncoder`](@ref).\n"
":name" = "Standardizer"
":human_name" = "standardizer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":inverse_transform", ":transform"]
":hyperparameters" = "`(:features, :ignore, :ordered_factor, :count)`"
":hyperparameter_types" = "`(\"Union{Function, AbstractVector{Symbol}}\", \"Bool\", \"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.DeterministicConstantClassifier]
":input_scitype" = "`ScientificTypesBase.Table`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.DeterministicConstantClassifier"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Constant classifier (Deterministic)."
":name" = "DeterministicConstantClassifier"
":human_name" = "deterministic constant classifier"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":predict"]
":hyperparameters" = "`()`"
":hyperparameter_types" = "`()`"
":hyperparameter_ranges" = "`()`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.UnivariateTimeTypeToContinuous]
":input_scitype" = "`AbstractVector{<:ScientificTypesBase.ScientificTimeType}`"
":output_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{AbstractVector{<:ScientificTypesBase.ScientificTimeType}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`AbstractVector{<:ScientificTypesBase.ScientificTimeType}`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.UnivariateTimeTypeToContinuous"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nUnivariateTimeTypeToContinuous\n```\n\nA model type for constructing a single variable transformer that creates continuous representations of temporally typed data, based on [MLJModels.jl](https://github.com/alan-turing-institute/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nUnivariateTimeTypeToContinuous = @load UnivariateTimeTypeToContinuous pkg=MLJModels\n```\n\nDo `model = UnivariateTimeTypeToContinuous()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `UnivariateTimeTypeToContinuous(zero_time=...)`.\n\nUse this model to convert vectors with a `TimeType` element type to vectors of `Float64` type (`Continuous` element scitype).\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, x)\n```\n\nwhere\n\n  * `x`: any abstract vector whose element type is a subtype of `Dates.TimeType`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `zero_time`: the time that is to correspond to 0.0 under transformations, with the type coinciding with the training data element type. If unspecified, the earliest time encountered in training is used.\n  * `step::Period=Hour(24)`: time interval to correspond to one unit under transformation\n\n# Operations\n\n  * `transform(mach, xnew)`: apply the encoding inferred when `mach` was fit\n\n# Fitted parameters\n\n`fitted_params(mach).fitresult` is the tuple `(zero_time, step)` actually used in transformations, which may differ from the user-specified hyper-parameters.\n\n# Example\n\n```\nusing MLJ\nusing Dates\n\nx = [Date(2001, 1, 1) + Day(i) for i in 0:4]\n\nencoder = UnivariateTimeTypeToContinuous(zero_time=Date(2000, 1, 1),\n                                         step=Week(1))\n\nmach = machine(encoder, x)\nfit!(mach)\njulia> transform(mach, x)\n5-element Vector{Float64}:\n 52.285714285714285\n 52.42857142857143\n 52.57142857142857\n 52.714285714285715\n 52.857142\n```\n"
":name" = "UnivariateTimeTypeToContinuous"
":human_name" = "single variable transformer that creates continuous representations of temporally typed data"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":transform"]
":hyperparameters" = "`(:zero_time, :step)`"
":hyperparameter_types" = "`(\"Union{Nothing, Dates.TimeType}\", \"Dates.Period\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.OneHotEncoder]
":input_scitype" = "`ScientificTypesBase.Table`"
":output_scitype" = "`ScientificTypesBase.Table`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.OneHotEncoder"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nOneHotEncoder\n```\n\nA model type for constructing a one-hot encoder, based on [MLJModels.jl](https://github.com/alan-turing-institute/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nOneHotEncoder = @load OneHotEncoder pkg=MLJModels\n```\n\nDo `model = OneHotEncoder()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `OneHotEncoder(features=...)`.\n\nUse this model to one-hot encode the `Multiclass` and `OrderedFactor` features (columns) of some table, leaving other columns unchanged.\n\nNew data to be transformed may lack features present in the fit data, but no *new* features can be present.\n\n**Warning:** This transformer assumes that `levels(col)` for any `Multiclass` or `OrderedFactor` column, `col`, is the same for training data and new data to be transformed.\n\nTo ensure *all* features are transformed into `Continuous` features, or dropped, use [`ContinuousEncoder`](@ref) instead.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nwhere\n\n  * `X`: any Tables.jl compatible table. Columns can be of mixed type but only those with element scitype `Multiclass` or `OrderedFactor` can be encoded. Check column scitypes with `schema(X)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `features`: a vector of symbols (column names). If empty (default) then all `Multiclass` and `OrderedFactor` features are encoded. Otherwise, encoding is further restricted to the specified features (`ignore=false`) or the unspecified features (`ignore=true`). This default behavior can be modified by the `ordered_factor` flag.\n  * `ordered_factor=false`: when `true`, `OrderedFactor` features are universally excluded\n  * `drop_last=true`: whether to drop the column corresponding to the final class of encoded features. For example, a three-class feature is spawned into three new features if `drop_last=false`, but just two features otherwise.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach).fitresult` are:\n\n  * `all_features`: names of all features encountered in training\n  * `fitted_levels_given_feature`: dictionary of the levels associated with each feature encoded, keyed on the feature name\n  * `ref_name_pairs_given_feature`: dictionary of pairs `r => ftr` (such as `0x00000001 => :grad__A`) where `r` is a CategoricalArrays.jl reference integer representing a level, and `ftr` the corresponding new feature name; the dictionary is keyed on the names of features that are encoded\n\n**Warning:** `fitted_params(mach)` does not have a standard form and  may change in the future.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `features_to_be_encoded`: names of input features to be encoded\n  * `new_features`: names of all output features\n\n# Example\n\n```\nusing MLJ\n\nX = (name=categorical([\"Danesh\", \"Lee\", \"Mary\", \"John\"]),\n     grade=categorical([\"A\", \"B\", \"A\", \"C\"], ordered=true),\n     height=[1.85, 1.67, 1.5, 1.67],\n     n_devices=[3, 2, 4, 3])\n\njulia> schema(X)\n┌───────────┬──────────────────┐\n│ names     │ scitypes         │\n├───────────┼──────────────────┤\n│ name      │ Multiclass{4}    │\n│ grade     │ OrderedFactor{3} │\n│ height    │ Continuous       │\n│ n_devices │ Count            │\n└───────────┴──────────────────┘\n\nhot = OneHotEncoder(drop_last=true)\nmach = fit!(machine(hot, X))\nW = transform(mach, X)\n\njulia> schema(W)\n┌──────────────┬────────────┐\n│ names        │ scitypes   │\n├──────────────┼────────────┤\n│ name__Danesh │ Continuous │\n│ name__John   │ Continuous │\n│ name__Lee    │ Continuous │\n│ grade__A     │ Continuous │\n│ grade__B     │ Continuous │\n│ height       │ Continuous │\n│ n_devices    │ Count      │\n└──────────────┴────────────┘\n```\n\nSee also [`ContinuousEncoder`](@ref).\n"
":name" = "OneHotEncoder"
":human_name" = "one-hot encoder"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":transform", ":OneHotEncoder"]
":hyperparameters" = "`(:features, :drop_last, :ordered_factor, :ignore)`"
":hyperparameter_types" = "`(\"Vector{Symbol}\", \"Bool\", \"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.ContinuousEncoder]
":input_scitype" = "`ScientificTypesBase.Table`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.ContinuousEncoder"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nContinuousEncoder\n```\n\nA model type for constructing a continuous encoder, based on [MLJModels.jl](https://github.com/alan-turing-institute/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nContinuousEncoder = @load ContinuousEncoder pkg=MLJModels\n```\n\nDo `model = ContinuousEncoder()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `ContinuousEncoder(drop_last=...)`.\n\nUse this model to arrange all features (columns) of a table to have `Continuous` element scitype, by applying the following protocol to each feature `ftr`:\n\n  * If `ftr` is already `Continuous` retain it.\n  * If `ftr` is `Multiclass`, one-hot encode it.\n  * If `ftr` is `OrderedFactor`, replace it with `coerce(ftr, Continuous)` (vector of floating point integers), unless `ordered_factors=false` is specified, in which case one-hot encode it.\n  * If `ftr` is `Count`, replace it with `coerce(ftr, Continuous)`.\n  * If `ftr` has some other element scitype, or was not observed in fitting the encoder, drop it from the table.\n\n**Warning:** This transformer assumes that `levels(col)` for any `Multiclass` or `OrderedFactor` column, `col`, is the same for training data and new data to be transformed.\n\nTo selectively one-hot-encode categorical features (without dropping columns) use [`OneHotEncoder`](@ref) instead.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nwhere\n\n  * `X`: any Tables.jl compatible table. Columns can be of mixed type but only those with element scitype `Multiclass` or `OrderedFactor` can be encoded. Check column scitypes with `schema(X)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `drop_last=true`: whether to drop the column corresponding to the final class of one-hot encoded features. For example, a three-class feature is spawned into three new features if `drop_last=false`, but two just features otherwise.\n  * `one_hot_ordered_factors=false`: whether to one-hot any feature with `OrderedFactor` element scitype, or to instead coerce it directly to a (single) `Continuous` feature using the order\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `features_to_keep`: names of features that will not be dropped from the table\n  * `one_hot_encoder`: the `OneHotEncoder` model instance for handling the one-hot encoding\n  * `one_hot_encoder_fitresult`: the fitted parameters of the `OneHotEncoder` model\n\n# Report\n\n  * `features_to_keep`: names of input features that will not be dropped from the table\n  * `new_features`: names of all output features\n\n# Example\n\n```julia\nX = (name=categorical([\"Danesh\", \"Lee\", \"Mary\", \"John\"]),\n     grade=categorical([\"A\", \"B\", \"A\", \"C\"], ordered=true),\n     height=[1.85, 1.67, 1.5, 1.67],\n     n_devices=[3, 2, 4, 3],\n     comments=[\"the force\", \"be\", \"with you\", \"too\"])\n\njulia> schema(X)\n┌───────────┬──────────────────┐\n│ names     │ scitypes         │\n├───────────┼──────────────────┤\n│ name      │ Multiclass{4}    │\n│ grade     │ OrderedFactor{3} │\n│ height    │ Continuous       │\n│ n_devices │ Count            │\n│ comments  │ Textual          │\n└───────────┴──────────────────┘\n\nencoder = ContinuousEncoder(drop_last=true)\nmach = fit!(machine(encoder, X))\nW = transform(mach, X)\n\njulia> schema(W)\n┌──────────────┬────────────┐\n│ names        │ scitypes   │\n├──────────────┼────────────┤\n│ name__Danesh │ Continuous │\n│ name__John   │ Continuous │\n│ name__Lee    │ Continuous │\n│ grade        │ Continuous │\n│ height       │ Continuous │\n│ n_devices    │ Continuous │\n└──────────────┴────────────┘\n\njulia> setdiff(schema(X).names, report(mach).features_to_keep) # dropped features\n1-element Vector{Symbol}:\n :comments\n\n```\n\nSee also [`OneHotEncoder`](@ref)\n"
":name" = "ContinuousEncoder"
":human_name" = "continuous encoder"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":fitted_params", ":transform", ":ContinuousEncoder"]
":hyperparameters" = "`(:drop_last, :one_hot_ordered_factors)`"
":hyperparameter_types" = "`(\"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.UnivariateBoxCoxTransformer]
":input_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":output_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.UnivariateBoxCoxTransformer"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nUnivariateBoxCoxTransformer\n```\n\nA model type for constructing a single variable Box-Cox transformer, based on [MLJModels.jl](https://github.com/alan-turing-institute/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nUnivariateBoxCoxTransformer = @load UnivariateBoxCoxTransformer pkg=MLJModels\n```\n\nDo `model = UnivariateBoxCoxTransformer()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `UnivariateBoxCoxTransformer(n=...)`.\n\nBox-Cox transformations attempt to make data look more normally distributed. This can improve performance and assist in the interpretation of models which suppose that data is generated by a normal distribution.\n\nA Box-Cox transformation (with shift) is of the form\n\n```\nx -> ((x + c)^λ - 1)/λ\n```\n\nfor some constant `c` and real `λ`, unless `λ = 0`, in which case the above is replaced with\n\n```\nx -> log(x + c)\n```\n\nGiven user-specified hyper-parameters `n::Integer` and `shift::Bool`, the present implementation learns the parameters `c` and `λ` from the training data as follows: If `shift=true` and zeros are encountered in the data, then `c` is set to `0.2` times the data mean.  If there are no zeros, then no shift is applied. Finally, `n` different values of `λ` between `-0.4` and `3` are considered, with `λ` fixed to the value maximizing normality of the transformed data.\n\n*Reference:* [Wikipedia entry for power  transform](https://en.wikipedia.org/wiki/Power_transform).\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, x)\n```\n\nwhere\n\n  * `x`: any abstract vector with element scitype `Continuous`; check the scitype with `scitype(x)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `n=171`: number of values of the exponent `λ` to try\n  * `shift=false`: whether to include a preliminary constant translation in transformations, in the presence of zeros\n\n# Operations\n\n  * `transform(mach, xnew)`: apply the Box-Cox transformation learned when fitting `mach`\n  * `inverse_transform(mach, z)`: reconstruct the vector `z` whose transformation learned by `mach` is `z`\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `λ`: the learned Box-Cox exponent\n  * `c`: the learned shift\n\n# Examples\n\n```\nusing MLJ\nusing UnicodePlots\nusing Random\nRandom.seed!(123)\n\ntransf = UnivariateBoxCoxTransformer()\n\nx = randn(1000).^2\n\nmach = machine(transf, x)\nfit!(mach)\n\nz = transform(mach, x)\n\njulia> histogram(x)\n                ┌                                        ┐\n   [ 0.0,  2.0) ┤███████████████████████████████████  848\n   [ 2.0,  4.0) ┤████▌ 109\n   [ 4.0,  6.0) ┤█▍ 33\n   [ 6.0,  8.0) ┤▍ 7\n   [ 8.0, 10.0) ┤▏ 2\n   [10.0, 12.0) ┤  0\n   [12.0, 14.0) ┤▏ 1\n                └                                        ┘\n                                 Frequency\n\njulia> histogram(z)\n                ┌                                        ┐\n   [-5.0, -4.0) ┤█▎ 8\n   [-4.0, -3.0) ┤████████▊ 64\n   [-3.0, -2.0) ┤█████████████████████▊ 159\n   [-2.0, -1.0) ┤█████████████████████████████▊ 216\n   [-1.0,  0.0) ┤███████████████████████████████████  254\n   [ 0.0,  1.0) ┤█████████████████████████▊ 188\n   [ 1.0,  2.0) ┤████████████▍ 90\n   [ 2.0,  3.0) ┤██▊ 20\n   [ 3.0,  4.0) ┤▎ 1\n                └                                        ┘\n                                 Frequency\n\n```\n"
":name" = "UnivariateBoxCoxTransformer"
":human_name" = "single variable Box-Cox transformer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":fitted_params", ":inverse_transform", ":transform", ":UnivariateBoxCoxTransformer"]
":hyperparameters" = "`(:n, :shift)`"
":hyperparameter_types" = "`(\"Int64\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.InteractionTransformer]
":input_scitype" = "`ScientificTypesBase.Table`"
":output_scitype" = "`ScientificTypesBase.Table`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.InteractionTransformer"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nInteractionTransformer\n```\n\nA model type for constructing a interaction transformer, based on [MLJModels.jl](https://github.com/alan-turing-institute/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nInteractionTransformer = @load InteractionTransformer pkg=MLJModels\n```\n\nDo `model = InteractionTransformer()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `InteractionTransformer(order=...)`.\n\nGenerates all polynomial interaction terms up to the given order for the subset of chosen columns.  Any column that contains elements with scitype `<:Infinite` is a valid basis to generate interactions.  If `features` is not specified, all such columns with scitype `<:Infinite` in the table are used as a basis.\n\nIn MLJ or MLJBase, you can transform features `X` with the single call\n\n```\ntransform(machine(model), X)\n```\n\nSee also the example below.\n\n# Hyper-parameters\n\n  * `order`: Maximum order of interactions to be generated.\n  * `features`: Restricts interations generation to those columns\n\n# Operations\n\n  * `transform(machine(model), X)`: Generates polynomial interaction terms out of table `X` using the hyper-parameters specified in `model`.\n\n# Example\n\n```\nusing MLJ\n\nX = (\n    A = [1, 2, 3],\n    B = [4, 5, 6],\n    C = [7, 8, 9],\n    D = [\"x₁\", \"x₂\", \"x₃\"]\n)\nit = InteractionTransformer(order=3)\nmach = machine(it)\n\njulia> transform(mach, X)\n(A = [1, 2, 3],\n B = [4, 5, 6],\n C = [7, 8, 9],\n D = [\"x₁\", \"x₂\", \"x₃\"],\n A_B = [4, 10, 18],\n A_C = [7, 16, 27],\n B_C = [28, 40, 54],\n A_B_C = [28, 80, 162],)\n\nit = InteractionTransformer(order=2, features=[:A, :B])\nmach = machine(it)\n\njulia> transform(mach, X)\n(A = [1, 2, 3],\n B = [4, 5, 6],\n C = [7, 8, 9],\n D = [\"x₁\", \"x₂\", \"x₃\"],\n A_B = [4, 10, 18],)\n\n```\n"
":name" = "InteractionTransformer"
":human_name" = "interaction transformer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Static`"
":implemented_methods" = [":clean!", ":transform"]
":hyperparameters" = "`(:order, :features)`"
":hyperparameter_types" = "`(\"Int64\", \"Union{Nothing, Vector{Symbol}}\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.ConstantRegressor]
":input_scitype" = "`ScientificTypesBase.Table`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.ConstantRegressor"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Constant regressor (Probabilistic)."
":name" = "ConstantRegressor"
":human_name" = "constant regressor"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":fitted_params", ":predict"]
":hyperparameters" = "`(:distribution_type,)`"
":hyperparameter_types" = "`(\"Type{D} where D<:Distributions.Sampleable\",)`"
":hyperparameter_ranges" = "`(nothing,)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.FeatureSelector]
":input_scitype" = "`ScientificTypesBase.Table`"
":output_scitype" = "`ScientificTypesBase.Table`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.FeatureSelector"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nFeatureSelector\n```\n\nA model type for constructing a feature selector, based on [MLJModels.jl](https://github.com/alan-turing-institute/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nFeatureSelector = @load FeatureSelector pkg=MLJModels\n```\n\nDo `model = FeatureSelector()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `FeatureSelector(features=...)`.\n\nUse this model to select features (columns) of a table, usually as part of a model `Pipeline`.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nwhere\n\n  * `X`: any table of input features, where \"table\" is in the sense of Tables.jl\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `features`: one of the following, with the behavior indicated:\n\n      * `[]` (empty, the default): filter out all features (columns) which were not encountered in training\n      * non-empty vector of feature names (symbols): keep only the specified features (`ignore=false`) or keep only unspecified features (`ignore=true`)\n      * function or other callable: keep a feature if the callable returns `true` on its name. For example, specifying `FeatureSelector(features = name -> name in [:x1, :x3], ignore = true)` has the same effect as `FeatureSelector(features = [:x1, :x3], ignore = true)`, namely to select all features, with the exception of `:x1` and `:x3`.\n  * `ignore`: whether to ignore or keep specified `features`, as explained above\n\n# Operations\n\n  * `transform(mach, Xnew)`: select features from the table `Xnew` as specified by the model, taking features seen during training into account, if relevant\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `features_to_keep`: the features that will be selected\n\n# Example\n\n```\nusing MLJ\n\nX = (ordinal1 = [1, 2, 3],\n     ordinal2 = coerce([\"x\", \"y\", \"x\"], OrderedFactor),\n     ordinal3 = [10.0, 20.0, 30.0],\n     ordinal4 = [-20.0, -30.0, -40.0],\n     nominal = coerce([\"Your father\", \"he\", \"is\"], Multiclass));\n\nselector = FeatureSelector(features=[:ordinal3, ], ignore=true);\n\njulia> transform(fit!(machine(selector, X)), X)\n(ordinal1 = [1, 2, 3],\n ordinal2 = CategoricalValue{Symbol,UInt32}[\"x\", \"y\", \"x\"],\n ordinal4 = [-20.0, -30.0, -40.0],\n nominal = CategoricalValue{String,UInt32}[\"Your father\", \"he\", \"is\"],)\n\n```\n"
":name" = "FeatureSelector"
":human_name" = "feature selector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":transform"]
":hyperparameters" = "`(:features, :ignore)`"
":hyperparameter_types" = "`(\"Union{Function, Vector{Symbol}}\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.UnivariateDiscretizer]
":input_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{AbstractVector{<:ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor}`"
":inverse_transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.UnivariateDiscretizer"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nUnivariateDiscretizer\n```\n\nA model type for constructing a single variable discretizer, based on [MLJModels.jl](https://github.com/alan-turing-institute/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nUnivariateDiscretizer = @load UnivariateDiscretizer pkg=MLJModels\n```\n\nDo `model = UnivariateDiscretizer()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `UnivariateDiscretizer(n_classes=...)`.\n\nDiscretization converts a `Continuous` vector into an `OrderedFactor` vector. In particular, the output is a `CategoricalVector` (whose reference type is optimized).\n\nThe transformation is chosen so that the vector on which the transformer is fit has, in transformed form, an approximately uniform distribution of values. Specifically, if `n_classes` is the level of discretization, then `2*n_classes - 1` ordered quantiles are computed, the odd quantiles being used for transforming (discretization) and the even quantiles for inverse transforming.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, x)\n```\n\nwhere\n\n  * `x`: any abstract vector with `Continuous` element scitype; check scitype with `scitype(x)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `n_classes`: number of discrete classes in the output\n\n# Operations\n\n  * `transform(mach, xnew)`: discretize `xnew` according to the discretization learned when fitting `mach`\n  * `inverse_transform(mach, z)`: attempt to reconstruct from `z` a vector that transforms to give `z`\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach).fitesult` include:\n\n  * `odd_quantiles`: quantiles used for transforming (length is `n_classes - 1`)\n  * `even_quantiles`: quantiles used for inverse transforming (length is `n_classes`)\n\n**Warning.** `fitted_params(mach)` does not have a standard form and  may change in the future.\n\n# Example\n\n```\nusing MLJ\nusing Random\nRandom.seed!(123)\n\ndiscretizer = UnivariateDiscretizer(n_classes=100)\nmach = machine(discretizer, randn(1000))\nfit!(mach)\n\njulia> x = rand(5)\n5-element Vector{Float64}:\n 0.6342070799721164\n 0.8681793651724181\n 0.43780421808821424\n 0.5740792503574783\n 0.22444170437768007\n\njulia> z = transform(mach, x)\n5-element CategoricalArrays.CategoricalArray{UInt8,1,UInt8}:\n 0x49\n 0x50\n 0x43\n 0x47\n 0x3a5\n\njulia> x_approx = inverse_transform(mach, z)\n5-element Vector{Float64}:\n 0.6333797607904535\n 0.855839325856769\n 0.433203047224622\n 0.5662624832429449\n 0.222065923759177\n```\n"
":name" = "UnivariateDiscretizer"
":human_name" = "single variable discretizer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":inverse_transform", ":transform", ":UnivariateDiscretizer"]
":hyperparameters" = "`(:n_classes,)`"
":hyperparameter_types" = "`(\"Int64\",)`"
":hyperparameter_ranges" = "`(nothing,)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.FillImputer]
":input_scitype" = "`ScientificTypesBase.Table`"
":output_scitype" = "`ScientificTypesBase.Table`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.FillImputer"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nFillImputer\n```\n\nA model type for constructing a fill imputer, based on [MLJModels.jl](https://github.com/alan-turing-institute/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nFillImputer = @load FillImputer pkg=MLJModels\n```\n\nDo `model = FillImputer()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `FillImputer(features=...)`.\n\nUse this model to impute `missing` values in tabular data. A fixed \"filler\" value is learned from the training data, one for each column of the table.\n\nFor imputing missing values in a vector, use [`UnivariateFillImputer`](@ref) instead.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have element scitypes `Union{Missing, T}`, where `T` is a subtype of `Continuous`, `Multiclass`, `OrderedFactor` or `Count`. Check scitypes with `schema(X)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `features`: a vector of names of features (symbols) for which imputation is to be attempted; default is empty, which is interpreted as \"impute all\".\n  * `continuous_fill`: function or other callable to determine value to be imputed in the case of `Continuous` (abstract float) data; default is to apply `median` after skipping `missing` values\n  * `count_fill`: function or other callable to determine value to be imputed in the case of `Count` (integer) data; default is to apply rounded `median` after skipping `missing` values\n  * `finite_fill`: function or other callable to determine value to be imputed in the case of `Multiclass` or `OrderedFactor` data (categorical vectors); default is to apply `mode` after skipping `missing` values\n\n# Operations\n\n  * `transform(mach, Xnew)`: return `Xnew` with missing values imputed with the fill values learned when fitting `mach`\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `features_seen_in_fit`: the names of features (columns) encountered during training\n  * `univariate_transformer`: the univariate model applied to determine   the fillers (it's fields contain the functions defining the filler computations)\n  * `filler_given_feature`: dictionary of filler values, keyed on feature (column) names\n\n# Examples\n\n```\nusing MLJ\nimputer = FillImputer()\n\nX = (a = [1.0, 2.0, missing, 3.0, missing],\n     b = coerce([\"y\", \"n\", \"y\", missing, \"y\"], Multiclass),\n     c = [1, 1, 2, missing, 3])\n\nschema(X)\njulia> schema(X)\n┌───────┬───────────────────────────────┐\n│ names │ scitypes                      │\n├───────┼───────────────────────────────┤\n│ a     │ Union{Missing, Continuous}    │\n│ b     │ Union{Missing, Multiclass{2}} │\n│ c     │ Union{Missing, Count}         │\n└───────┴───────────────────────────────┘\n\nmach = machine(imputer, X)\nfit!(mach)\n\njulia> fitted_params(mach).filler_given_feature\n(filler = 2.0,)\n\njulia> fitted_params(mach).filler_given_feature\nDict{Symbol, Any} with 3 entries:\n  :a => 2.0\n  :b => \"y\"\n  :c => 2\n\njulia> transform(mach, X)\n(a = [1.0, 2.0, 2.0, 3.0, 2.0],\n b = CategoricalValue{String, UInt32}[\"y\", \"n\", \"y\", \"y\", \"y\"],\n c = [1, 1, 2, 2, 3],)\n```\n\nSee also [`UnivariateFillImputer`](@ref).\n"
":name" = "FillImputer"
":human_name" = "fill imputer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":fitted_params", ":transform", ":FillImputer"]
":hyperparameters" = "`(:features, :continuous_fill, :count_fill, :finite_fill)`"
":hyperparameter_types" = "`(\"Vector{Symbol}\", \"Function\", \"Function\", \"Function\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.DeterministicConstantRegressor]
":input_scitype" = "`ScientificTypesBase.Table`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.DeterministicConstantRegressor"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Constant regressor (Deterministic)."
":name" = "DeterministicConstantRegressor"
":human_name" = "deterministic constant regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":predict"]
":hyperparameters" = "`()`"
":hyperparameter_types" = "`()`"
":hyperparameter_ranges" = "`()`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.UnivariateStandardizer]
":input_scitype" = "`AbstractVector{<:ScientificTypesBase.Infinite}`"
":output_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{AbstractVector{<:ScientificTypesBase.Infinite}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Infinite}`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.UnivariateStandardizer"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nUnivariateStandardizer()\n```\n\nTransformer type for standardizing (whitening) single variable data.\n\nThis model may be deprecated in the future. Consider using [`Standardizer`](@ref), which handles both tabular *and* univariate data.\n"
":name" = "UnivariateStandardizer"
":human_name" = "single variable discretizer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":fitted_params", ":inverse_transform", ":transform"]
":hyperparameters" = "`()`"
":hyperparameter_types" = "`()`"
":hyperparameter_ranges" = "`()`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.UnivariateFillImputer]
":input_scitype" = "`Union{AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.Count}}, AbstractVector{<:Union{Missing, ScientificTypesBase.Finite}}}`"
":output_scitype" = "`Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{Union{AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.Count}}, AbstractVector{<:Union{Missing, ScientificTypesBase.Finite}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":inverse_transform_scitype" = "`Union{AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.Count}}, AbstractVector{<:Union{Missing, ScientificTypesBase.Finite}}}`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.UnivariateFillImputer"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nUnivariateFillImputer\n```\n\nA model type for constructing a single variable fill imputer, based on [MLJModels.jl](https://github.com/alan-turing-institute/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nUnivariateFillImputer = @load UnivariateFillImputer pkg=MLJModels\n```\n\nDo `model = UnivariateFillImputer()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `UnivariateFillImputer(continuous_fill=...)`.\n\nUse this model to imputing `missing` values in a vector with a fixed value learned from the non-missing values of training vector.\n\nFor imputing missing values in tabular data, use [`FillImputer`](@ref) instead.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, x)\n```\n\nwhere\n\n  * `x`: any abstract vector with element scitype `Union{Missing, T}` where `T` is a subtype of `Continuous`, `Multiclass`, `OrderedFactor` or `Count`; check scitype using `scitype(x)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `continuous_fill`: function or other callable to determine value to be imputed in the case of `Continuous` (abstract float) data; default is to apply `median` after skipping `missing` values\n  * `count_fill`: function or other callable to determine value to be imputed in the case of `Count` (integer) data; default is to apply rounded `median` after skipping `missing` values\n  * `finite_fill`: function or other callable to determine value to be imputed in the case of `Multiclass` or `OrderedFactor` data (categorical vectors); default is to apply `mode` after skipping `missing` values\n\n# Operations\n\n  * `transform(mach, xnew)`: return `xnew` with missing values imputed with the fill values learned when fitting `mach`\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `filler`: the fill value to be imputed in all new data\n\n# Examples\n\n```\nusing MLJ\nimputer = UnivariateFillImputer()\n\nx_continuous = [1.0, 2.0, missing, 3.0]\nx_multiclass = coerce([\"y\", \"n\", \"y\", missing, \"y\"], Multiclass)\nx_count = [1, 1, 1, 2, missing, 3, 3]\n\nmach = machine(imputer, x_continuous)\nfit!(mach)\n\njulia> fitted_params(mach)\n(filler = 2.0,)\n\njulia> transform(mach, [missing, missing, 101.0])\n3-element Vector{Float64}:\n 2.0\n 2.0\n 101.0\n\nmach2 = machine(imputer, x_multiclass) |> fit!\n\njulia> transform(mach2, x_multiclass)\n5-element CategoricalArray{String,1,UInt32}:\n \"y\"\n \"n\"\n \"y\"\n \"y\"\n \"y\"\n\nmach3 = machine(imputer, x_count) |> fit!\n\njulia> transform(mach3, [missing, missing, 5])\n3-element Vector{Int64}:\n 2\n 2\n 5\n```\n\nFor imputing tabular data, use [`FillImputer`](@ref).\n"
":name" = "UnivariateFillImputer"
":human_name" = "single variable fill imputer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":fitted_params", ":transform", ":UnivariateFillImputer"]
":hyperparameters" = "`(:continuous_fill, :count_fill, :finite_fill)`"
":hyperparameter_types" = "`(\"Function\", \"Function\", \"Function\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.MCDDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.MCDDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nMCDDetector(store_precision = true,\n                 assume_centered = false,\n                 support_fraction = nothing,\n                 random_state = nothing)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.mcd](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.mcd)\n"
":name" = "MCDDetector"
":human_name" = "mcd detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:store_precision, :assume_centered, :support_fraction, :random_state)`"
":hyperparameter_types" = "`(\"Bool\", \"Bool\", \"Union{Nothing, Real}\", \"Union{Nothing, Integer}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.COPODDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.COPODDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nCOPODDetector()\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.copod](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.copod)\n"
":name" = "COPODDetector"
":human_name" = "copod detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`()`"
":hyperparameter_types" = "`()`"
":hyperparameter_ranges" = "`()`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.HBOSDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.HBOSDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nHBOSDetector(n_bins = 10,\n          alpha = 0.1,\n          tol = 0.5)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.hbos](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.hbos)\n"
":name" = "HBOSDetector"
":human_name" = "hbos detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:n_bins, :alpha, :tol)`"
":hyperparameter_types" = "`(\"Integer\", \"Real\", \"Real\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.IForestDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.IForestDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nIForestDetector(n_estimators = 100,\n                     max_samples = \"auto\",\n                     max_features = 1.0\n                     bootstrap = false,\n                     behaviour = \"new\",\n                     random_state = nothing,\n                     verbose = 0,\n                     n_jobs = 1)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.iforest](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.iforest)\n"
":name" = "IForestDetector"
":human_name" = "i forest detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:n_estimators, :max_samples, :max_features, :bootstrap, :behaviour, :random_state, :verbose, :n_jobs)`"
":hyperparameter_types" = "`(\"Integer\", \"Union{Real, String}\", \"Real\", \"Bool\", \"String\", \"Union{Nothing, Integer}\", \"Integer\", \"Integer\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.SOSDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.SOSDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nSOSDetector(perplexity = 4.5,\n         metric = \"minkowski\",\n         eps = 1e-5)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.sos](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.sos)\n"
":name" = "SOSDetector"
":human_name" = "sos detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:perplexity, :metric, :eps)`"
":hyperparameter_types" = "`(\"Real\", \"String\", \"Real\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.ABODDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.ABODDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nABODDetector(n_neighbors = 5,\n                  method = \"fast\")\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.abod](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.abod)\n"
":name" = "ABODDetector"
":human_name" = "abod detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:n_neighbors, :method)`"
":hyperparameter_types" = "`(\"Integer\", \"String\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.LOFDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.LOFDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLOFDetector(n_neighbors = 5,\n                 method = \"largest\",\n                 algorithm = \"auto\",\n                 leaf_size = 30,\n                 metric = \"minkowski\",\n                 p = 2,\n                 metric_params = nothing,\n                 n_jobs = 1)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lof](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lof)\n"
":name" = "LOFDetector"
":human_name" = "lof detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:n_neighbors, :algorithm, :leaf_size, :metric, :p, :metric_params, :n_jobs)`"
":hyperparameter_types" = "`(\"Integer\", \"String\", \"Integer\", \"String\", \"Union{Nothing, Integer}\", \"Any\", \"Integer\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.PCADetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.PCADetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nPCADetector(n_components = nothing,\n                 n_selected_components = nothing,\n                 copy = true,\n                 whiten = false,\n                 svd_solver = \"auto\",\n                 tol = 0.0\n                 iterated_power = \"auto\",\n                 standardization = true,\n                 weighted = true,\n                 random_state = nothing)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.pca](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.pca)\n"
":name" = "PCADetector"
":human_name" = "pca detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:n_components, :n_selected_components, :copy, :whiten, :svd_solver, :tol, :iterated_power, :standardization, :weighted, :random_state)`"
":hyperparameter_types" = "`(\"Union{Nothing, Real}\", \"Union{Nothing, Integer}\", \"Bool\", \"Bool\", \"String\", \"Real\", \"Union{Integer, String}\", \"Bool\", \"Bool\", \"Union{Nothing, Integer}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.OCSVMDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.OCSVMDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nOCSVMDetector(kernel = \"rbf\",\n                   degree = 3,\n                   gamma = \"auto\",\n                   coef0 = 0.0,\n                   tol = 0.001,\n                   nu = 0.5,\n                   shrinking = true,\n                   cache_size = 200,\n                   verbose = false,\n                   max_iter = -1)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.ocsvm](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.ocsvm)\n"
":name" = "OCSVMDetector"
":human_name" = "ocsvm detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:kernel, :degree, :gamma, :coef0, :tol, :nu, :shrinking, :cache_size, :verbose, :max_iter)`"
":hyperparameter_types" = "`(\"String\", \"Integer\", \"Union{Real, String}\", \"Real\", \"Real\", \"Real\", \"Bool\", \"Integer\", \"Bool\", \"Integer\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.SODDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.SODDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nSODDetector(n_neighbors = 5,\n                 ref_set = 10,\n                 alpha = 0.8)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.sod](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.sod)\n"
":name" = "SODDetector"
":human_name" = "sod detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:n_neighbors, :ref_set, :alpha)`"
":hyperparameter_types" = "`(\"Integer\", \"Integer\", \"Real\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.LODADetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.LODADetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLODADetector(n_bins = 10,\n                  n_random_cuts = 100)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.loda](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.loda)\n"
":name" = "LODADetector"
":human_name" = "loda detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:n_bins, :n_random_cuts)`"
":hyperparameter_types" = "`(\"Integer\", \"Integer\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.KNNDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.KNNDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nKNNDetector(n_neighbors = 5,\n         method = \"largest\",\n         radius = 1.0,\n         algorithm = \"auto\",\n         leaf_size = 30,\n         metric = \"minkowski\",\n         p = 2,\n         metric_params = nothing,\n         n_jobs = 1)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.knn](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.knn)\n"
":name" = "KNNDetector"
":human_name" = "knn detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:n_neighbors, :method, :radius, :algorithm, :leaf_size, :metric, :p, :metric_params, :n_jobs)`"
":hyperparameter_types" = "`(\"Integer\", \"String\", \"Real\", \"String\", \"Integer\", \"String\", \"Union{Nothing, Integer}\", \"Any\", \"Integer\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.COFDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.COFDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nCOFDetector(n_neighbors = 5)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cof](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cof)\n"
":name" = "COFDetector"
":human_name" = "cof detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:n_neighbors,)`"
":hyperparameter_types" = "`(\"Integer\",)`"
":hyperparameter_ranges" = "`(nothing,)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.CBLOFDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.CBLOFDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nCBLOFDetector(n_clusters = 8,\n                   alpha = 0.9,\n                   beta = 5,\n                   use_weights = false,\n                   random_state = nothing,\n                   n_jobs = 1)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cblof](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cblof)\n"
":name" = "CBLOFDetector"
":human_name" = "cblof detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:n_clusters, :alpha, :beta, :use_weights, :random_state, :n_jobs)`"
":hyperparameter_types" = "`(\"Integer\", \"Real\", \"Real\", \"Bool\", \"Union{Nothing, Integer}\", \"Integer\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.LOCIDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.LOCIDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLOCIDetector(alpha = 0.5,\n                  k = 3)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.loci](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.loci)\n"
":name" = "LOCIDetector"
":human_name" = "loci detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:alpha, :k)`"
":hyperparameter_types" = "`(\"Real\", \"Real\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.LMDDDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.LMDDDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLMDDDetector(n_iter = 50,\n                  dis_measure = \"aad\",\n                  random_state = nothing)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lmdd](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lmdd)\n"
":name" = "LMDDDetector"
":human_name" = "lmdd detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:n_iter, :dis_measure, :random_state)`"
":hyperparameter_types" = "`(\"Integer\", \"String\", \"Union{Nothing, Integer}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.RODDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.RODDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nRODDetector(parallel_execution = false)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.rod](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.rod)\n"
":name" = "RODDetector"
":human_name" = "rod detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:parallel_execution,)`"
":hyperparameter_types" = "`(\"Bool\",)`"
":hyperparameter_ranges" = "`(nothing,)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OneRule.OneRuleClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Finite}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "OneRule"
":package_license" = "MIT"
":load_path" = "OneRule.OneRuleClassifier"
":package_uuid" = "90484964-6d6a-4979-af09-8657dbed84ff"
":package_url" = "https://github.com/roland-KA/OneRule.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nOneRuleClassifier\n```\n\nA model type for constructing a one rule classifier, based on [OneRule.jl](https://github.com/roland-KA/OneRule.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nOneRuleClassifier = @load OneRuleClassifier pkg=OneRule\n```\n\nDo `model = OneRuleClassifier()` to construct an instance with default hyper-parameters. \n\n`OneRuleClassifier` implements the OneRule method for classification by Robert Holte      (\"Very simple classification rules perform well on most commonly used datasets\"      in: Machine Learning 11.1 (1993), pp. 63-90). \n\n```\nFor more information see:\n\n- Witten, Ian H., Eibe Frank, and Mark A. Hall. \n  Data Mining Practical Machine Learning Tools and Techniques Third Edition. \n  Morgan Kaufmann, 2017, pp. 93-96.\n- [Machine Learning - (One|Simple) Rule](https://datacadamia.com/data_mining/one_rule)\n- [OneRClassifier - One Rule for Classification](http://rasbt.github.io/mlxtend/user_guide/classifier/OneRClassifier/)\n```\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with     mach = machine(model, X, y) where\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have one of the following element scitypes: `Multiclass`, `OrderedFactor`, or `<:Finite`; check column scitypes with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `OrderedFactor` or `Multiclass`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\nThis classifier has no hyper-parameters.\n\n# Operations\n\n  * `predict(mach, Xnew)`: return (deterministic) predictions of the target given features `Xnew` having the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `tree`: the tree (a `OneTree`) returned by the core OneTree.jl algorithm\n  * `all_classes`: all classes (i.e. levels) of the target (used also internally to transfer `levels`-information to `predict`)\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `tree`: The `OneTree` created based on the training data\n  * `nrules`: The number of rules `tree` contains\n  * `error_rate`: fraction of wrongly classified instances\n  * `error_count`: number of wrongly classified instances\n  * `classes_seen`: list of target classes actually observed in training\n  * `features`: the names of the features encountered in training\n\n# Examples\n\n```\nusing MLJ\n\nORClassifier = @load OneRuleClassifier pkg=OneRule\n\norc = ORClassifier()\n\noutlook = [\"sunny\", \"sunny\", \"overcast\", \"rainy\", \"rainy\", \"rainy\", \"overcast\", \"sunny\", \"sunny\", \"rainy\",  \"sunny\", \"overcast\", \"overcast\", \"rainy\"]\ntemperature = [\"hot\", \"hot\", \"hot\", \"mild\", \"cool\", \"cool\", \"cool\", \"mild\", \"cool\", \"mild\", \"mild\", \"mild\", \"hot\", \"mild\"]\nhumidity = [\"high\", \"high\", \"high\", \"high\", \"normal\", \"normal\", \"normal\", \"high\", \"normal\", \"normal\", \"normal\", \"high\", \"normal\", \"high\"]\nwindy = [\"false\", \"true\", \"false\", \"false\", \"false\", \"true\", \"true\", \"false\", \"false\", \"false\", \"true\", \"true\", \"false\", \"true\"]\n\nweather_data = (outlook = outlook, temperature = temperature, humidity = humidity, windy = windy)\nplay_data = [\"no\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\"]\n\nweather = coerce(weather_data, Textual => Multiclass)\nplay = coerce(play, Multiclass)\n\nmach = machine(orc, weather, play)\nfit!(mach)\n\nyhat = MLJ.predict(mach, weather)       # in a real context 'new' `weather` data would be used\none_tree = fitted_params(mach).tree\nreport(mach).error_rate\n```\n\nSee also [OneRule.jl](https://github.com/roland-KA/OneRule.jl).\n"
":name" = "OneRuleClassifier"
":human_name" = "one rule classifier"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`()`"
":hyperparameter_types" = "`()`"
":hyperparameter_ranges" = "`()`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[LIBSVM.EpsilonSVR]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "LIBSVM"
":package_license" = "unknown"
":load_path" = "MLJLIBSVMInterface.EpsilonSVR"
":package_uuid" = "b1bec4e5-fd48-53fe-b0cb-9723c09d164b"
":package_url" = "https://github.com/mpastell/LIBSVM.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nEpsilonSVR\n```\n\nA model type for constructing a ϵ-support vector regressor, based on [LIBSVM.jl](https://github.com/mpastell/LIBSVM.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nEpsilonSVR = @load EpsilonSVR pkg=LIBSVM\n```\n\nDo `model = EpsilonSVR()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `EpsilonSVR(kernel=...)`.\n\nReference for algorithm and core C-library: C.-C. Chang and C.-J. Lin (2011): \"LIBSVM: a library for support vector machines.\" *ACM Transactions on Intelligent Systems and Technology*, 2(3):27:1–27:27. Updated at [https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf](https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf). \n\nThis model is an adaptation of the classifier `SVC` to regression, but has an additional parameter `epsilon` (denoted $ϵ$ in the cited reference).\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with:\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have `Continuous` element scitype; check column scitypes with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `Continuous`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `kernel=LIBSVM.Kernel.RadialBasis`: either an object that can be called, as in `kernel(x1, x2)`, or one of the built-in kernels from the LIBSVM.jl package listed below.  Here `x1` and `x2` are vectors whose lengths match the number of columns of the training data `X` (see \"Examples\" below).\n\n      * `LIBSVM.Kernel.Linear`: `(x1, x2) -> x1'*x2`\n      * `LIBSVM.Kernel.Polynomial`: `(x1, x2) -> gamma*x1'*x2 + coef0)^degree`\n      * `LIBSVM.Kernel.RadialBasis`: `(x1, x2) -> (exp(-gamma*norm(x1 - x2)^2))`\n      * `LIBSVM.Kernel.Sigmoid`: `(x1, x2) - > tanh(gamma*x1'*x2 + coef0)`\n\n    Here `gamma`, `coef0`, `degree` are other hyper-parameters. Serialization of models with user-defined kernels comes with some restrictions. See [LIVSVM.jl issue91](https://github.com/JuliaML/LIBSVM.jl/issues/91)\n  * `gamma = 0.0`: kernel parameter (see above); if `gamma==-1.0` then `gamma = 1/nfeatures` is used in training, where `nfeatures` is the number of features (columns of `X`).  If `gamma==0.0` then `gamma = 1/(var(Tables.matrix(X))*nfeatures)` is used. Actual value used appears in the report (see below).\n  * `coef0 = 0.0`: kernel parameter (see above)\n  * `degree::Int32 = Int32(3)`: degree in polynomial kernel (see above)\n\n  * `cost=1.0` (range (0, `Inf`)): the parameter denoted $C$ in the cited reference; for greater regularization, decrease `cost`\n  * `epsilon=0.1` (range (0, `Inf`)): the parameter denoted $ϵ$ in the cited reference; `epsilon` is the thickness of the penalty-free neighborhood of the graph of the prediction function (\"slab\" or \"tube\"). Specifically, a data point `(x, y)` incurs no training loss unless it is outside this neighborhood; the further away it is from the this neighborhood, the greater the loss penalty.\n  * `cachesize=200.0` cache memory size in MB\n  * `tolerance=0.001`: tolerance for the stopping criterion\n  * `shrinking=true`: whether to use shrinking heuristics\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given features `Xnew` having the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `libsvm_model`: the trained model object created by the LIBSVM.jl package\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `gamma`: actual value of the kernel parameter `gamma` used in training\n\n# Examples\n\n## Using a built-in kernel\n\n```\nusing MLJ\nimport LIBSVM\n\nEpsilonSVR = @load EpsilonSVR pkg=LIBSVM            # model type\nmodel = EpsilonSVR(kernel=LIBSVM.Kernel.Polynomial) # instance\n\nX, y = make_regression(rng=123) # table, vector\nmach = machine(model, X, y) |> fit!\n\nXnew, _ = make_regression(3, rng=123)\n\njulia> yhat = predict(mach, Xnew)\n3-element Vector{Float64}:\n  0.2512132502584155\n  0.007340201523624579\n -0.2482949812264707\n```\n\n## User-defined kernels\n\n```\nk(x1, x2) = x1'*x2 # equivalent to `LIBSVM.Kernel.Linear`\nmodel = EpsilonSVR(kernel=k)\nmach = machine(model, X, y) |> fit!\n\njulia> yhat = predict(mach, Xnew)\n3-element Vector{Float64}:\n  1.1121225361666656\n  0.04667702229741916\n -0.6958148424680672\n```\n\nSee also [`NuSVR`](@ref), [LIVSVM.jl](https://github.com/JuliaML/LIBSVM.jl) and the original C implementation [documentation](https://github.com/cjlin1/libsvm/blob/master/README).\n"
":name" = "EpsilonSVR"
":human_name" = "ϵ-support vector regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:kernel, :gamma, :epsilon, :cost, :cachesize, :degree, :coef0, :tolerance, :shrinking)`"
":hyperparameter_types" = "`(\"Any\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Int32\", \"Float64\", \"Float64\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[LIBSVM.LinearSVC]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}, AbstractDict{ScientificTypesBase.Finite, <:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "LIBSVM"
":package_license" = "unknown"
":load_path" = "MLJLIBSVMInterface.LinearSVC"
":package_uuid" = "b1bec4e5-fd48-53fe-b0cb-9723c09d164b"
":package_url" = "https://github.com/mpastell/LIBSVM.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`true`"
":supports_online" = "`false`"
":docstring" = "```\nLinearSVC\n```\n\nA model type for constructing a linear support vector classifier, based on [LIBSVM.jl](https://github.com/mpastell/LIBSVM.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nLinearSVC = @load LinearSVC pkg=LIBSVM\n```\n\nDo `model = LinearSVC()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `LinearSVC(solver=...)`.\n\nReference for algorithm and core C-library: Rong-En Fan et al (2008): \"LIBLINEAR: A Library for Large Linear Classification.\" *Journal of Machine Learning Research* 9 1871-1874. Available at [https://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf](https://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf). \n\nThis model type is similar to `SVC` from the same package with the setting `kernel=LIBSVM.Kernel.KERNEL.Linear`, but is optimized for the linear case.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with one of:\n\n```\nmach = machine(model, X, y)\nmach = machine(model, X, y, w)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have `Continuous` element scitype; check column scitypes with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `<:OrderedFactor` or `<:Multiclass`; check the scitype with `scitype(y)`\n  * `w`: a dictionary of class weights, keyed on `levels(y)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `solver=LIBSVM.Linearsolver.L2R_L2LOSS_SVC_DUAL`: linear solver, which must be one of the following from the LIBSVM.jl package:\n\n      * `LIBSVM.Linearsolver.L2R_LR`: L2-regularized logistic regression (primal))\n      * `LIBSVM.Linearsolver.L2R_L2LOSS_SVC_DUAL`: L2-regularized L2-loss support vector classification (dual)\n      * `LIBSVM.Linearsolver.L2R_L2LOSS_SVC`: L2-regularized L2-loss support vector classification (primal)\n      * `LIBSVM.Linearsolver.L2R_L1LOSS_SVC_DUAL`: L2-regularized L1-loss support vector classification (dual)\n      * `LIBSVM.Linearsolver.MCSVM_CS`: support vector classification by Crammer and Singer) `LIBSVM.Linearsolver.L1R_L2LOSS_SVC`: L1-regularized L2-loss support vector classification)\n      * `LIBSVM.Linearsolver.L1R_LR`:  L1-regularized logistic regression\n      * `LIBSVM.Linearsolver.L2R_LR_DUAL`: L2-regularized logistic regression (dual)\n  * `tolerance::Float64=Inf`: tolerance for the stopping criterion;\n  * `cost=1.0` (range (0, `Inf`)): the parameter denoted $C$ in the cited reference; for greater regularization, decrease `cost`\n  * `bias= -1.0`: if `bias >= 0`, instance `x` becomes `[x; bias]`; if `bias < 0`, no bias term added (default -1)\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given features `Xnew` having the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `libsvm_model`: the trained model object created by the LIBSVM.jl package\n  * `encoding`: class encoding used internally by `libsvm_model` - a dictionary of class labels keyed on the internal integer representation\n\n# Examples\n\n```\nusing MLJ\nimport LIBSVM\n\nLinearSVC = @load LinearSVC pkg=LIBSVM               # model type\nmodel = LinearSVC(solver=LIBSVM.Linearsolver.L2R_LR) # instance\n\nX, y = @load_iris # table, vector\nmach = machine(model, X, y) |> fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\n\njulia> yhat = predict(mach, Xnew)\n3-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"virginica\"\n \"versicolor\"\n \"virginica\"\n```\n\n## Incorporating class weights\n\n```julia\nweights = Dict(\"virginica\" => 1, \"versicolor\" => 20, \"setosa\" => 1)\nmach = machine(model, X, y, weights) |> fit!\n\njulia> yhat = predict(mach, Xnew)\n3-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"versicolor\"\n \"versicolor\"\n \"versicolor\"\n```\n\nSee also the [`SVC`](@ref) and [`NuSVC`](@ref) classifiers, and [LIVSVM.jl](https://github.com/JuliaML/LIBSVM.jl) and the original C implementation [documentation](https://github.com/cjlin1/liblinear/blob/master/README).\n"
":name" = "LinearSVC"
":human_name" = "linear support vector classifier"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:solver, :tolerance, :cost, :bias)`"
":hyperparameter_types" = "`(\"LIBSVM.Linearsolver.LINEARSOLVER\", \"Float64\", \"Float64\", \"Float64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[LIBSVM.NuSVR]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "LIBSVM"
":package_license" = "unknown"
":load_path" = "MLJLIBSVMInterface.NuSVR"
":package_uuid" = "b1bec4e5-fd48-53fe-b0cb-9723c09d164b"
":package_url" = "https://github.com/mpastell/LIBSVM.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nNuSVR\n```\n\nA model type for constructing a ν-support vector regressor, based on [LIBSVM.jl](https://github.com/mpastell/LIBSVM.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nNuSVR = @load NuSVR pkg=LIBSVM\n```\n\nDo `model = NuSVR()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `NuSVR(kernel=...)`.\n\nReference for algorithm and core C-library: C.-C. Chang and C.-J. Lin (2011): \"LIBSVM: a library for support vector machines.\" *ACM Transactions on Intelligent Systems and Technology*, 2(3):27:1–27:27. Updated at [https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf](https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf). \n\nThis model is a re-parameterization of `EpsilonSVR` in which the `epsilon` hyper-parameter is replaced with a new parameter `nu` (denoted $ν$ in the cited reference) which attempts to control the number of support vectors directly.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with:\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have `Continuous` element scitype; check column scitypes with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `Continuous`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  *   * `kernel=LIBSVM.Kernel.RadialBasis`: either an object that can be\n\n    called, as in `kernel(x1, x2)`, or one of the built-in kernels from the LIBSVM.jl package listed below.  Here `x1` and `x2` are vectors whose lengths match the number of columns of the training data `X` (see \"Examples\" below).\n\n      * `LIBSVM.Kernel.Linear`: `(x1, x2) -> x1'*x2`\n      * `LIBSVM.Kernel.Polynomial`: `(x1, x2) -> gamma*x1'*x2 + coef0)^degree`\n      * `LIBSVM.Kernel.RadialBasis`: `(x1, x2) -> (exp(-gamma*norm(x1 - x2)^2))`\n      * `LIBSVM.Kernel.Sigmoid`: `(x1, x2) - > tanh(gamma*x1'*x2 + coef0)`\n\n    Here `gamma`, `coef0`, `degree` are other hyper-parameters. Serialization of models with user-defined kernels comes with some restrictions. See [LIVSVM.jl issue91](https://github.com/JuliaML/LIBSVM.jl/issues/91)\n  * `gamma = 0.0`: kernel parameter (see above); if `gamma==-1.0` then `gamma = 1/nfeatures` is used in training, where `nfeatures` is the number of features (columns of `X`).  If `gamma==0.0` then `gamma = 1/(var(Tables.matrix(X))*nfeatures)` is used. Actual value used appears in the report (see below).\n  * `coef0 = 0.0`: kernel parameter (see above)\n  * `degree::Int32 = Int32(3)`: degree in polynomial kernel (see above)\n\n  * `cost=1.0` (range (0, `Inf`)): the parameter denoted $C$ in the cited reference; for greater regularization, decrease `cost`\n  * `nu=0.5` (range (0, 1]): An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. Denoted $ν$ in the cited paper. Changing `nu` changes the thickness of some neighborhood of the graph of the prediction function (\"tube\" or \"slab\") and a training error is said to occur when a data point `(x, y)` lies outside of that neighborhood.\n  * `cachesize=200.0` cache memory size in MB\n  * `tolerance=0.001`: tolerance for the stopping criterion\n  * `shrinking=true`: whether to use shrinking heuristics\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given features `Xnew` having the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `libsvm_model`: the trained model object created by the LIBSVM.jl package\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `gamma`: actual value of the kernel parameter `gamma` used in training\n\n# Examples\n\n## Using a built-in kernel\n\n```\nusing MLJ\nimport LIBSVM\n\nNuSVR = @load NuSVR pkg=LIBSVM                 # model type\nmodel = NuSVR(kernel=LIBSVM.Kernel.Polynomial) # instance\n\nX, y = make_regression(rng=123) # table, vector\nmach = machine(model, X, y) |> fit!\n\nXnew, _ = make_regression(3, rng=123)\n\njulia> yhat = predict(mach, Xnew)\n3-element Vector{Float64}:\n  0.2008156459920009\n  0.1131520519131709\n -0.2076156254934889\n```\n\n## User-defined kernels\n\n```\nk(x1, x2) = x1'*x2 # equivalent to `LIBSVM.Kernel.Linear`\nmodel = NuSVR(kernel=k)\nmach = machine(model, X, y) |> fit!\n\njulia> yhat = predict(mach, Xnew)\n3-element Vector{Float64}:\n  1.1211558175964662\n  0.06677125944808422\n -0.6817578942749346\n```\n\nSee also [`EpsilonSVR`](@ref), [LIVSVM.jl](https://github.com/JuliaML/LIBSVM.jl) and the original C implementation [documentation](https://github.com/cjlin1/libsvm/blob/master/README).\n"
":name" = "NuSVR"
":human_name" = "ν-support vector regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:kernel, :gamma, :nu, :cost, :cachesize, :degree, :coef0, :tolerance, :shrinking)`"
":hyperparameter_types" = "`(\"Any\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Int32\", \"Float64\", \"Float64\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[LIBSVM.NuSVC]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "LIBSVM"
":package_license" = "unknown"
":load_path" = "MLJLIBSVMInterface.NuSVC"
":package_uuid" = "b1bec4e5-fd48-53fe-b0cb-9723c09d164b"
":package_url" = "https://github.com/mpastell/LIBSVM.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nNuSVC\n```\n\nA model type for constructing a ν-support vector classifier, based on [LIBSVM.jl](https://github.com/mpastell/LIBSVM.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nNuSVC = @load NuSVC pkg=LIBSVM\n```\n\nDo `model = NuSVC()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `NuSVC(kernel=...)`.\n\nReference for algorithm and core C-library: C.-C. Chang and C.-J. Lin (2011): \"LIBSVM: a library for support vector machines.\" *ACM Transactions on Intelligent Systems and Technology*, 2(3):27:1–27:27. Updated at [https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf](https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf). \n\nThis model is a re-parameterization of the [`SVC`](@ref) classifier, where `nu` replaces `cost`, and is mathematically equivalent to it. The parameter `nu` allows more direct control over the number of support vectors (see under \"Hyper-parameters\").\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with:\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have `Continuous` element scitype; check column scitypes with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `<:OrderedFactor` or `<:Multiclass`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `kernel=LIBSVM.Kernel.RadialBasis`: either an object that can be called, as in `kernel(x1, x2)`, or one of the built-in kernels from the LIBSVM.jl package listed below.  Here `x1` and `x2` are vectors whose lengths match the number of columns of the training data `X` (see \"Examples\" below).\n\n      * `LIBSVM.Kernel.Linear`: `(x1, x2) -> x1'*x2`\n      * `LIBSVM.Kernel.Polynomial`: `(x1, x2) -> gamma*x1'*x2 + coef0)^degree`\n      * `LIBSVM.Kernel.RadialBasis`: `(x1, x2) -> (exp(-gamma*norm(x1 - x2)^2))`\n      * `LIBSVM.Kernel.Sigmoid`: `(x1, x2) - > tanh(gamma*x1'*x2 + coef0)`\n\n    Here `gamma`, `coef0`, `degree` are other hyper-parameters. Serialization of models with user-defined kernels comes with some restrictions. See [LIVSVM.jl issue91](https://github.com/JuliaML/LIBSVM.jl/issues/91)\n  * `gamma = 0.0`: kernel parameter (see above); if `gamma==-1.0` then `gamma = 1/nfeatures` is used in training, where `nfeatures` is the number of features (columns of `X`).  If `gamma==0.0` then `gamma = 1/(var(Tables.matrix(X))*nfeatures)` is used. Actual value used appears in the report (see below).\n  * `coef0 = 0.0`: kernel parameter (see above)\n  * `degree::Int32 = Int32(3)`: degree in polynomial kernel (see above)\n\n  * `nu=0.5` (range (0, 1]): An upper bound on the fraction of margin errors and a lower bound of the fraction of support vectors. Denoted `ν` in the cited paper. Changing `nu` changes the thickness of the margin (a neighborhood of the decision surface) and a margin error is said to have occurred if a training observation lies on the wrong side of the surface or within the margin.\n  * `cachesize=200.0` cache memory size in MB\n  * `tolerance=0.001`: tolerance for the stopping criterion\n  * `shrinking=true`: whether to use shrinking heuristics\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given features `Xnew` having the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `libsvm_model`: the trained model object created by the LIBSVM.jl package\n  * `encoding`: class encoding used internally by `libsvm_model` - a dictionary of class labels keyed on the internal integer representation\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `gamma`: actual value of the kernel parameter `gamma` used in training\n\n# Examples\n\n## Using a built-in kernel\n\n```\nusing MLJ\nimport LIBSVM\n\nNuSVC = @load NuSVC pkg=LIBSVM                 # model type\nmodel = NuSVC(kernel=LIBSVM.Kernel.Polynomial) # instance\n\nX, y = @load_iris # table, vector\nmach = machine(model, X, y) |> fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\n\njulia> yhat = predict(mach, Xnew)\n3-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"virginica\"\n \"virginica\"\n \"virginica\"\n```\n\n## User-defined kernels\n\n```\nk(x1, x2) = x1'*x2 # equivalent to `LIBSVM.Kernel.Linear`\nmodel = NuSVC(kernel=k)\nmach = machine(model, X, y) |> fit!\n\njulia> yhat = predict(mach, Xnew)\n3-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"virginica\"\n \"virginica\"\n \"virginica\"\n```\n\nSee also the classifiers [`SVC`](@ref) and [`LinearSVC`](@ref), [LIVSVM.jl](https://github.com/JuliaML/LIBSVM.jl) and the original C implementation. [documentation](https://github.com/cjlin1/libsvm/blob/master/README).\n"
":name" = "NuSVC"
":human_name" = "ν-support vector classifier"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:kernel, :gamma, :nu, :cachesize, :degree, :coef0, :tolerance, :shrinking)`"
":hyperparameter_types" = "`(\"Any\", \"Float64\", \"Float64\", \"Float64\", \"Int32\", \"Float64\", \"Float64\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[LIBSVM.SVC]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}, AbstractDict{ScientificTypesBase.Finite, <:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "LIBSVM"
":package_license" = "unknown"
":load_path" = "MLJLIBSVMInterface.SVC"
":package_uuid" = "b1bec4e5-fd48-53fe-b0cb-9723c09d164b"
":package_url" = "https://github.com/mpastell/LIBSVM.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`true`"
":supports_online" = "`false`"
":docstring" = "```\nSVC\n```\n\nA model type for constructing a C-support vector classifier, based on [LIBSVM.jl](https://github.com/mpastell/LIBSVM.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nSVC = @load SVC pkg=LIBSVM\n```\n\nDo `model = SVC()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `SVC(kernel=...)`.\n\nReference for algorithm and core C-library: C.-C. Chang and C.-J. Lin (2011): \"LIBSVM: a library for support vector machines.\" *ACM Transactions on Intelligent Systems and Technology*, 2(3):27:1–27:27. Updated at [https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf](https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf). \n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with one of:\n\n```\nmach = machine(model, X, y)\nmach = machine(model, X, y, w)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have `Continuous` element scitype; check column scitypes with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `<:OrderedFactor` or `<:Multiclass`; check the scitype with `scitype(y)`\n  * `w`: a dictionary of class weights, keyed on `levels(y)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `kernel=LIBSVM.Kernel.RadialBasis`: either an object that can be called, as in `kernel(x1, x2)`, or one of the built-in kernels from the LIBSVM.jl package listed below.  Here `x1` and `x2` are vectors whose lengths match the number of columns of the training data `X` (see \"Examples\" below).\n\n      * `LIBSVM.Kernel.Linear`: `(x1, x2) -> x1'*x2`\n      * `LIBSVM.Kernel.Polynomial`: `(x1, x2) -> gamma*x1'*x2 + coef0)^degree`\n      * `LIBSVM.Kernel.RadialBasis`: `(x1, x2) -> (exp(-gamma*norm(x1 - x2)^2))`\n      * `LIBSVM.Kernel.Sigmoid`: `(x1, x2) - > tanh(gamma*x1'*x2 + coef0)`\n\n    Here `gamma`, `coef0`, `degree` are other hyper-parameters. Serialization of models with user-defined kernels comes with some restrictions. See [LIVSVM.jl issue91](https://github.com/JuliaML/LIBSVM.jl/issues/91)\n  * `gamma = 0.0`: kernel parameter (see above); if `gamma==-1.0` then `gamma = 1/nfeatures` is used in training, where `nfeatures` is the number of features (columns of `X`).  If `gamma==0.0` then `gamma = 1/(var(Tables.matrix(X))*nfeatures)` is used. Actual value used appears in the report (see below).\n  * `coef0 = 0.0`: kernel parameter (see above)\n  * `degree::Int32 = Int32(3)`: degree in polynomial kernel (see above)\n\n  * `cost=1.0` (range (0, `Inf`)): the parameter denoted $C$ in the cited reference; for greater regularization, decrease `cost`\n  * `cachesize=200.0` cache memory size in MB\n  * `tolerance=0.001`: tolerance for the stopping criterion\n  * `shrinking=true`: whether to use shrinking heuristics\n  * `probability=false`: whether to base classification on calibrated probabilities (expensive) or to use the raw decision function (recommended). Note that in either case `predict` returns point predictions and not probabilities, so that this option has little utility in the current re-implementation.\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given features `Xnew` having the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `libsvm_model`: the trained model object created by the LIBSVM.jl package\n  * `encoding`: class encoding used internally by `libsvm_model` - a dictionary of class labels keyed on the internal integer representation\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `gamma`: actual value of the kernel parameter `gamma` used in training\n\n# Examples\n\n## Using a built-in kernel\n\n```\nusing MLJ\nimport LIBSVM\n\nSVC = @load SVC pkg=LIBSVM                   # model type\nmodel = SVC(kernel=LIBSVM.Kernel.Polynomial) # instance\n\nX, y = @load_iris # table, vector\nmach = machine(model, X, y) |> fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\n\njulia> yhat = predict(mach, Xnew)\n3-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"virginica\"\n \"virginica\"\n \"virginica\"\n```\n\n## User-defined kernels\n\n```\nk(x1, x2) = x1'*x2 # equivalent to `LIBSVM.Kernel.Linear`\nmodel = SVC(kernel=k)\nmach = machine(model, X, y) |> fit!\n\njulia> yhat = predict(mach, Xnew)\n3-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"virginica\"\n \"virginica\"\n \"virginica\"\n```\n\n## Incorporating class weights\n\nIn either scenario above, we can do:\n\n```julia\nweights = Dict(\"virginica\" => 1, \"versicolor\" => 20, \"setosa\" => 1)\nmach = machine(model, X, y, weights) |> fit!\n\njulia> yhat = predict(mach, Xnew)\n3-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"versicolor\"\n \"versicolor\"\n \"versicolor\"\n```\n\nSee also the classifiers [`NuSVC`](@ref) and [`LinearSVC`](@ref), and [LIVSVM.jl](https://github.com/JuliaML/LIBSVM.jl) and the original C implementation [documentation](https://github.com/cjlin1/libsvm/blob/master/README).\n"
":name" = "SVC"
":human_name" = "C-support vector classifier"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:kernel, :gamma, :cost, :cachesize, :degree, :coef0, :tolerance, :shrinking, :probability)`"
":hyperparameter_types" = "`(\"Any\", \"Float64\", \"Float64\", \"Float64\", \"Int32\", \"Float64\", \"Float64\", \"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[LIBSVM.OneClassSVM]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Binary}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.OrderedFactor{2}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "LIBSVM"
":package_license" = "unknown"
":load_path" = "MLJLIBSVMInterface.OneClassSVM"
":package_uuid" = "b1bec4e5-fd48-53fe-b0cb-9723c09d164b"
":package_url" = "https://github.com/mpastell/LIBSVM.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nOneClassSVM\n```\n\nA model type for constructing a one-class support vector machine, based on [LIBSVM.jl](https://github.com/mpastell/LIBSVM.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nOneClassSVM = @load OneClassSVM pkg=LIBSVM\n```\n\nDo `model = OneClassSVM()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `OneClassSVM(kernel=...)`.\n\nReference for algorithm and core C-library: C.-C. Chang and C.-J. Lin (2011): \"LIBSVM: a library for support vector machines.\" *ACM Transactions on Intelligent Systems and Technology*, 2(3):27:1–27:27. Updated at [https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf](https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf). \n\nThis model is an outlier detection model delivering raw scores based on the decision function of a support vector machine. Like the [`NuSVC`](@ref) classifier, it uses the `nu` re-parameterization of the `cost` parameter appearing in standard support vector classification [`SVC`](@ref).\n\nTo extract normalized scores (\"probabilities\") wrap the model using `ProbabilisticDetector` from [OutlierDetection.jl](https://github.com/OutlierDetectionJL/OutlierDetection.jl). For threshold-based classification, wrap the probabilistic model using MLJ's `BinaryThresholdPredictor`. Examples of wrapping appear below.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with:\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have `Continuous` element scitype; check column scitypes with `schema(X)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `kernel=LIBSVM.Kernel.RadialBasis`: either an object that can be called, as in `kernel(x1, x2)`, or one of the built-in kernels from the LIBSVM.jl package listed below.  Here `x1` and `x2` are vectors whose lengths match the number of columns of the training data `X` (see \"Examples\" below).\n\n      * `LIBSVM.Kernel.Linear`: `(x1, x2) -> x1'*x2`\n      * `LIBSVM.Kernel.Polynomial`: `(x1, x2) -> gamma*x1'*x2 + coef0)^degree`\n      * `LIBSVM.Kernel.RadialBasis`: `(x1, x2) -> (exp(-gamma*norm(x1 - x2)^2))`\n      * `LIBSVM.Kernel.Sigmoid`: `(x1, x2) - > tanh(gamma*x1'*x2 + coef0)`\n\n    Here `gamma`, `coef0`, `degree` are other hyper-parameters. Serialization of models with user-defined kernels comes with some restrictions. See [LIVSVM.jl issue91](https://github.com/JuliaML/LIBSVM.jl/issues/91)\n  * `gamma = 0.0`: kernel parameter (see above); if `gamma==-1.0` then `gamma = 1/nfeatures` is used in training, where `nfeatures` is the number of features (columns of `X`).  If `gamma==0.0` then `gamma = 1/(var(Tables.matrix(X))*nfeatures)` is used. Actual value used appears in the report (see below).\n  * `coef0 = 0.0`: kernel parameter (see above)\n  * `degree::Int32 = Int32(3)`: degree in polynomial kernel (see above)\n\n  * `nu=0.5` (range (0, 1]): An upper bound on the fraction of margin errors and a lower bound of the fraction of support vectors. Denoted `ν` in the cited paper. Changing `nu` changes the thickness of the margin (a neighborhood of the decision surface) and a margin error is said to have occurred if a training observation lies on the wrong side of the surface or within the margin.\n  * `cachesize=200.0` cache memory size in MB\n  * `tolerance=0.001`: tolerance for the stopping criterion\n  * `shrinking=true`: whether to use shrinking heuristics\n\n# Operations\n\n  * `transform(mach, Xnew)`: return scores for outlierness, given features `Xnew` having the same scitype as `X` above. The greater the score, the more likely it is an outlier. This score is based on the SVM decision function. For normalized scores, wrap `model` using `ProbabilisticDetector` from OutlierDetection.jl and call `predict` instead, and for threshold-based classification, wrap again using `BinaryThresholdPredictor`. See the examples below.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `libsvm_model`: the trained model object created by the LIBSVM.jl package\n  * `orientation`: this equals `1` if the decision function for `libsvm_model` is increasing with increasing outlierness, and `-1` if it is decreasing instead. Correspondingly, the `libsvm_model` attaches `true` to outliers in the first case, and `false` in the second. (The `scores` given in the MLJ report and generated by `MLJ.transform` already correct for this ambiguity, which is therefore only an issue for users directly accessing `libsvm_model`.)\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `gamma`: actual value of the kernel parameter `gamma` used in training\n\n# Examples\n\n## Generating raw scores for outlierness\n\n```\nusing MLJ\nimport LIBSVM\nimport StableRNGs.StableRNG\n\nOneClassSVM = @load OneClassSVM pkg=LIBSVM           # model type\nmodel = OneClassSVM(kernel=LIBSVM.Kernel.Polynomial) # instance\n\nrng = StableRNG(123)\nXmatrix = randn(rng, 5, 3)\nXmatrix[1, 1] = 100.0\nX = MLJ.table(Xmatrix)\n\nmach = machine(model, X) |> fit!\n\n# training scores (outliers have larger scores):\njulia> report(mach).scores\n5-element Vector{Float64}:\n  6.711689156091755e-7\n -6.740101976655081e-7\n -6.711632439648446e-7\n -6.743015858874887e-7\n -6.745393717880104e-7\n\n# scores for new data:\nXnew = MLJ.table(rand(rng, 2, 3))\n\njulia> transform(mach, rand(rng, 2, 3))\n2-element Vector{Float64}:\n -6.746293022511047e-7\n -6.744289265348623e-7\n```\n\n## Generating probabilistic predictions of outlierness\n\nContinuing the previous example:\n\n```\nusing OutlierDetection\npmodel = ProbabilisticDetector(model)\npmach = machine(pmodel, X) |> fit!\n\n# probabilistic predictions on new data:\n\njulia> y_prob = predict(pmach, Xnew)\n2-element UnivariateFiniteVector{OrderedFactor{2}, String, UInt8, Float64}:\n UnivariateFinite{OrderedFactor{2}}(normal=>1.0, outlier=>9.57e-5)\n UnivariateFinite{OrderedFactor{2}}(normal=>1.0, outlier=>0.0)\n\n# probabilities for outlierness:\n\njulia> pdf.(y_prob, \"outlier\")\n2-element Vector{Float64}:\n 9.572583265925801e-5\n 0.0\n\n# raw scores are still available using `transform`:\n\njulia> transform(pmach, Xnew)\n2-element Vector{Float64}:\n 9.572583265925801e-5\n 0.0\n```\n\n## Outlier classification using a probability threshold:\n\nContinuing the previous example:\n\n```\ndmodel = BinaryThresholdPredictor(pmodel, threshold=0.9)\ndmach = machine(dmodel, X) |> fit!\n\njulia> yhat = predict(dmach, Xnew)\n2-element CategoricalArrays.CategoricalArray{String,1,UInt8}:\n \"normal\"\n \"normal\"\n```\n\n## User-defined kernels\n\nContinuing the first example:\n\n```\nk(x1, x2) = x1'*x2 # equivalent to `LIBSVM.Kernel.Linear`\nmodel = OneClassSVM(kernel=k)\nmach = machine(model, X) |> fit!\n\njulia> yhat = transform(mach, Xnew)\n2-element Vector{Float64}:\n -0.4825363352732942\n -0.4848772169720227\n```\n\nSee also [LIVSVM.jl](https://github.com/JuliaML/LIBSVM.jl) and the original C implementation [documentation](https://github.com/cjlin1/libsvm/blob/master/README). For an alternative source of outlier detection models with an MLJ interface, see [OutlierDetection.jl](https://outlierdetectionjl.github.io/OutlierDetection.jl/dev/).\n"
":name" = "OneClassSVM"
":human_name" = "one-class support vector machine"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":transform"]
":hyperparameters" = "`(:kernel, :gamma, :nu, :cachesize, :degree, :coef0, :tolerance, :shrinking)`"
":hyperparameter_types" = "`(\"Any\", \"Float64\", \"Float64\", \"Float64\", \"Int32\", \"Float64\", \"Float64\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[TSVD.TSVDTransformer]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "TSVD"
":package_license" = "MIT"
":load_path" = "MLJTSVDInterface.TSVDTransformer"
":package_uuid" = "9449cd9e-2762-5aa3-a617-5413e99d722e"
":package_url" = "https://github.com/JuliaLinearAlgebra/TSVD.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Truncated SVD dimensionality reduction"
":name" = "TSVDTransformer"
":human_name" = "truncated SVD transformer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":transform"]
":hyperparameters" = "`(:nvals, :maxiter, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Int64\", \"Union{Int64, Random.AbstractRNG}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[GLM.LinearBinaryClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Binary}`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Binary}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Binary}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Binary}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "GLM"
":package_license" = "MIT"
":load_path" = "MLJGLMInterface.LinearBinaryClassifier"
":package_uuid" = "38e38edf-8417-5370-95a0-9cbb8c7f171a"
":package_url" = "https://github.com/JuliaStats/GLM.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLinearBinaryClassifier\n```\n\nA model type for constructing a linear binary classifier, based on [GLM.jl](https://github.com/JuliaStats/GLM.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nLinearBinaryClassifier = @load LinearBinaryClassifier pkg=GLM\n```\n\nDo `model = LinearBinaryClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `LinearBinaryClassifier(fit_intercept=...)`.\n\n`LinearBinaryClassifier` is a [generalized linear model](https://en.wikipedia.org/wiki/Generalized_linear_model#Variance_function), specialised to the case of a binary target variable, with a user-specified link function. Options exist to specify an intercept or offset feature.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with one of:\n\n```\nmach = machine(model, X, y)\nmach = machine(model, X, y, w)\n```\n\nHere\n\n  * `X`: is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check the scitype with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `<:OrderedFactor(2)` or `<:Multiclass(2)`; check the scitype with `schema(y)`\n  * `w`: is a vector of `Real` per-observation weights\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `fit_intercept=true`: Whether to calculate the intercept for this model.  If set to false,  no intercept will be calculated (e.g. the data is expected to be centered)\n  * `link=GLM.LogitLink`: The function which links the linear prediction function to the  probability of a particular outcome or class. This must have type `GLM.Link01`. Options  include `GLM.LogitLink()`, `GLM.ProbitLink()`, `CloglogLink(),`CauchitLink()`.\n  * `offsetcol=nothing`: Name of the column to be used as an offset, if any.  An offset is a  variable which is known to have a coefficient of 1.\n  * `maxiter::Integer=30`: The maximum number of iterations allowed to achieve convergence.\n  * `atol::Real=1e-6`: Absolute threshold for convergence. Convergence is achieved when the  relative change in deviance is less than `max(rtol*dev, atol). This term exists to avoid  failure when deviance is unchanged except for rounding errors.\n  * `rtol::Real=1e-6`: Relative threshold for convergence. Convergence is achieved when the  relative change in deviance is less than `max(rtol*dev, atol). This term exists to avoid  failure when deviance is unchanged except for rounding errors.\n  * `minstepfac::Real=0.001`: Minimum step fraction. Must be between 0 and 1. Lower bound for the factor used to update the linear fit.\n  * `report_keys::Union{Symbol, Nothing}=DEFAULT_KEYS`: keys to be used in the report. Should be one of: `:deviance`, `:dof_residual`, `:stderror`, `:vcov`, `:coef_table`.\n\n# Operations\n\n  * `predict(mach, Xnew)`: Return predictions of the target given features `Xnew` having the same scitype as `X` above. Predictions are probabilistic.\n  * `predict_mode(mach, Xnew)`: Return the modes of the probabilistic predictions returned  above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `features`: The names of the features used during model fitting.\n  * `coef`: The linear coefficients determined by the model.\n  * `intercept`: The intercept determined by the model.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `deviance`: Measure of deviance of fitted model with respect to a perfectly fitted model. For a linear model, this is the weighted residual sum of squares\n  * `dof_residual`: The degrees of freedom for residuals, when meaningful.\n  * `stderror`: The standard errors of the coefficients.\n  * `vcov`: The estimated variance-covariance matrix of the coefficient estimates.\n  * `coef_table`: Table which displays coefficients and summarizes their significance and confidence intervals.\n\n# Examples\n\n```\nusing MLJ\nimport GLM # namespace must be available\n\nLinearBinaryClassifier = @load LinearBinaryClassifier pkg=GLM\nclf = LinearBinaryClassifier(fit_intercept=false, link=GLM.ProbitLink())\n\nX, y = @load_crabs\n\nmach = machine(clf, X, y) |> fit!\n\nXnew = (;FL = [8.1, 24.8, 7.2],\n        RW = [5.1, 25.7, 6.4],\n        CL = [15.9, 46.7, 14.3],\n        CW = [18.7, 59.7, 12.2],\n        BD = [6.2, 23.6, 8.4],)\n\nyhat = predict(mach, Xnew) # probabilistic predictions\npdf(yhat, levels(y)) # probability matrix\np_B = pdf.(yhat, \"B\")\nclass_labels = predict_mode(mach, Xnew)\n\nfitted_params(mach).features\nfitted_params(mach).coef\nfitted_params(mach).intercept\n\nreport(mach)\n```\n\nSee also [`LinearRegressor`](@ref), [`LinearCountRegressor`](@ref)\n"
":name" = "LinearBinaryClassifier"
":human_name" = "linear binary classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":predict_mean"]
":hyperparameters" = "`(:fit_intercept, :link, :offsetcol, :maxiter, :atol, :rtol, :minstepfac, :report_keys)`"
":hyperparameter_types" = "`(\"Bool\", \"GLM.Link01\", \"Union{Nothing, Symbol}\", \"Integer\", \"Real\", \"Real\", \"Real\", \"Union{Nothing, AbstractVector{Symbol}}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[GLM.LinearCountRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Count}`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Count}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Count}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{ScientificTypesBase.Count}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "GLM"
":package_license" = "MIT"
":load_path" = "MLJGLMInterface.LinearCountRegressor"
":package_uuid" = "38e38edf-8417-5370-95a0-9cbb8c7f171a"
":package_url" = "https://github.com/JuliaStats/GLM.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLinearCountRegressor\n```\n\nA model type for constructing a linear count regressor, based on [GLM.jl](https://github.com/JuliaStats/GLM.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nLinearCountRegressor = @load LinearCountRegressor pkg=GLM\n```\n\nDo `model = LinearCountRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `LinearCountRegressor(fit_intercept=...)`.\n\n`LinearCountRegressor` is a [generalized linear model](https://en.wikipedia.org/wiki/Generalized_linear_model#Variance_function), specialised to the case of a `Count` target variable (non-negative, unbounded integer) with user-specified link function. Options exist to specify an intercept or offset feature.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with one of:\n\n```\nmach = machine(model, X, y)\nmach = machine(model, X, y, w)\n```\n\nHere\n\n  * `X`: is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check the scitype with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `Count`; check the scitype with `schema(y)`\n  * `w`: is a vector of `Real` per-observation weights\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `fit_intercept=true`: Whether to calculate the intercept for this model. If set to false,  no intercept will be calculated (e.g. the data is expected to be centered)\n  * `distribution=Distributions.Poisson()`: The distribution which the residuals/errors of the  model should fit.\n  * `link=GLM.LogLink()`: The function which links the linear prediction function to the  probability of a particular outcome or class. This should be one of the following:  `GLM.IdentityLink()`, `GLM.InverseLink()`, `GLM.InverseSquareLink()`, `GLM.LogLink()`,  `GLM.SqrtLink()`.\n  * `offsetcol=nothing`: Name of the column to be used as an offset, if any.  An offset is a  variable which is known to have a coefficient of 1.\n  * `maxiter::Integer=30`: The maximum number of iterations allowed to achieve convergence.\n  * `atol::Real=1e-6`: Absolute threshold for convergence. Convergence is achieved when the  relative change in deviance is less than `max(rtol*dev, atol). This term exists to avoid  failure when deviance is unchanged except for rounding errors.\n  * `rtol::Real=1e-6`: Relative threshold for convergence. Convergence is achieved when the  relative change in deviance is less than `max(rtol*dev, atol). This term exists to avoid  failure when deviance is unchanged except for rounding errors.\n  * `minstepfac::Real=0.001`: Minimum step fraction. Must be between 0 and 1. Lower bound for the factor used to update the linear fit.\n  * `report_keys::Union{Symbol, Nothing}=DEFAULT_KEYS`: keys to be used in the report. Should be one of: `:deviance`, `:dof_residual`, `:stderror`, `:vcov`, `:coef_table`.\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given new features `Xnew` having  the same Scitype as `X` above. Predictions are probabilistic.\n  * `predict_mean(mach, Xnew)`: instead return the mean of each prediction above\n  * `predict_median(mach, Xnew)`: instead return the median of each prediction above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `features`: The names of the features encountered during model fitting.\n  * `coef`: The linear coefficients determined by the model.\n  * `intercept`: The intercept determined by the model.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `deviance`: Measure of deviance of fitted model with respect to a perfectly fitted model. For a linear model, this is the weighted residual sum of squares\n  * `dof_residual`: The degrees of freedom for residuals, when meaningful.\n  * `stderror`: The standard errors of the coefficients.\n  * `vcov`: The estimated variance-covariance matrix of the coefficient estimates.\n  * `coef_table`: Table which displays coefficients and summarizes their significance and confidence intervals.\n\n# Examples\n\n```\nusing MLJ\nimport MLJ.Distributions.Poisson\n\n# Generate some data whose target y looks Poisson when conditioned on\n# X:\nN = 10_000\nw = [1.0, -2.0, 3.0]\nmu(x) = exp(w'x) # mean for a log link function\nXmat = rand(N, 3)\nX = MLJ.table(Xmat)\ny = map(1:N) do i\n    x = Xmat[i, :]\n    rand(Poisson(mu(x)))\nend;\n\nCountRegressor = @load LinearCountRegressor pkg=GLM\nmodel = CountRegressor(fit_intercept=false)\nmach = machine(model, X, y)\nfit!(mach)\n\nXnew = MLJ.table(rand(3, 3))\nyhat = predict(mach, Xnew)\nyhat_point = predict_mean(mach, Xnew)\n\n# get coefficients approximating `w`:\njulia> fitted_params(mach).coef\n3-element Vector{Float64}:\n  0.9969008753103842\n -2.0255901752504775\n  3.014407534033522\n\nreport(mach)\n```\n\nSee also [`LinearRegressor`](@ref), [`LinearBinaryClassifier`](@ref)\n"
":name" = "LinearCountRegressor"
":human_name" = "linear count regressor"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":predict_mean"]
":hyperparameters" = "`(:fit_intercept, :distribution, :link, :offsetcol, :maxiter, :atol, :rtol, :minstepfac, :report_keys)`"
":hyperparameter_types" = "`(\"Bool\", \"Distributions.Distribution\", \"GLM.Link\", \"Union{Nothing, Symbol}\", \"Integer\", \"Real\", \"Real\", \"Real\", \"Union{Nothing, AbstractVector{Symbol}}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[GLM.LinearRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "GLM"
":package_license" = "MIT"
":load_path" = "MLJGLMInterface.LinearRegressor"
":package_uuid" = "38e38edf-8417-5370-95a0-9cbb8c7f171a"
":package_url" = "https://github.com/JuliaStats/GLM.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLinearRegressor\n```\n\nA model type for constructing a linear regressor, based on [GLM.jl](https://github.com/JuliaStats/GLM.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nLinearRegressor = @load LinearRegressor pkg=GLM\n```\n\nDo `model = LinearRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `LinearRegressor(fit_intercept=...)`.\n\n`LinearRegressor` assumes the target is a continuous variable whose conditional distribution is normal with constant variance, and whose expected value is a linear combination of the features (identity link function). Options exist to specify an intercept or offset feature.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with one of:\n\n```\nmach = machine(model, X, y)\nmach = machine(model, X, y, w)\n```\n\nHere\n\n  * `X`: is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check the scitype with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `Continuous`; check the scitype with `scitype(y)`\n  * `w`: is a vector of `Real` per-observation weights\n\n# Hyper-parameters\n\n  * `fit_intercept=true`: Whether to calculate the intercept for this model.  If set to false, no intercept will be calculated (e.g. the data is expected  to be centered)\n  * `dropcollinear=false`: Whether to drop features in the training data to ensure linear independence.  If true , only the first of each set of linearly-dependent features is used. The coefficient for redundant linearly dependent features is `0.0` and all associated statistics are set to `NaN`.\n  * `offsetcol=nothing`: Name of the column to be used as an offset, if any.  An offset is a variable which is known to have a coefficient of 1.\n  * `report_keys::Union{Symbol, Nothing}=DEFAULT_KEYS`: keys to be used in the report. Should be one of: `:deviance`, `:dof_residual`, `:stderror`, `:vcov`, `:coef_table`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given new  features `Xnew` having the same Scitype as `X` above. Predictions are  probabilistic.\n  * `predict_mean(mach, Xnew)`: instead return the mean of  each prediction above\n  * `predict_median(mach, Xnew)`: instead return the median of  each prediction above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `features`: The names of the features encountered during model fitting.\n  * `coef`: The linear coefficients determined by the model.\n  * `intercept`: The intercept determined by the model.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `deviance`: Measure of deviance of fitted model with respect to a perfectly fitted model. For a linear model, this is the weighted residual sum of squares\n  * `dof_residual`: The degrees of freedom for residuals, when meaningful.\n  * `stderror`: The standard errors of the coefficients.\n  * `vcov`: The estimated variance-covariance matrix of the coefficient estimates.\n  * `coef_table`: Table which displays coefficients and summarizes their significance and confidence intervals.\n\n# Examples\n\n```\nusing MLJ\nLinearRegressor = @load LinearRegressor pkg=GLM\nglm = LinearRegressor()\n\nX, y = make_regression(100, 2) # synthetic data\nmach = machine(glm, X, y) |> fit!\n\nXnew, _ = make_regression(3, 2)\nyhat = predict(mach, Xnew) # new predictions\nyhat_point = predict_mean(mach, Xnew) # new predictions\n\nfitted_params(mach).features\nfitted_params(mach).coef # x1, x2, intercept\nfitted_params(mach).intercept\n\nreport(mach)\n```\n\nSee also [`LinearCountRegressor`](@ref), [`LinearBinaryClassifier`](@ref)\n"
":name" = "LinearRegressor"
":human_name" = "linear regressor"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":predict_mean"]
":hyperparameters" = "`(:fit_intercept, :dropcollinear, :offsetcol, :report_keys)`"
":hyperparameter_types" = "`(\"Bool\", \"Bool\", \"Union{Nothing, Symbol}\", \"Union{Nothing, AbstractVector{Symbol}}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJFlux.MultitargetNeuralNetworkRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJFlux"
":package_license" = "MIT"
":load_path" = "MLJFlux.MultitargetNeuralNetworkRegressor"
":package_uuid" = "094fc8d1-fd35-5302-93ea-dabda2abf845"
":package_url" = "https://github.com/alan-turing-institute/MLJFlux.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nMultitargetNeuralNetworkRegressor\n```\n\nA model type for constructing a multitarget neural network regressor, based on [MLJFlux.jl](https://github.com/alan-turing-institute/MLJFlux.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nMultitargetNeuralNetworkRegressor = @load MultitargetNeuralNetworkRegressor pkg=MLJFlux\n```\n\nDo `model = MultitargetNeuralNetworkRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `MultitargetNeuralNetworkRegressor(builder=...)`.\n\n`MultitargetNeuralNetworkRegressor` is for training a data-dependent Flux.jl neural network to predict a multi-valued `Continuous` target, represented as a table, given a table of `Continuous` features. Users provide a recipe for constructing the network, based on properties of the data that is encountered, by specifying an appropriate `builder`. See MLJFlux documentation for more on builders.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n  * `y` is the target, which can be any table of output targets whose element scitype is `Continuous`; check column scitypes with `schema(y)`.\n\n# Hyper-parameters\n\n  * `builder=MLJFlux.Linear(σ=Flux.relu)`: An MLJFlux builder that constructs a neural network. Possible `builders` include: `Linear`, `Short`, and `MLP`. See MLJFlux documentation for more on builders, and the example below for using the `@builder` convenience macro.\n  * `optimiser::Flux.Adam()`: A `Flux.Optimise` optimiser. The optimiser performs the updating of the weights of the network. For further reference, see [the Flux optimiser documentation](https://fluxml.ai/Flux.jl/stable/training/optimisers/). To choose a learning rate (the update rate of the optimizer), a good rule of thumb is to start out at `10e-3`, and tune using powers of 10 between `1` and `1e-7`.\n  * `loss=Flux.mse`: The loss function which the network will optimize. Should be a function which can be called in the form `loss(yhat, y)`.  Possible loss functions are listed in [the Flux loss function documentation](https://fluxml.ai/Flux.jl/stable/models/losses/). For a regression task, natural loss functions are:\n\n      * `Flux.mse`\n      * `Flux.mae`\n      * `Flux.msle`\n      * `Flux.huber_loss`\n\n    Currently MLJ measures are not supported as loss functions here.\n  * `epochs::Int=10`: The duration of training, in epochs. Typically, one epoch represents one pass through the complete the training dataset.\n  * `batch_size::int=1`: the batch size to be used for training, representing the number of samples per update of the network weights. Typically, batch size is between 8 and\n\n    512. Increassing batch size may accelerate training if `acceleration=CUDALibs()` and a\n\n    GPU is available.\n  * `lambda::Float64=0`: The strength of the weight regularization penalty. Can be any value in the range `[0, ∞)`.\n  * `alpha::Float64=0`: The L2/L1 mix of regularization, in the range `[0, 1]`. A value of 0 represents L2 regularization, and a value of 1 represents L1 regularization.\n  * `rng::Union{AbstractRNG, Int64}`: The random number generator or seed used during training.\n  * `optimizer_changes_trigger_retraining::Bool=false`: Defines what happens when re-fitting a machine if the associated optimiser has changed. If `true`, the associated machine will retrain from scratch on `fit!` call, otherwise it will not.\n  * `acceleration::AbstractResource=CPU1()`: Defines on what hardware training is done. For Training on GPU, use `CUDALibs()`.\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given new features `Xnew` having the same scitype as `X` above. Predictions are deterministic.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `chain`: The trained \"chain\" (Flux.jl model), namely the series of layers,  functions, and activations  which make up the neural network.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `training_losses`: A vector of training losses (penalised if `lambda != 0`) in  historical order, of length `epochs + 1`.  The first element is the pre-training loss.\n\n# Examples\n\nIn this example we apply a multi-target regression model to synthetic data:\n\n```julia\nusing MLJ\nimport MLJFlux\nusing Flux\n```\n\nFirst, we generate some synthetic data (needs MLJBase 0.20.16 or higher):\n\n```julia\nX, y = make_regression(100, 9; n_targets = 2) # both tables\nschema(y)\nschema(X)\n```\n\nSplitting off a test set:\n\n```julia\n(X, Xtest), (y, ytest) = partition((X, y), 0.7, multi=true);\n```\n\nNext, we can define a `builder`, making use of a convenience macro to do so.  In the following `@builder` call, `n_in` is a proxy for the number input features and `n_out` the number of target variables (both known at `fit!` time), while `rng` is a proxy for a RNG (which will be passed from the `rng` field of `model` defined below).\n\n```julia\nbuilder = MLJFlux.@builder begin\n    init=Flux.glorot_uniform(rng)\n    Chain(\n        Dense(n_in, 64, relu, init=init),\n        Dense(64, 32, relu, init=init),\n        Dense(32, n_out, init=init),\n    )\nend\n```\n\nInstantiating the regression model:\n\n```julia\nMultitargetNeuralNetworkRegressor = @load MultitargetNeuralNetworkRegressor\nmodel = MultitargetNeuralNetworkRegressor(builder=builder, rng=123, epochs=20)\n```\n\nWe will arrange for standardization of the the target by wrapping our model in  `TransformedTargetModel`, and standardization of the features by inserting the wrapped  model in a pipeline:\n\n```julia\npipe = Standardizer |> TransformedTargetModel(model, target=Standardizer)\n```\n\nIf we fit with a high verbosity (>1), we will see the losses during training. We can also see the losses in the output of `report(mach)`\n\n```julia\nmach = machine(pipe, X, y)\nfit!(mach, verbosity=2)\n\n# first element initial loss, 2:end per epoch training losses\nreport(mach).transformed_target_model_deterministic.model.training_losses\n```\n\nFor experimenting with learning rate, see the [`NeuralNetworkRegressor`](@ref) example.\n\n```\npipe.transformed_target_model_deterministic.model.optimiser.eta = 0.0001\n```\n\nWith the learning rate fixed, we can now compute a CV estimate of the performance (using all data bound to `mach`) and compare this with performance on the test set:\n\n```julia\n# custom MLJ loss:\nmulti_loss(yhat, y) = l2(MLJ.matrix(yhat), MLJ.matrix(y)) |> mean\n\n# CV estimate, based on `(X, y)`:\nevaluate!(mach, resampling=CV(nfolds=5), measure=multi_loss)\n\n# loss for `(Xtest, test)`:\nfit!(mach) # trains on all data `(X, y)`\nyhat = predict(mach, Xtest)\nmulti_loss(yhat, ytest)\n```\n\nSee also [`NeuralNetworkRegressor`](@ref)\n"
":name" = "MultitargetNeuralNetworkRegressor"
":human_name" = "multitarget neural network regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":predict"]
":hyperparameters" = "`(:builder, :optimiser, :loss, :epochs, :batch_size, :lambda, :alpha, :rng, :optimiser_changes_trigger_retraining, :acceleration)`"
":hyperparameter_types" = "`(\"Any\", \"Any\", \"Any\", \"Int64\", \"Int64\", \"Float64\", \"Float64\", \"Union{Integer, Random.AbstractRNG}\", \"Bool\", \"ComputationalResources.AbstractResource\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = ":epochs"
":supports_training_losses" = "`true`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`(:optimiser, :builder)`"
":reporting_operations" = "`()`"

[MLJFlux.NeuralNetworkClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJFlux"
":package_license" = "MIT"
":load_path" = "MLJFlux.NeuralNetworkClassifier"
":package_uuid" = "094fc8d1-fd35-5302-93ea-dabda2abf845"
":package_url" = "https://github.com/alan-turing-institute/MLJFlux.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nNeuralNetworkClassifier\n```\n\nA model type for constructing a neural network classifier, based on [MLJFlux.jl](https://github.com/alan-turing-institute/MLJFlux.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nNeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux\n```\n\nDo `model = NeuralNetworkClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `NeuralNetworkClassifier(builder=...)`.\n\n`NeuralNetworkClassifier` is for training a data-dependent Flux.jl neural network for making probabilistic predictions of a `Multiclass` or `OrderedFactor` target, given a table of `Continuous` features. Users provide a recipe for constructing  the network, based on properties of the data that is encountered, by specifying  an appropriate `builder`. See MLJFlux documentation for more on builders.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `Multiclass` or `OrderedFactor`; check the scitype with `scitype(y)`\n\nTrain the machine with `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `builder=MLJFlux.Short()`: An MLJFlux builder that constructs a neural network. Possible  `builders` include: `MLJFlux.Linear`, `MLJFlux.Short`, and `MLJFlux.MLP`. See  MLJFlux.jl documentation for examples of user-defined builders. See also `finaliser`  below.\n  * `optimiser::Flux.Adam()`: A `Flux.Optimise` optimiser. The optimiser performs the updating of the weights of the network. For further reference, see [the Flux optimiser documentation](https://fluxml.ai/Flux.jl/stable/training/optimisers/). To choose a learning rate (the update rate of the optimizer), a good rule of thumb is to start out at `10e-3`, and tune using powers of 10 between `1` and `1e-7`.\n  * `loss=Flux.crossentropy`: The loss function which the network will optimize. Should be a function which can be called in the form `loss(yhat, y)`.  Possible loss functions are listed in [the Flux loss function documentation](https://fluxml.ai/Flux.jl/stable/models/losses/). For a classification task, the most natural loss functions are:\n\n      * `Flux.crossentropy`: Standard multiclass classification loss, also known as the log loss.\n      * `Flux.logitcrossentopy`: Mathematically equal to crossentropy, but numerically more stable than finalising the outputs with `softmax` and then calculating crossentropy. You will need to specify `finaliser=identity` to remove MLJFlux's default softmax finaliser, and understand that the output of `predict` is then unnormalized (no longer probabilistic).\n      * `Flux.tversky_loss`: Used with imbalanced data to give more weight to false negatives.\n      * `Flux.focal_loss`: Used with highly imbalanced data. Weights harder examples more than easier examples.\n\n    Currently MLJ measures are not supported values of `loss`.\n  * `epochs::Int=10`: The duration of training, in epochs. Typically, one epoch represents one pass through the complete the training dataset.\n  * `batch_size::int=1`: the batch size to be used for training, representing the number of samples per update of the network weights. Typically, batch size is between 8 and\n\n    512. Increassing batch size may accelerate training if `acceleration=CUDALibs()` and a\n\n    GPU is available.\n  * `lambda::Float64=0`: The strength of the weight regularization penalty. Can be any value in the range `[0, ∞)`.\n  * `alpha::Float64=0`: The L2/L1 mix of regularization, in the range `[0, 1]`. A value of 0 represents L2 regularization, and a value of 1 represents L1 regularization.\n  * `rng::Union{AbstractRNG, Int64}`: The random number generator or seed used during training.\n  * `optimizer_changes_trigger_retraining::Bool=false`: Defines what happens when re-fitting a machine if the associated optimiser has changed. If `true`, the associated machine will retrain from scratch on `fit!` call, otherwise it will not.\n  * `acceleration::AbstractResource=CPU1()`: Defines on what hardware training is done. For Training on GPU, use `CUDALibs()`.\n  * `finaliser=Flux.softmax`: The final activation function of the neural network (applied after the network defined by `builder`). Defaults to `Flux.softmax`.\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given new features `Xnew`, which should have the same scitype as `X` above. Predictions are probabilistic but uncalibrated.\n  * `predict_mode(mach, Xnew)`: Return the modes of the probabilistic predictions returned above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `chain`: The trained \"chain\" (Flux.jl model), namely the series of layers,  functions, and activations which make up the neural network. This includes  the final layer specified by `finaliser` (eg, `softmax`).\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `training_losses`: A vector of training losses (penalised if `lambda != 0`) in  historical order, of length `epochs + 1`.  The first element is the pre-training loss.\n\n# Examples\n\nIn this example we build a classification model using the Iris dataset. This is a very basic example, using a default builder and no standardization.  For a more advanced illustration, see [`NeuralNetworkRegressor`](@ref) or [`ImageClassifier`](@ref), and examples in the MLJFlux.jl documentation.\n\n```julia\nusing MLJ\nusing Flux\nimport RDatasets\n```\n\nFirst, we can load the data:\n\n```julia\niris = RDatasets.dataset(\"datasets\", \"iris\");\ny, X = unpack(iris, ==(:Species), rng=123); # a vector and a table\nNeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux\nclf = NeuralNetworkClassifier()\n```\n\nNext, we can train the model:\n\n```julia\nmach = machine(clf, X, y)\nfit!(mach)\n```\n\nWe can train the model in an incremental fashion, altering the learning rate as we go, provided `optimizer_changes_trigger_retraining` is `false` (the default). Here, we also change the number of (total) iterations:\n\n```julia\nclf.optimiser.eta = clf.optimiser.eta * 2\nclf.epochs = clf.epochs + 5\n\nfit!(mach, verbosity=2) # trains 5 more epochs\n```\n\nWe can inspect the mean training loss using the `cross_entropy` function:\n\n```julia\ntraining_loss = cross_entropy(predict(mach, X), y) |> mean\n```\n\nAnd we can access the Flux chain (model) using `fitted_params`:\n\n```julia\nchain = fitted_params(mach).chain\n```\n\nFinally, we can see how the out-of-sample performance changes over time, using MLJ's `learning_curve` function:\n\n```julia\nr = range(clf, :epochs, lower=1, upper=200, scale=:log10)\ncurve = learning_curve(clf, X, y,\n                     range=r,\n                     resampling=Holdout(fraction_train=0.7),\n                     measure=cross_entropy)\nusing Plots\nplot(curve.parameter_values,\n     curve.measurements,\n     xlab=curve.parameter_name,\n     xscale=curve.parameter_scale,\n     ylab = \"Cross Entropy\")\n\n```\n\nSee also [`ImageClassifier`](@ref).\n"
":name" = "NeuralNetworkClassifier"
":human_name" = "neural network classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":predict"]
":hyperparameters" = "`(:builder, :finaliser, :optimiser, :loss, :epochs, :batch_size, :lambda, :alpha, :rng, :optimiser_changes_trigger_retraining, :acceleration)`"
":hyperparameter_types" = "`(\"Any\", \"Any\", \"Any\", \"Any\", \"Int64\", \"Int64\", \"Float64\", \"Float64\", \"Union{Int64, Random.AbstractRNG}\", \"Bool\", \"ComputationalResources.AbstractResource\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = ":epochs"
":supports_training_losses" = "`true`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`(:optimiser, :builder)`"
":reporting_operations" = "`()`"

[MLJFlux.ImageClassifier]
":input_scitype" = "`AbstractVector{<:ScientificTypesBase.Image}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Multiclass}`"
":fit_data_scitype" = "`Tuple{AbstractVector{<:ScientificTypesBase.Image}, AbstractVector{<:ScientificTypesBase.Multiclass}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Multiclass}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJFlux"
":package_license" = "MIT"
":load_path" = "MLJFlux.ImageClassifier"
":package_uuid" = "094fc8d1-fd35-5302-93ea-dabda2abf845"
":package_url" = "https://github.com/alan-turing-institute/MLJFlux.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nImageClassifier\n```\n\nA model type for constructing a image classifier, based on [MLJFlux.jl](https://github.com/alan-turing-institute/MLJFlux.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nImageClassifier = @load ImageClassifier pkg=MLJFlux\n```\n\nDo `model = ImageClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `ImageClassifier(builder=...)`.\n\n`ImageClassifier` classifies images using a neural network adapted to the type of images provided (color or gray scale). Predictions are probabilistic. Users provide a recipe for constructing the network, based on properties of the image encountered, by specifying an appropriate `builder`. See MLJFlux documentation for more on builders.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any `AbstractVector` of images with `ColorImage` or `GrayImage` scitype; check  the scitype with `scitype(X)` and refer to ScientificTypes.jl documentation on coercing  typical image formats into an appropriate type.\n  * `y` is the target, which can be any `AbstractVector` whose element  scitype is `Multiclass`; check the scitype with `scitype(y)`.\n\nTrain the machine with `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `builder`: An MLJFlux builder that constructs the neural network.  The fallback builds a  depth-16 VGG architecture adapted to the image size and number of target classes, with  no batch normalization; see the Metalhead.jl documentation for details. See the example  below for a user-specified builder. A convenience macro `@builder` is also  available. See also `finaliser` below.\n  * `optimiser::Flux.Adam()`: A `Flux.Optimise` optimiser. The optimiser performs the updating of the weights of the network. For further reference, see [the Flux optimiser documentation](https://fluxml.ai/Flux.jl/stable/training/optimisers/). To choose a learning rate (the update rate of the optimizer), a good rule of thumb is to start out at `10e-3`, and tune using powers of 10 between `1` and `1e-7`.\n  * `loss=Flux.crossentropy`: The loss function which the network will optimize. Should be a function which can be called in the form `loss(yhat, y)`.  Possible loss functions are listed in [the Flux loss function documentation](https://fluxml.ai/Flux.jl/stable/models/losses/). For a classification task, the most natural loss functions are:\n\n      * `Flux.crossentropy`: Standard multiclass classification loss, also known as the log loss.\n      * `Flux.logitcrossentopy`: Mathematically equal to crossentropy, but numerically more stable than finalising the outputs with `softmax` and then calculating crossentropy. You will need to specify `finaliser=identity` to remove MLJFlux's default softmax finaliser, and understand that the output of `predict` is then unnormalized (no longer probabilistic).\n      * `Flux.tversky_loss`: Used with imbalanced data to give more weight to false negatives.\n      * `Flux.focal_loss`: Used with highly imbalanced data. Weights harder examples more than easier examples.\n\n    Currently MLJ measures are not supported values of `loss`.\n  * `epochs::Int=10`: The duration of training, in epochs. Typically, one epoch represents one pass through the complete the training dataset.\n  * `batch_size::int=1`: the batch size to be used for training, representing the number of samples per update of the network weights. Typically, batch size is between 8 and\n\n    512. Increassing batch size may accelerate training if `acceleration=CUDALibs()` and a\n\n    GPU is available.\n  * `lambda::Float64=0`: The strength of the weight regularization penalty. Can be any value in the range `[0, ∞)`.\n  * `alpha::Float64=0`: The L2/L1 mix of regularization, in the range `[0, 1]`. A value of 0 represents L2 regularization, and a value of 1 represents L1 regularization.\n  * `rng::Union{AbstractRNG, Int64}`: The random number generator or seed used during training.\n  * `optimizer_changes_trigger_retraining::Bool=false`: Defines what happens when re-fitting a machine if the associated optimiser has changed. If `true`, the associated machine will retrain from scratch on `fit!` call, otherwise it will not.\n  * `acceleration::AbstractResource=CPU1()`: Defines on what hardware training is done. For Training on GPU, use `CUDALibs()`.\n  * `finaliser=Flux.softmax`: The final activation function of the neural network (applied after the network defined by `builder`). Defaults to `Flux.softmax`.\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given new features `Xnew`, which should have the same scitype as `X` above. Predictions are probabilistic but uncalibrated.\n  * `predict_mode(mach, Xnew)`: Return the modes of the probabilistic predictions returned above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `chain`: The trained \"chain\" (Flux.jl model), namely the series of layers,  functions, and activations  which make up the neural network. This includes  the final layer specified by `finaliser` (eg, `softmax`).\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `training_losses`: A vector of training losses (penalised if `lambda != 0`) in  historical order, of length `epochs + 1`.  The first element is the pre-training loss.\n\n# Examples\n\nIn this example we use MLJFlux and a custom builder to classify the MNIST image dataset.\n\n```julia\nusing MLJ\nusing Flux\nimport MLJFlux\nimport MLJIteration # for `skip` control\n```\n\nFirst we want to download the MNIST dataset, and unpack into images and labels:\n\n```julia\nimport MLDatasets: MNIST\ndata = MNIST(split=:train)\nimages, labels = data.features, data.targets\n```\n\nIn MLJ, integers cannot be used for encoding categorical data, so we must coerce them into the `Multiclass` scitype:\n\n```julia\nlabels = coerce(labels, Multiclass);\n```\n\nAbove `images` is a single array but MLJFlux requires the images to be a vector of individual image arrays:\n\n```\nimages = coerce(images, GrayImage);\nimages[1]\n```\n\nWe start by defining a suitable `builder` object. This is a recipe for building the neural network. Our builder will work for images of any (constant) size, whether they be color or black and white (ie, single or multi-channel).  The architecture always consists of six alternating convolution and max-pool layers, and a final dense layer; the filter size and the number of channels after each convolution layer is customizable.\n\n```julia\nimport MLJFlux\n\nstruct MyConvBuilder\n    filter_size::Int\n    channels1::Int\n    channels2::Int\n    channels3::Int\nend\n\nmake2d(x::AbstractArray) = reshape(x, :, size(x)[end])\n\nfunction MLJFlux.build(b::MyConvBuilder, rng, n_in, n_out, n_channels)\n    k, c1, c2, c3 = b.filter_size, b.channels1, b.channels2, b.channels3\n    mod(k, 2) == 1 || error(\"`filter_size` must be odd. \")\n    p = div(k - 1, 2) # padding to preserve image size\n    init = Flux.glorot_uniform(rng)\n    front = Chain(\n        Conv((k, k), n_channels => c1, pad=(p, p), relu, init=init),\n        MaxPool((2, 2)),\n        Conv((k, k), c1 => c2, pad=(p, p), relu, init=init),\n        MaxPool((2, 2)),\n        Conv((k, k), c2 => c3, pad=(p, p), relu, init=init),\n        MaxPool((2 ,2)),\n        make2d)\n    d = Flux.outputsize(front, (n_in..., n_channels, 1)) |> first\n    return Chain(front, Dense(d, n_out, init=init))\nend\n```\n\nIt is important to note that in our `build` function, there is no final `softmax`. This is applied by default in all MLJFlux classifiers (override this using the `finaliser` hyperparameter).\n\nNow that our builder is defined, we can instantiate the actual MLJFlux model. If you have a GPU, you can substitute in `acceleration=CUDALibs()` below to speed up training.\n\n```julia\nImageClassifier = @load ImageClassifier pkg=MLJFlux\nclf = ImageClassifier(builder=MyConvBuilder(3, 16, 32, 32),\n                      batch_size=50,\n                      epochs=10,\n                      rng=123)\n```\n\nYou can add Flux options such as `optimiser` and `loss` in the snippet above. Currently, `loss` must be a flux-compatible loss, and not an MLJ measure.\n\nNext, we can bind the model with the data in a machine, and train using the first 500 images:\n\n```julia\nmach = machine(clf, images, labels);\nfit!(mach, rows=1:500, verbosity=2);\nreport(mach)\nchain = fitted_params(mach)\nFlux.params(chain)[2]\n```\n\nWe can tack on 20 more epochs by modifying the `epochs` field, and iteratively fit some more:\n\n```julia\nclf.epochs = clf.epochs + 20\nfit!(mach, rows=1:500, verbosity=2);\n```\n\nWe can also make predictions and calculate an out-of-sample loss estimate, using any MLJ measure (loss/score):\n\n```julia\npredicted_labels = predict(mach, rows=501:1000);\ncross_entropy(predicted_labels, labels[501:1000]) |> mean\n```\n\nThe preceding `fit!`/`predict`/evaluate workflow can be alternatively executed as follows:\n\n```julia\nevaluate!(mach,\n          resampling=Holdout(fraction_train=0.5),\n          measure=cross_entropy,\n          rows=1:1000,\n          verbosity=0)\n```\n\nSee also [`NeuralNetworkClassifier`](@ref).\n"
":name" = "ImageClassifier"
":human_name" = "image classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":predict"]
":hyperparameters" = "`(:builder, :finaliser, :optimiser, :loss, :epochs, :batch_size, :lambda, :alpha, :rng, :optimiser_changes_trigger_retraining, :acceleration)`"
":hyperparameter_types" = "`(\"Any\", \"Any\", \"Any\", \"Any\", \"Int64\", \"Int64\", \"Float64\", \"Float64\", \"Union{Int64, Random.AbstractRNG}\", \"Bool\", \"ComputationalResources.AbstractResource\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = ":epochs"
":supports_training_losses" = "`true`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`(:optimiser, :builder)`"
":reporting_operations" = "`()`"

[MLJFlux.NeuralNetworkRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJFlux"
":package_license" = "MIT"
":load_path" = "MLJFlux.NeuralNetworkRegressor"
":package_uuid" = "094fc8d1-fd35-5302-93ea-dabda2abf845"
":package_url" = "https://github.com/alan-turing-institute/MLJFlux.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nNeuralNetworkRegressor\n```\n\nA model type for constructing a neural network regressor, based on [MLJFlux.jl](https://github.com/alan-turing-institute/MLJFlux.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nNeuralNetworkRegressor = @load NeuralNetworkRegressor pkg=MLJFlux\n```\n\nDo `model = NeuralNetworkRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `NeuralNetworkRegressor(builder=...)`.\n\n`NeuralNetworkRegressor` is for training a data-dependent Flux.jl neural network to predict a `Continuous` target, given a table of `Continuous` features. Users provide a recipe for constructing the network, based on properties of the data that is encountered, by specifying an appropriate `builder`. See MLJFlux documentation for more on builders.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check the column scitypes with `schema(X)`.\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `Continuous`; check the scitype with `scitype(y)`\n\nTrain the machine with `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `builder=MLJFlux.Linear(σ=Flux.relu)`: An MLJFlux builder that constructs a neural  network. Possible `builders` include: `MLJFlux.Linear`, `MLJFlux.Short`, and  `MLJFlux.MLP`. See MLJFlux documentation for more on builders, and the example below  for using the `@builder` convenience macro.\n  * `optimiser::Flux.Adam()`: A `Flux.Optimise` optimiser. The optimiser performs the updating of the weights of the network. For further reference, see [the Flux optimiser documentation](https://fluxml.ai/Flux.jl/stable/training/optimisers/). To choose a learning rate (the update rate of the optimizer), a good rule of thumb is to start out at `10e-3`, and tune using powers of 10 between `1` and `1e-7`.\n  * `loss=Flux.mse`: The loss function which the network will optimize. Should be a function which can be called in the form `loss(yhat, y)`.  Possible loss functions are listed in [the Flux loss function documentation](https://fluxml.ai/Flux.jl/stable/models/losses/). For a regression task, natural loss functions are:\n\n      * `Flux.mse`\n      * `Flux.mae`\n      * `Flux.msle`\n      * `Flux.huber_loss`\n\n    Currently MLJ measures are not supported as loss functions here.\n  * `epochs::Int=10`: The duration of training, in epochs. Typically, one epoch represents one pass through the complete the training dataset.\n  * `batch_size::int=1`: the batch size to be used for training, representing the number of samples per update of the network weights. Typically, batch size is between 8 and\n\n    512. Increasing batch size may accelerate training if `acceleration=CUDALibs()` and a\n\n    GPU is available.\n  * `lambda::Float64=0`: The strength of the weight regularization penalty. Can be any value in the range `[0, ∞)`.\n  * `alpha::Float64=0`: The L2/L1 mix of regularization, in the range `[0, 1]`. A value of 0 represents L2 regularization, and a value of 1 represents L1 regularization.\n  * `rng::Union{AbstractRNG, Int64}`: The random number generator or seed used during training.\n  * `optimizer_changes_trigger_retraining::Bool=false`: Defines what happens when re-fitting a machine if the associated optimiser has changed. If `true`, the associated machine will retrain from scratch on `fit!` call, otherwise it will not.\n  * `acceleration::AbstractResource=CPU1()`: Defines on what hardware training is done. For Training on GPU, use `CUDALibs()`.\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given new features `Xnew`, which should have the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `chain`: The trained \"chain\" (Flux.jl model), namely the series of layers, functions,  and activations which make up the neural network.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `training_losses`: A vector of training losses (penalized if `lambda != 0`) in  historical order, of length `epochs + 1`.  The first element is the pre-training loss.\n\n# Examples\n\nIn this example we build a regression model for the Boston house price dataset.\n\n```julia\nusing MLJ\nimport MLJFlux\nusing Flux\n```\n\nFirst, we load in the data: The `:MEDV` column becomes the target vector `y`, and all remaining columns go into a table `X`, with the exception of `:CHAS`:\n\n```julia\ndata = OpenML.load(531); # Loads from https://www.openml.org/d/531\ny, X = unpack(data, ==(:MEDV), !=(:CHAS); rng=123);\n\nscitype(y)\nschema(X)\n```\n\nSince MLJFlux models do not handle ordered factors, we'll treat `:RAD` as `Continuous`:\n\n```julia\nX = coerce(X, :RAD=>Continuous)\n```\n\nSplitting off a test set:\n\n```julia\n(X, Xtest), (y, ytest) = partition((X, y), 0.7, multi=true);\n```\n\nNext, we can define a `builder`, making use of a convenience macro to do so.  In the following `@builder` call, `n_in` is a proxy for the number input features (which will be known at `fit!` time) and `rng` is a proxy for a RNG (which will be passed from the `rng` field of `model` defined below). We also have the parameter `n_out` which is the number of output features. As we are doing single target regression, the value passed will always be `1`, but the builder we define will also work for [`MultitargetNeuralRegressor`](@ref).\n\n```julia\nbuilder = MLJFlux.@builder begin\n    init=Flux.glorot_uniform(rng)\n    Chain(\n        Dense(n_in, 64, relu, init=init),\n        Dense(64, 32, relu, init=init),\n        Dense(32, n_out, init=init),\n    )\nend\n```\n\nInstantiating a model:\n\n```julia\nNeuralNetworkRegressor = @load NeuralNetworkRegressor pkg=MLJFlux\nmodel = NeuralNetworkRegressor(\n    builder=builder,\n    rng=123,\n    epochs=20\n)\n```\n\nWe arrange for standardization of the the target by wrapping our model in `TransformedTargetModel`, and standardization of the features by inserting the wrapped model in a pipeline:\n\n```julia\npipe = Standardizer |> TransformedTargetModel(model, target=Standardizer)\n```\n\nIf we fit with a high verbosity (>1), we will see the losses during training. We can also see the losses in the output of `report(mach)`.\n\n```julia\nmach = machine(pipe, X, y)\nfit!(mach, verbosity=2)\n\n# first element initial loss, 2:end per epoch training losses\nreport(mach).transformed_target_model_deterministic.model.training_losses\n```\n\n## Experimenting with learning rate\n\nWe can visually compare how the learning rate affects the predictions:\n\n```julia\nusing Plots\n\nrates = rates = [5e-5, 1e-4, 0.005, 0.001, 0.05]\nplt=plot()\n\nforeach(rates) do η\n  pipe.transformed_target_model_deterministic.model.optimiser.eta = η\n  fit!(mach, force=true, verbosity=0)\n  losses =\n      report(mach).transformed_target_model_deterministic.model.training_losses[3:end]\n  plot!(1:length(losses), losses, label=η)\nend\n\nplt\n\npipe.transformed_target_model_deterministic.model.optimiser.eta = 0.0001\n```\n\nWith the learning rate fixed, we compute a CV estimate of the performance (using all data bound to `mach`) and compare this with performance on the test set:\n\n```julia\n# CV estimate, based on `(X, y)`:\nevaluate!(mach, resampling=CV(nfolds=5), measure=l2)\n\n# loss for `(Xtest, test)`:\nfit!(mach) # train on `(X, y)`\nyhat = predict(mach, Xtest)\nl2(yhat, ytest)  |> mean\n```\n\nThese losses, for the pipeline model, refer to the target on the original, unstandardized, scale.\n\nFor implementing stopping criterion and other iteration controls, refer to examples linked from the MLJFlux documentation.\n\nSee also [`MultitargetNeuralNetworkRegressor`](@ref)\n"
":name" = "NeuralNetworkRegressor"
":human_name" = "neural network regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":predict"]
":hyperparameters" = "`(:builder, :optimiser, :loss, :epochs, :batch_size, :lambda, :alpha, :rng, :optimiser_changes_trigger_retraining, :acceleration)`"
":hyperparameter_types" = "`(\"Any\", \"Any\", \"Any\", \"Int64\", \"Int64\", \"Float64\", \"Float64\", \"Union{Integer, Random.AbstractRNG}\", \"Bool\", \"ComputationalResources.AbstractResource\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = ":epochs"
":supports_training_losses" = "`true`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`(:optimiser, :builder)`"
":reporting_operations" = "`()`"
