
[BetaML.RandomForestRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Trees.RandomForestRegressor"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```julia\nmutable struct RandomForestRegressor <: MLJModelInterface.Deterministic\n```\n\nA simple Random Forest model for regression with support for Missing data, from the Beta Machine Learning Toolkit (BetaML).\n\n# Hyperparameters:\n\n  * `n_trees::Int64`: Number of (decision) trees in the forest [def: `30`]\n  * `max_depth::Int64`: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: `0`, i.e. no limits]\n  * `min_gain::Float64`: The minimum information gain to allow for a node's partition [def: `0`]\n  * `min_records::Int64`: The minimum number of records a node must holds to consider for a partition of it [def: `2`]\n  * `max_features::Int64`: The maximum number of (random) features to consider at each partitioning [def: `0`, i.e. square root of the data dimension]\n  * `splitting_criterion::Function`: This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the \"impurity\" of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: `variance`]. Either `variance` or a custom function. It can also be an anonymous function.\n  * `β::Float64`: Parameter that regulate the weights of the scoring of each tree, to be (optionally) used in prediction based on the error of the individual trees computed on the records on which trees have not been trained. Higher values favour \"better\" trees, but too high values will cause overfitting [def: `0`, i.e. uniform weigths]\n  * `rng::Random.AbstractRNG`: A Random Number Generator to be used in stochastic parts of the code [deafult: `Random.GLOBAL_RNG`]\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> X, y                        = @load_boston;\n\njulia> modelType                   = @load RandomForestRegressor pkg = \"BetaML\" verbosity=0\nBetaML.Trees.RandomForestRegressor\n\njulia> model                       = modelType()\nRandomForestRegressor(\n  n_trees = 30, \n  max_depth = 0, \n  min_gain = 0.0, \n  min_records = 2, \n  max_features = 0, \n  splitting_criterion = BetaML.Utils.variance, \n  β = 0.0, \n  rng = Random._GLOBAL_RNG())\n\njulia> (fitResults, cache, report) = MLJ.fit(model, 0, X, y);\n\njulia> y_est                       = predict(model, fitResults, X)\n506-element Vector{Float64}:\n 25.283333333333335\n 22.700999999999997\n 36.67500000000002\n  ⋮\n 19.378333333333334\n 24.191666666666663\n 23.726666666666674\n 15.393333333333327\n```\n"
":name" = "RandomForestRegressor"
":human_name" = "random forest regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":predict"]
":hyperparameters" = "`(:n_trees, :max_depth, :min_gain, :min_records, :max_features, :splitting_criterion, :β, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Int64\", \"Float64\", \"Int64\", \"Int64\", \"Function\", \"Float64\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.GaussianMixtureImputer]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Imputation.GaussianMixtureImputer"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```julia\nmutable struct GaussianMixtureImputer <: MLJModelInterface.Unsupervised\n```\n\nImpute missing values using a probabilistic approach (Gaussian Mixture Models) fitted using the Expectation-Maximisation algorithm, from the Beta Machine Learning Toolkit (BetaML).\n\n# Hyperparameters:\n\n  * `n_classes::Int64`: Number of mixtures (latent classes) to consider [def: 3]\n  * `initial_probmixtures::Vector{Float64}`: Initial probabilities of the categorical distribution (n_classes x 1) [default: `[]`]\n  * `mixtures::Union{Type, Vector{<:BetaML.GMM.AbstractMixture}}`: An array (of length `n_classes``) of the mixtures to employ (see the [`?GMM`](@ref GMM) module in BetaML). Each mixture object can be provided with or without its parameters (e.g. mean and variance for the gaussian ones). Fully qualified mixtures are useful only if the`initialisation*strategy`parameter is  set to \"gived\"` This parameter can also be given symply in term of a _type*. In this case it is automatically extended to a vector of `n_classes``mixtures of the specified type. Note that mixing of different mixture types is not currently supported and that currently implemented mixtures are`SphericalGaussian`,`DiagonalGaussian`and`FullGaussian`. [def:`DiagonalGaussian`]\n  * `tol::Float64`: Tolerance to stop the algorithm [default: 10^(-6)]\n  * `minimum_variance::Float64`: Minimum variance for the mixtures [default: 0.05]\n  * `minimum_covariance::Float64`: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance.\n  * `initialisation_strategy::String`: The computation method of the vector of the initial mixtures. One of the following:\n\n      * \"grid\": using a grid approach\n      * \"given\": using the mixture provided in the fully qualified `mixtures` parameter\n      * \"kmeans\": use first kmeans (itself initialised with a \"grid\" strategy) to set the initial mixture centers [default]\n\n    Note that currently \"random\" and \"shuffle\" initialisations are not supported in gmm-based algorithms.\n\n  * `rng::Random.AbstractRNG`: A Random Number Generator to be used in stochastic parts of the code [deafult: `Random.GLOBAL_RNG`]\n\n# Example :\n\n```julia\njulia> using MLJ\n\njulia> X = [1 10.5;1.5 missing; 1.8 8; 1.7 15; 3.2 40; missing missing; 3.3 38; missing -2.3; 5.2 -2.4] |> table ;\n\njulia> modelType                   = @load GaussianMixtureImputer pkg = \"BetaML\" verbosity=0\nBetaML.Imputation.GaussianMixtureImputer\n\njulia> model                       = modelType(initialisation_strategy=\"grid\")\nGaussianMixtureImputer(\n  n_classes = 3, \n  initial_probmixtures = Float64[], \n  mixtures = BetaML.GMM.DiagonalGaussian{Float64}[BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing)], \n  tol = 1.0e-6, \n  minimum_variance = 0.05, \n  minimum_covariance = 0.0, \n  initialisation_strategy = \"grid\", \n  rng = Random._GLOBAL_RNG())\n\njulia> (fitResults, cache, report) = MLJ.fit(model, 0, X);\n\njulia> X_full                      = transform(model, fitResults, X) |> MLJ.matrix\n9×2 Matrix{Float64}:\n 1.0      10.5\n 1.5      14.7366\n 1.8       8.0\n 1.7      15.0\n 3.2      40.0\n 2.51842  15.1747\n 3.3      38.0\n 2.47412  -2.3\n 5.2      -2.4\n```\n"
":name" = "GaussianMixtureImputer"
":human_name" = "gaussian mixture imputer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":transform"]
":hyperparameters" = "`(:n_classes, :initial_probmixtures, :mixtures, :tol, :minimum_variance, :minimum_covariance, :initialisation_strategy, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Vector{Float64}\", \"Union{Type, Vector{<:BetaML.GMM.AbstractMixture}}\", \"Float64\", \"Float64\", \"Float64\", \"String\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.RandomForestClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.Finite}}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}, AbstractVector{<:Union{Missing, ScientificTypesBase.Finite}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Trees.RandomForestClassifier"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```julia\nmutable struct RandomForestClassifier <: MLJModelInterface.Probabilistic\n```\n\nA simple Random Forest model for classification with support for Missing data, from the Beta Machine Learning Toolkit (BetaML).\n\n# Hyperparameters:\n\n  * `n_trees::Int64`\n  * `max_depth::Int64`: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: `0`, i.e. no limits]\n  * `min_gain::Float64`: The minimum information gain to allow for a node's partition [def: `0`]\n  * `min_records::Int64`: The minimum number of records a node must holds to consider for a partition of it [def: `2`]\n  * `max_features::Int64`: The maximum number of (random) features to consider at each partitioning [def: `0`, i.e. square root of the data dimensions]\n  * `splitting_criterion::Function`: This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the \"impurity\" of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: `gini`]. Either `gini`, `entropy` or a custom function. It can also be an anonymous function.\n  * `β::Float64`: Parameter that regulate the weights of the scoring of each tree, to be (optionally) used in prediction based on the error of the individual trees computed on the records on which trees have not been trained. Higher values favour \"better\" trees, but too high values will cause overfitting [def: `0`, i.e. uniform weigths]\n  * `rng::Random.AbstractRNG`: A Random Number Generator to be used in stochastic parts of the code [deafult: `Random.GLOBAL_RNG`]\n\n# Example :\n\n```julia\njulia> using MLJ\n\njulia> X, y                        = @load_iris;\n\njulia> modelType                   = @load RandomForestClassifier pkg = \"BetaML\" verbosity=0\nBetaML.Trees.RandomForestClassifier\n\njulia> model                       = modelType()\nRandomForestClassifier(\n  n_trees = 30, \n  max_depth = 0, \n  min_gain = 0.0, \n  min_records = 2, \n  max_features = 0, \n  splitting_criterion = BetaML.Utils.gini, \n  β = 0.0, \n  rng = Random._GLOBAL_RNG())\n\njulia> (fitResults, cache, report) = MLJ.fit(model, 0, X, y);\n\njulia> class_est                   = predict(model, fitResults, X)\n150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float64}:\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n ⋮\n UnivariateFinite{Multiclass{3}}(setosa=>0.0, versicolor=>0.0, virginica=>1.0)\n UnivariateFinite{Multiclass{3}}(setosa=>0.0, versicolor=>0.0, virginica=>1.0)\n UnivariateFinite{Multiclass{3}}(setosa=>0.0, versicolor=>0.0, virginica=>1.0)\n```\n"
":name" = "RandomForestClassifier"
":human_name" = "random forest classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":fit", ":predict"]
":hyperparameters" = "`(:n_trees, :max_depth, :min_gain, :min_records, :max_features, :splitting_criterion, :β, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Int64\", \"Float64\", \"Int64\", \"Int64\", \"Function\", \"Float64\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.RandomForestImputer]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Known}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Known}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Imputation.RandomForestImputer"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```julia\nmutable struct RandomForestImputer <: MLJModelInterface.Unsupervised\n```\n\nImpute missing values using Random Forests, from the Beta Machine Learning Toolkit (BetaML).\n\n# Hyperparameters:\n\n  * `n_trees::Int64`: Number of (decision) trees in the forest [def: `30`]\n  * `max_depth::Union{Nothing, Int64}`: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: `nothing`, i.e. no limits]\n  * `min_gain::Float64`: The minimum information gain to allow for a node's partition [def: `0`]\n  * `min_records::Int64`: The minimum number of records a node must holds to consider for a partition of it [def: `2`]\n  * `max_features::Union{Nothing, Int64}`: The maximum number of (random) features to consider at each partitioning [def: `nothing`, i.e. square root of the data dimension]\n  * `forced_categorical_cols::Vector{Int64}`: Specify the positions of the integer columns to treat as categorical instead of cardinal. [Default: empty vector (all numerical cols are treated as cardinal by default and the others as categorical)]\n  * `splitting_criterion::Union{Nothing, Function}`: Either `gini`, `entropy` or `variance`. This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the \"impurity\" of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: `nothing`, i.e. `gini` for categorical labels (classification task) and `variance` for numerical labels(regression task)]. It can be an anonymous function.\n  * `recursive_passages::Int64`: Define the times to go trough the various columns to impute their data. Useful when there are data to impute on multiple columns. The order of the first passage is given by the decreasing number of missing values per column, the other passages are random [default: `1`].\n  * `rng::Random.AbstractRNG`: A Random Number Generator to be used in stochastic parts of the code [deafult: `Random.GLOBAL_RNG`]\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> X = [1 10.5;1.5 missing; 1.8 8; 1.7 15; 3.2 40; missing missing; 3.3 38; missing -2.3; 5.2 -2.4] |> table ;\n\njulia> modelType                   = @load RandomForestImputer pkg = \"BetaML\" verbosity=0\nBetaML.Imputation.RandomForestImputer\n\njulia> model                       = modelType(n_trees=40)\nRandomForestImputer(\n  n_trees = 40, \n  max_depth = nothing, \n  min_gain = 0.0, \n  min_records = 2, \n  max_features = nothing, \n  forced_categorical_cols = Int64[], \n  splitting_criterion = nothing, \n  recursive_passages = 1, \n  rng = Random._GLOBAL_RNG())\n\njulia> (fitResults, cache, report) = MLJ.fit(model, 0, X);\n\njulia> X_full                      = transform(model, fitResults, X) |> MLJ.matrix\n9×2 Matrix{Float64}:\n 1.0    10.5\n 1.5    10.3333\n 1.8     8.0\n 1.7    15.0\n 3.2    40.0\n 2.415   8.6545\n 3.3    38.0\n 3.72   -2.3\n 5.2    -2.4\n```\n"
":name" = "RandomForestImputer"
":human_name" = "random forest imputer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":transform"]
":hyperparameters" = "`(:n_trees, :max_depth, :min_gain, :min_records, :max_features, :forced_categorical_cols, :splitting_criterion, :recursive_passages, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Union{Nothing, Int64}\", \"Float64\", \"Int64\", \"Union{Nothing, Int64}\", \"Vector{Int64}\", \"Union{Nothing, Function}\", \"Int64\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.DecisionTreeRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Trees.DecisionTreeRegressor"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```julia\nmutable struct DecisionTreeRegressor <: MLJModelInterface.Deterministic\n```\n\nA simple Decision Tree model for regression with support for Missing data, from the Beta Machine Learning Toolkit (BetaML).\n\n# Hyperparameters:\n\n  * `max_depth::Int64`: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: `0`, i.e. no limits]\n  * `min_gain::Float64`: The minimum information gain to allow for a node's partition [def: `0`]\n  * `min_records::Int64`: The minimum number of records a node must holds to consider for a partition of it [def: `2`]\n  * `max_features::Int64`: The maximum number of (random) features to consider at each partitioning [def: `0`, i.e. look at all features]\n  * `splitting_criterion::Function`: This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the \"impurity\" of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: `variance`]. Either `variance` or a custom function. It can also be an anonymous function.\n  * `rng::Random.AbstractRNG`: A Random Number Generator to be used in stochastic parts of the code [deafult: `Random.GLOBAL_RNG`]\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> X, y                        = @load_boston;\n\njulia> modelType                   = @load DecisionTreeRegressor pkg = \"BetaML\" verbosity=0\nBetaML.Trees.DecisionTreeRegressor\n\njulia> model                       = modelType()\nDecisionTreeRegressor(\n  max_depth = 0, \n  min_gain = 0.0, \n  min_records = 2, \n  max_features = 0, \n  splitting_criterion = BetaML.Utils.variance, \n  rng = Random._GLOBAL_RNG())\n\njulia> (fitResults, cache, report) = MLJ.fit(model, 0, X, y);\n\njulia> y_est                       = predict(model, fitResults, X)\n506-element Vector{Float64}:\n 26.35\n 21.6\n 34.8\n  ⋮\n 23.75\n 22.2\n 13.2\n```\n"
":name" = "DecisionTreeRegressor"
":human_name" = "decision tree regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":predict"]
":hyperparameters" = "`(:max_depth, :min_gain, :min_records, :max_features, :splitting_criterion, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Float64\", \"Int64\", \"Int64\", \"Function\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.LinearPerceptron]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Infinite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Infinite}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Perceptron.LinearPerceptron"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```julia\nmutable struct LinearPerceptron <: MLJModelInterface.Probabilistic\n```\n\nThe classical perceptron algorithm using one-vs-all for multiclass, from the Beta Machine Learning Toolkit (BetaML).\n\n# Hyperparameters:\n\n  * `initial_coefficients::Union{Nothing, Matrix{Float64}}`: N-classes by D-dimensions matrix of initial linear coefficients [def: `nothing`, i.e. zeros]\n  * `initial_constant::Union{Nothing, Vector{Float64}}`: N-classes vector of initial contant terms [def: `nothing`, i.e. zeros]\n  * `epochs::Int64`: Maximum number of epochs, i.e. passages trough the whole training sample [def: `1000`]\n  * `shuffle::Bool`: Whether to randomly shuffle the data at each iteration (epoch) [def: `true`]\n  * `force_origin::Bool`: Whether to force the parameter associated with the constant term to remain zero [def: `false`]\n  * `return_mean_hyperplane::Bool`: Whether to return the average hyperplane coefficients instead of the final ones  [def: `false`]\n  * `rng::Random.AbstractRNG`: A Random Number Generator to be used in stochastic parts of the code [deafult: `Random.GLOBAL_RNG`]\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> X, y                        = @load_iris;\n\njulia> modelType                   = @load LinearPerceptron pkg = \"BetaML\"\n[ Info: For silent loading, specify `verbosity=0`. \nimport BetaML ✔\nBetaML.Perceptron.LinearPerceptron\n\njulia> model                       = modelType()\nLinearPerceptron(\n  initial_coefficients = nothing, \n  initial_constant = nothing, \n  epochs = 1000, \n  shuffle = true, \n  force_origin = false, \n  return_mean_hyperplane = false, \n  rng = Random._GLOBAL_RNG())\n\njulia> (fitResults, cache, report) = MLJ.fit(model, 0, X, y);\n\njulia> est_classes                 = predict(model, fitResults, X)\n150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt8, Float64}:\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>2.44e-35, virginica=>4.91e-306)\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>6.18e-20, virginica=>1.81e-280)\n ⋮\n UnivariateFinite{Multiclass{3}}(setosa=>1.26e-69, versicolor=>7.77e-89, virginica=>1.0)\n UnivariateFinite{Multiclass{3}}(setosa=>2.65e-102, versicolor=>1.45e-137, virginica=>1.0)\n UnivariateFinite{Multiclass{3}}(setosa=>6.79e-64, versicolor=>2.8400000000000003e-75, virginica=>1.0)\n```\n"
":name" = "LinearPerceptron"
":human_name" = "linear perceptron"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":fit", ":predict"]
":hyperparameters" = "`(:initial_coefficients, :initial_constant, :epochs, :shuffle, :force_origin, :return_mean_hyperplane, :rng)`"
":hyperparameter_types" = "`(\"Union{Nothing, Matrix{Float64}}\", \"Union{Nothing, Vector{Float64}}\", \"Int64\", \"Bool\", \"Bool\", \"Bool\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.Pegasos]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Infinite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Infinite}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Perceptron.Pegasos"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```julia\nmutable struct Pegasos <: MLJModelInterface.Probabilistic\n```\n\nThe gradient-based linear \"pegasos\" classifier using one-vs-all for multiclass, from the Beta Machine Learning Toolkit (BetaML).\n\n# Hyperparameters:\n\n  * `initial_coefficients::Union{Nothing, Matrix{Float64}}`: N-classes by D-dimensions matrix of initial linear coefficients [def: `nothing`, i.e. zeros]\n  * `initial_constant::Union{Nothing, Vector{Float64}}`: N-classes vector of initial contant terms [def: `nothing`, i.e. zeros]\n  * `learning_rate::Function`: Learning rate [def: (epoch -> 1/sqrt(epoch))]\n  * `learning_rate_multiplicative::Float64`: Multiplicative term of the learning rate [def: `0.5`]\n  * `epochs::Int64`: Maximum number of epochs, i.e. passages trough the whole training sample [def: `1000`]\n  * `shuffle::Bool`: Whether to randomly shuffle the data at each iteration (epoch) [def: `true`]\n  * `force_origin::Bool`: Whether to force the parameter associated with the constant term to remain zero [def: `false`]\n  * `return_mean_hyperplane::Bool`: Whether to return the average hyperplane coefficients instead of the final ones  [def: `false`]\n  * `rng::Random.AbstractRNG`: A Random Number Generator to be used in stochastic parts of the code [deafult: `Random.GLOBAL_RNG`]\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> X, y                        = @load_iris;\n\njulia> modelType                   = @load Pegasos pkg = \"BetaML\" verbosity=0\nBetaML.Perceptron.Pegasos\n\njulia> model                       = modelType()\nPegasos(\n  initial_coefficients = nothing, \n  initial_constant = nothing, \n  learning_rate = BetaML.Perceptron.var\"#71#73\"(), \n  learning_rate_multiplicative = 0.5, \n  epochs = 1000, \n  shuffle = true, \n  force_origin = false, \n  return_mean_hyperplane = false, \n  rng = Random._GLOBAL_RNG())\n\njulia> (fitResults, cache, report) = MLJ.fit(model, 0, X, y);\n\njulia> est_classes                 = predict(model, fitResults, X)\n150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt8, Float64}:\n UnivariateFinite{Multiclass{3}}(setosa=>0.867, versicolor=>0.0554, virginica=>0.0772)\n UnivariateFinite{Multiclass{3}}(setosa=>0.852, versicolor=>0.0691, virginica=>0.0785)\n UnivariateFinite{Multiclass{3}}(setosa=>0.865, versicolor=>0.0645, virginica=>0.0705)\n ⋮\n UnivariateFinite{Multiclass{3}}(setosa=>0.299, versicolor=>0.0667, virginica=>0.635)\n UnivariateFinite{Multiclass{3}}(setosa=>0.28, versicolor=>0.0598, virginica=>0.66)\n UnivariateFinite{Multiclass{3}}(setosa=>0.34, versicolor=>0.0798, virginica=>0.58)\n```\n"
":name" = "Pegasos"
":human_name" = "pegasos"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":fit", ":predict"]
":hyperparameters" = "`(:initial_coefficients, :initial_constant, :learning_rate, :learning_rate_multiplicative, :epochs, :shuffle, :force_origin, :return_mean_hyperplane, :rng)`"
":hyperparameter_types" = "`(\"Union{Nothing, Matrix{Float64}}\", \"Union{Nothing, Vector{Float64}}\", \"Function\", \"Float64\", \"Int64\", \"Bool\", \"Bool\", \"Bool\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.KMedoids]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`AbstractArray{<:ScientificTypesBase.Multiclass}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Clustering.KMedoids"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```julia\nmutable struct KMedoids <: MLJModelInterface.Unsupervised\n```\n\n# Parameters:\n\n  * `n_classes::Int64`: Number of classes to discriminate the data [def: 3]\n  * `dist::Function`: Function to employ as distance. Default to the Euclidean distance. Can be one of the predefined distances (`l1_distance`, `l2_distance`, `l2squared_distance`),  `cosine_distance`), any user defined function accepting two vectors and returning a scalar or an anonymous function with the same characteristics.\n  * `initialisation_strategy::String`: The computation method of the vector of the initial representatives. One of the following:\n\n      * \"random\": randomly in the X space\n      * \"grid\": using a grid approach\n      * \"shuffle\": selecting randomly within the available points [default]\n      * \"given\": using a provided set of initial representatives provided in the `initial_representatives` parameter\n\n  * `initial_representatives::Union{Nothing, Matrix{Float64}}`: Provided (K x D) matrix of initial representatives (useful only with `initialisation_strategy=\"given\"`) [default: `nothing`]\n  * `rng::Random.AbstractRNG`: Random Number Generator [deafult: `Random.GLOBAL_RNG`]\n\nThe K-medoids clustering algorithm with customisable distance function, from the Beta Machine Learning Toolkit (BetaML).\n\nSimilar to K-Means, but the \"representatives\" (the cetroids) are guaranteed to be one of the training points. The algorithm work with any arbitrary distance measure.\n\n# Notes:\n\n  * data must be numerical\n  * online fitting (re-fitting with new data) is supported\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> modelType                   = @load KMedoids pkg = \"BetaML\"\n[ Info: For silent loading, specify `verbosity=0`. \nimport BetaML ✔\nBetaML.Clustering.KMedoids\n\njulia> model                       = modelType()\nKMedoids(\n  n_classes = 3, \n  dist = BetaML.Clustering.var\"#49#51\"(), \n  initialisation_strategy = \"shuffle\", \n  initial_representatives = nothing, \n  rng = Random._GLOBAL_RNG())\n\njulia> X, y                        = @load_iris;\n\njulia> (fitResults, cache, report) = MLJ.fit(model, 0, X);\n\njulia> est_classes                 = predict(model, fitResults, X)\n150-element CategoricalArrays.CategoricalArray{Int64,1,UInt32}:\n 2\n 3\n 3\n ⋮\n 1\n 1\n 1\n```\n"
":name" = "KMedoids"
":human_name" = "k medoids"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":fitted_params", ":predict", ":transform"]
":hyperparameters" = "`(:n_classes, :dist, :initialisation_strategy, :initial_representatives, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Function\", \"String\", \"Union{Nothing, Matrix{Float64}}\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.NeuralNetworkRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}`"
":predict_scitype" = "`AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Nn.NeuralNetworkRegressor"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```julia\nmutable struct NeuralNetworkRegressor <: MLJModelInterface.Deterministic\n```\n\nA simple but flexible Feedforward Neural Network, from the Beta Machine Learning Toolkit (BetaML) for regression of a single dimensional target.\n\n# Parameters:\n\n  * `layers`: Array of layer objects [def: `nothing`, i.e. basic network]. See `subtypes(BetaML.AbstractLayer)` for supported layers\n  * `loss`: Loss (cost) function [def: `squared_cost`]. Should always assume y and ŷ as matrices, even if the regression task is 1-D\n\n    !!! warning\n        If you change the parameter `loss`, you need to either provide its derivative on the parameter `dloss` or use autodiff with `dloss=nothing`.\n\n  * `dloss`: Derivative of the loss function [def: `dsquared_cost`, i.e. use the derivative of the squared cost]. Use `nothing` for autodiff.\n  * `epochs`: Number of epochs, i.e. passages trough the whole training sample [def: `1000`]\n  * `batch_size`: Size of each individual batch [def: `32`]\n  * `opt_alg`: The optimisation algorithm to update the gradient at each batch [def: `ADAM()`]\n  * `shuffle`: Whether to randomly shuffle the data at each iteration (epoch) [def: `true`]\n  * `descr`: An optional title and/or description for this model\n  * `cb`: A call back function to provide information during training [def: `fitting_info`\n  * `rng`: Random Number Generator (see [`FIXEDSEED`](@ref)) [deafult: `Random.GLOBAL_RNG`]\n\n# Notes:\n\n  * data must be numerical\n  * the label should be be a *n-records* vector.\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> modelType                   = @load NeuralNetworkRegressor pkg = \"BetaML\"\n[ Info: For silent loading, specify `verbosity=0`. \nimport BetaML ✔\nBetaML.Nn.NeuralNetworkRegressor\n\njulia> layers                      = [BetaML.DenseLayer(12,20,f=BetaML.relu),BetaML.DenseLayer(20,20,f=BetaML.relu),BetaML.DenseLayer(20,1,f=BetaML.relu)];\n\njulia> model                       = modelType(layers=layers,opt_alg=BetaML.ADAM())\nNeuralNetworkRegressor(\n  layers = BetaML.Nn.AbstractLayer[BetaML.Nn.DenseLayer([-0.32801116352654236 0.19721617381409956 … 0.17423147551933688 -0.3203352184325144; 0.20325978849525422 0.2753359303406094 … -0.054177947724910136 0.040744621813733006; … ; 0.3614670391623493 0.4184392845285712 … -0.14577760559119207 -0.12430574279080464; -0.04477463648956215 -0.04575413793278821 … 0.2586292045719249 -0.4146332506686543], [0.386016400524039, -0.4120960765923787, -0.37660375260656787, 0.3754674172848425, 0.3933763861297827, -0.09574612456235942, 0.28147281593639867, -0.11333754049443168, -0.19680033976399594, -0.24747338342736486, 0.022885791740458516, -0.34253183385897484, 0.22126071792632201, -0.3539779424727334, -0.37335255502088455, -0.2462814314064721, 0.01620706528968724, -0.3724728631729394, 0.21311037493715396, -0.20613597904524303], BetaML.Utils.relu, nothing), BetaML.Nn.DenseLayer([0.37603456115187256 0.3542546426240723 … -0.0024023384912328916 0.1834672226168586; -0.1535424198342724 -0.07672083294894799 … -0.1433915698536904 -0.1633699269469485; … ; -0.16189872793833512 0.32683924051358165 … -0.08638288054654059 -0.3802058507922781; -0.19558165681593773 0.16664095708205845 … 0.2503794347207368 -0.031688833520039705], [0.021102385823098146, 0.22228546967483392, 0.1300959971946743, -0.20976715493972442, 0.04091175703653677, 0.023810417350970836, 0.2781644696873053, -0.3057357809062001, 0.10103624908600595, 0.12700817756236799, 0.08642857384856573, -0.1675652351991456, -0.17329950695590257, 0.12896500307404696, -0.1484448116427858, -0.24124008136893604, -0.08216916194774915, 0.33079670478470163, 0.19806334350809457, 0.32549757061401846], BetaML.Utils.relu, nothing), BetaML.Nn.DenseLayer([-0.035318774804408926 0.2774737129427495 … 0.07256585990736009 0.229332566953939], [0.39178172498331654], BetaML.Utils.relu, nothing)], \n  loss = BetaML.Utils.squared_cost, \n  dloss = BetaML.Utils.dsquared_cost, \n  epochs = 100, \n  batch_size = 32, \n  opt_alg = BetaML.Nn.ADAM(BetaML.Nn.var\"#69#72\"(), 1.0, 0.9, 0.999, 1.0e-8, BetaML.Nn.Learnable[], BetaML.Nn.Learnable[]), \n  shuffle = true, \n  descr = \"\", \n  cb = BetaML.Nn.fitting_info, \n  rng = Random._GLOBAL_RNG())\n\njulia> (fitResults, cache, report) = MLJ.fit(model, 0, X, y);\n\njulia> y_est                       = predict(model, fitResults, X)\n506-element Vector{Float64}:\n 29.67359458542452\n 27.72073250260763\n  ⋮\n 27.259757923962265\n```\n"
":name" = "NeuralNetworkRegressor"
":human_name" = "neural network regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":predict"]
":hyperparameters" = "`(:layers, :loss, :dloss, :epochs, :batch_size, :opt_alg, :shuffle, :descr, :cb, :rng)`"
":hyperparameter_types" = "`(\"Union{Nothing, Vector{BetaML.Nn.AbstractLayer}}\", \"Union{Nothing, Function}\", \"Union{Nothing, Function}\", \"Int64\", \"Int64\", \"BetaML.Nn.OptimisationAlgorithm\", \"Bool\", \"String\", \"Function\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.MultitargetGaussianMixtureRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Infinite}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractMatrix{<:ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Infinite}}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractMatrix{<:ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.GMM.MultitargetGaussianMixtureRegressor"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```julia\nmutable struct MultitargetGaussianMixtureRegressor <: MLJModelInterface.Deterministic\n```\n\nA non-linear regressor derived from fitting the data on a probabilistic model (Gaussian Mixture Model). Relatively fast.\n\nThis is the multi-target version of the model. If you want to predict a single label (y), use the MLJ model [`GaussianMixtureRegressor`](@ref).\n\n# Hyperparameters:\n\n  * `n_classes::Int64`: Number of mixtures (latent classes) to consider [def: 3]\n  * `initial_probmixtures::Vector{Float64}`: Initial probabilities of the categorical distribution (n_classes x 1) [default: `[]`]\n  * `mixtures::Union{Type, Vector{<:BetaML.GMM.AbstractMixture}}`: An array (of length `n_classes``) of the mixtures to employ (see the [`?GMM`](@ref GMM) module). Each mixture object can be provided with or without its parameters (e.g. mean and variance for the gaussian ones). Fully qualified mixtures are useful only if the`initialisation*strategy`parameter is  set to \"gived\"` This parameter can also be given symply in term of a _type*. In this case it is automatically extended to a vector of `n_classes``mixtures of the specified type. Note that mixing of different mixture types is not currently supported. [def:`[DiagonalGaussian() for i in 1:n_classes]`]\n  * `tol::Float64`: Tolerance to stop the algorithm [default: 10^(-6)]\n  * `minimum_variance::Float64`: Minimum variance for the mixtures [default: 0.05]\n  * `minimum_covariance::Float64`: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance (see notes).\n  * `initialisation_strategy::String`: The computation method of the vector of the initial mixtures. One of the following:\n\n      * \"grid\": using a grid approach\n      * \"given\": using the mixture provided in the fully qualified `mixtures` parameter\n      * \"kmeans\": use first kmeans (itself initialised with a \"grid\" strategy) to set the initial mixture centers [default]\n\n    Note that currently \"random\" and \"shuffle\" initialisations are not supported in gmm-based algorithms.\n\n  * `maximum_iterations::Int64`: Maximum number of iterations [def: `typemax(Int64)`, i.e. ∞]\n  * `rng::Random.AbstractRNG`: Random Number Generator [deafult: `Random.GLOBAL_RNG`]\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> modelType                   = @load MultitargetGaussianMixtureRegressor pkg = \"BetaML\"\n[ Info: For silent loading, specify `verbosity=0`. \nimport BetaML ✔\nBetaML.GMM.MultitargetGaussianMixtureRegressor\njulia> model                       = modelType()\nMultitargetGaussianMixtureRegressor(\n  n_classes = 3, \n  initial_probmixtures = Float64[], \n  mixtures = BetaML.GMM.DiagonalGaussian{Float64}[BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing)], \n  tol = 1.0e-6, \n  minimum_variance = 0.05, \n  minimum_covariance = 0.0, \n  initialisation_strategy = \"kmeans\", \n  maximum_iterations = 9223372036854775807, \n  rng = Random._GLOBAL_RNG())\n\njulia> X, y                        = @load_boston;\n\njulia> ydouble = hcat(y,y);\n\njulia> (fitResults, cache, report) = MLJ.fit(model, 0, X, ydouble);\n\njulia> y_est                       = predict(model, fitResults, X)\n506×2 Matrix{Float64}:\n 24.5785  24.5785\n 24.5785  24.5785\n 24.5785  24.5785\n  ⋮       \n 17.0039  17.0039\n 17.0039  17.0039\n 17.0039  17.0039\n```\n"
":name" = "MultitargetGaussianMixtureRegressor"
":human_name" = "multitarget gaussian mixture regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":predict"]
":hyperparameters" = "`(:n_classes, :initial_probmixtures, :mixtures, :tol, :minimum_variance, :minimum_covariance, :initialisation_strategy, :maximum_iterations, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Vector{Float64}\", \"Union{Type, Vector{<:BetaML.GMM.AbstractMixture}}\", \"Float64\", \"Float64\", \"Float64\", \"String\", \"Int64\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.GaussianMixtureRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Infinite}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Infinite}}}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.GMM.GaussianMixtureRegressor"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```julia\nmutable struct GaussianMixtureRegressor <: MLJModelInterface.Deterministic\n```\n\nA non-linear regressor derived from fitting the data on a probabilistic model (Gaussian Mixture Model). Relatively fast.\n\nThis is the single-target version of the model. If you want to predict several labels (y) at once, use the MLJ model [`MultitargetGaussianMixtureRegressor`](@ref).\n\n# Hyperparameters:\n\n  * `n_classes::Int64`: Number of mixtures (latent classes) to consider [def: 3]\n  * `initial_probmixtures::Vector{Float64}`: Initial probabilities of the categorical distribution (n_classes x 1) [default: `[]`]\n  * `mixtures::Union{Type, Vector{<:BetaML.GMM.AbstractMixture}}`: An array (of length `n_classes``) of the mixtures to employ (see the [`?GMM`](@ref GMM) module). Each mixture object can be provided with or without its parameters (e.g. mean and variance for the gaussian ones). Fully qualified mixtures are useful only if the`initialisation*strategy`parameter is  set to \"gived\"` This parameter can also be given symply in term of a _type*. In this case it is automatically extended to a vector of `n_classes``mixtures of the specified type. Note that mixing of different mixture types is not currently supported. [def:`[DiagonalGaussian() for i in 1:n_classes]`]\n  * `tol::Float64`: Tolerance to stop the algorithm [default: 10^(-6)]\n  * `minimum_variance::Float64`: Minimum variance for the mixtures [default: 0.05]\n  * `minimum_covariance::Float64`: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance (see notes).\n  * `initialisation_strategy::String`: The computation method of the vector of the initial mixtures. One of the following:\n\n      * \"grid\": using a grid approach\n      * \"given\": using the mixture provided in the fully qualified `mixtures` parameter\n      * \"kmeans\": use first kmeans (itself initialised with a \"grid\" strategy) to set the initial mixture centers [default]\n\n    Note that currently \"random\" and \"shuffle\" initialisations are not supported in gmm-based algorithms.\n\n  * `maximum_iterations::Int64`: Maximum number of iterations [def: `typemax(Int64)`, i.e. ∞]\n  * `rng::Random.AbstractRNG`: Random Number Generator [deafult: `Random.GLOBAL_RNG`]\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> modelType                   = @load GaussianMixtureRegressor pkg = \"BetaML\"\n[ Info: For silent loading, specify `verbosity=0`. \nimport BetaML ✔\nBetaML.GMM.GaussianMixtureRegressor\n\njulia> model                       = modelType()\nGaussianMixtureRegressor(\nn_classes = 3, \ninitial_probmixtures = Float64[], \nmixtures = BetaML.GMM.DiagonalGaussian{Float64}[BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing)], \ntol = 1.0e-6, \nminimum_variance = 0.05, \nminimum_covariance = 0.0, \ninitialisation_strategy = \"kmeans\", \nmaximum_iterations = 9223372036854775807, \nrng = Random._GLOBAL_RNG())\n\njulia> X, y                        = @load_boston;\n\njulia> (fitResults, cache, report) = MLJ.fit(model, 0, X, y);\n\njulia> y_est                       = predict(model, fitResults, X)\n506-element Vector{Float64}:\n24.703442835305577\n24.70344283512716\n24.70344283528249\n⋮\n17.172486989759676\n17.172486989759676\n17.172486989759644\n```\n"
":name" = "GaussianMixtureRegressor"
":human_name" = "gaussian mixture regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":predict"]
":hyperparameters" = "`(:n_classes, :initial_probmixtures, :mixtures, :tol, :minimum_variance, :minimum_covariance, :initialisation_strategy, :maximum_iterations, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Vector{Float64}\", \"Union{Type, Vector{<:BetaML.GMM.AbstractMixture}}\", \"Float64\", \"Float64\", \"Float64\", \"String\", \"Int64\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.MultitargetNeuralNetworkRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVecOrMat{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}, AbstractVecOrMat{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}`"
":predict_scitype" = "`AbstractVecOrMat{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Nn.MultitargetNeuralNetworkRegressor"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```julia\nmutable struct MultitargetNeuralNetworkRegressor <: MLJModelInterface.Deterministic\n```\n\nA simple but flexible Feedforward Neural Network, from the Beta Machine Learning Toolkit (BetaML) for regression of multiple dimensional targets.\n\n# Parameters:\n\n  * `layers`: Array of layer objects [def: `nothing`, i.e. basic network]. See `subtypes(BetaML.AbstractLayer)` for supported layers\n  * `loss`: Loss (cost) function [def: `squared_cost`].  Should always assume y and ŷ as matrices.\n\n    !!! warning\n        If you change the parameter `loss`, you need to either provide its derivative on the parameter `dloss` or use autodiff with `dloss=nothing`.\n\n  * `dloss`: Derivative of the loss function [def: `dsquared_cost`, i.e. use the derivative of the squared cost]. Use `nothing` for autodiff.\n  * `epochs`: Number of epochs, i.e. passages trough the whole training sample [def: `1000`]\n  * `batch_size`: Size of each individual batch [def: `32`]\n  * `opt_alg`: The optimisation algorithm to update the gradient at each batch [def: `ADAM()`]\n  * `shuffle`: Whether to randomly shuffle the data at each iteration (epoch) [def: `true`]\n  * `descr`: An optional title and/or description for this model\n  * `cb`: A call back function to provide information during training [def: `fitting_info`\n  * `rng`: Random Number Generator (see [`FIXEDSEED`](@ref)) [deafult: `Random.GLOBAL_RNG`]\n\n# Notes:\n\n  * data must be numerical\n  * the label should be a *n-records* by *n-dimensions* matrix\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> X, y                        = @load_boston;\n\njulia> ydouble                     = hcat(y,y);\n\njulia> modelType                   = @load MultitargetNeuralNetworkRegressor pkg = \"BetaML\"\n[ Info: For silent loading, specify `verbosity=0`. \nimport BetaML ✔\nBetaML.Nn.MultitargetNeuralNetworkRegressor\n\njulia> layers                      = [BetaML.DenseLayer(12,50,f=BetaML.relu),BetaML.DenseLayer(50,50,f=BetaML.relu),BetaML.DenseLayer(50,2,f=BetaML.relu)];\n\njulia> model                       = modelType(layers=layers,opt_alg=BetaML.ADAM())\nMultitargetNeuralNetworkRegressor(\n  layers = BetaML.Nn.AbstractLayer[BetaML.Nn.DenseLayer([-0.14268958168480084 0.1556430517823459 … -0.08125686623988268 -0.2544570399728793; 0.28423814923214763 -0.1372659640176363 … 0.2264470618518154 -0.06631320101636362; … ; 0.02789179672476405 0.28348513690171906 … 0.2871912147350063 0.11554385516710886; -0.06320205436628074 -0.10694711454519892 … -0.10253686449899962 -0.26585990317571573], [0.09338448989761905, 0.2718624735230576, 0.023797261385177626, -0.17917031167475778, 0.15385702004431373, 0.012842680042847276, -0.10232304504376691, -0.13099353498374394, -0.11649189067696844, 0.30591295324151513  …  -0.2972600758671511, -0.177382174249729, -0.26266997240771395, 0.20268565473608047, 0.014804452498253184, 0.24784415091647882, 0.27962551308477157, -0.2880952267241536, 0.26057211923117796, -0.044009535090302976], BetaML.Utils.relu, nothing), BetaML.Nn.DenseLayer([-0.10136741184492606 -0.13038485207770573 … 0.1165162505227173 -0.025817955934162834; -0.20802525780664402 0.15425857417999556 … -0.19434363128519133 0.17652319228668767; … ; -0.10027182894787812 -0.16280219623873593 … -0.16389190054287556 -0.16859625236026915; 0.03561207609341421 -0.05272100409252414 … 0.18362700621532496 -0.11053112518410535], [0.2049701239390826, 0.04727896759708039, 0.22583290172299525, 0.13866713565359567, -0.032397509451043055, 0.041099957445332624, -0.2401413229195337, -0.022035553374859435, -0.2420707290337102, -0.0007123143227169282  …  -0.04350755341649204, 0.13228009527783768, -0.1313043131118029, -0.09176750039253359, 0.17829147060531736, -0.22431760512441942, 0.022861161675965136, -0.022343912739403338, -0.15410438565251305, -0.16252399721019406], BetaML.Utils.relu, nothing), BetaML.Nn.DenseLayer([-0.1865482260376025 -0.12501419399141886 … 0.1502899731849523 0.26034732010433115; -0.2829352616445401 -0.13834226908657268 … -0.016410622720088086 0.0022255074057040414], [0.1750612378638422, 0.16520212643140864], BetaML.Utils.relu, nothing)], \n  loss = BetaML.Utils.squared_cost, \n  dloss = BetaML.Utils.dsquared_cost, \n  epochs = 100, \n  batch_size = 32, \n  opt_alg = BetaML.Nn.ADAM(BetaML.Nn.var\"#69#72\"(), 1.0, 0.9, 0.999, 1.0e-8, BetaML.Nn.Learnable[], BetaML.Nn.Learnable[]), \n  shuffle = true, \n  descr = \"\", \n  cb = BetaML.Nn.fitting_info, \n  rng = Random._GLOBAL_RNG())\n\njulia> (fitResults, cache, report) = MLJ.fit(model, -1, X, ydouble);\n\njulia> y_est                       = predict(model, fitResults, X)\n506×2 Matrix{Float64}:\n 29.7411  28.8886\n 25.8501  26.5058\n 29.3501  29.9779\n  ⋮       \n 30.3606  30.6514\n 28.2101  28.3246\n 24.1113  23.9118\n```\n"
":name" = "MultitargetNeuralNetworkRegressor"
":human_name" = "multitarget neural network regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":predict"]
":hyperparameters" = "`(:layers, :loss, :dloss, :epochs, :batch_size, :opt_alg, :shuffle, :descr, :cb, :rng)`"
":hyperparameter_types" = "`(\"Union{Nothing, Vector{BetaML.Nn.AbstractLayer}}\", \"Union{Nothing, Function}\", \"Union{Nothing, Function}\", \"Int64\", \"Int64\", \"BetaML.Nn.OptimisationAlgorithm\", \"Bool\", \"String\", \"Function\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.KernelPerceptron]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Infinite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Infinite}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Perceptron.KernelPerceptron"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```julia\nmutable struct KernelPerceptron <: MLJModelInterface.Probabilistic\n```\n\nThe kernel perceptron algorithm using one-vs-one for multiclass, from the Beta Machine Learning Toolkit (BetaML).\n\n# Hyperparameters:\n\n  * `kernel::Function`: Kernel function to employ. See `?radial_kernel` or `?polynomial_kernel` (once loaded the BetaML package) for details or check `?BetaML.Utils` to verify if other kernels are defined (you can alsways define your own kernel) [def: [`radial_kernel`](@ref)]\n  * `epochs::Int64`: Maximum number of epochs, i.e. passages trough the whole training sample [def: `100`]\n  * `initial_errors::Union{Nothing, Vector{Vector{Int64}}}`: Initial distribution of the number of errors errors [def: `nothing`, i.e. zeros]. If provided, this should be a nModels-lenght vector of nRecords integer values vectors , where nModels is computed as `(n_classes  * (n_classes - 1)) / 2`\n  * `shuffle::Bool`: Whether to randomly shuffle the data at each iteration (epoch) [def: `true`]\n  * `rng::Random.AbstractRNG`: A Random Number Generator to be used in stochastic parts of the code [deafult: `Random.GLOBAL_RNG`]\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> X, y                        = @load_iris;\n\njulia> modelType                   = @load KernelPerceptron pkg = \"BetaML\"\n[ Info: For silent loading, specify `verbosity=0`. \nimport BetaML ✔\nBetaML.Perceptron.KernelPerceptron\n\njulia> model                       = modelType()\nKernelPerceptron(\n  kernel = BetaML.Utils.radial_kernel, \n  epochs = 100, \n  initial_errors = nothing, \n  shuffle = true, \n  rng = Random._GLOBAL_RNG())\n\njulia> (fitResults, cache, report) = MLJ.fit(model, 0, X, y);\n\njulia> est_classes                 = predict(model, fitResults, X)\n150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt8, Float64}:\n UnivariateFinite{Multiclass{3}}(setosa=>0.665, versicolor=>0.245, virginica=>0.09)\n UnivariateFinite{Multiclass{3}}(setosa=>0.665, versicolor=>0.245, virginica=>0.09)\n ⋮\n UnivariateFinite{Multiclass{3}}(setosa=>0.09, versicolor=>0.245, virginica=>0.665)\n UnivariateFinite{Multiclass{3}}(setosa=>0.09, versicolor=>0.245, virginica=>0.665)\n UnivariateFinite{Multiclass{3}}(setosa=>0.09, versicolor=>0.245, virginica=>0.665)\n```\n"
":name" = "KernelPerceptron"
":human_name" = "kernel perceptron"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":fit", ":predict"]
":hyperparameters" = "`(:kernel, :epochs, :initial_errors, :shuffle, :rng)`"
":hyperparameter_types" = "`(\"Function\", \"Int64\", \"Union{Nothing, Vector{Vector{Int64}}}\", \"Bool\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.KMeans]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`AbstractArray{<:ScientificTypesBase.Multiclass}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Clustering.KMeans"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```julia\nmutable struct KMeans <: MLJModelInterface.Unsupervised\n```\n\nThe classical KMeans clustering algorithm, from the Beta Machine Learning Toolkit (BetaML).\n\n# Parameters:\n\n  * `n_classes::Int64`: Number of classes to discriminate the data [def: 3]\n  * `dist::Function`: Function to employ as distance. Default to the Euclidean distance. Can be one of the predefined distances (`l1_distance`, `l2_distance`, `l2squared_distance`),  `cosine_distance`), any user defined function accepting two vectors and returning a scalar or an anonymous function with the same characteristics. Attention that, contrary to `KMedoids`, the `KMeansClusterer` algorithm is not guaranteed to converge with other distances than the Euclidean one.\n  * `initialisation_strategy::String`: The computation method of the vector of the initial representatives. One of the following:\n\n      * \"random\": randomly in the X space\n      * \"grid\": using a grid approach\n      * \"shuffle\": selecting randomly within the available points [default]\n      * \"given\": using a provided set of initial representatives provided in the `initial_representatives` parameter\n\n  * `initial_representatives::Union{Nothing, Matrix{Float64}}`: Provided (K x D) matrix of initial representatives (useful only with `initialisation_strategy=\"given\"`) [default: `nothing`]\n  * `rng::Random.AbstractRNG`: Random Number Generator [deafult: `Random.GLOBAL_RNG`]\n\n# Notes:\n\n  * data must be numerical\n  * online fitting (re-fitting with new data) is supported\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> modelType                   = @load KMeans pkg = \"BetaML\"\n[ Info: For silent loading, specify `verbosity=0`. \nimport BetaML ✔\nBetaML.Clustering.KMeans\n\njulia> model                       = modelType()\nKMeans(\n n_classes = 3, \n dist = BetaML.Clustering.var\"#25#27\"(), \n initialisation_strategy = \"shuffle\", \n initial_representatives = nothing, \n rng = Random._GLOBAL_RNG())\n\njulia> X, y                        = @load_iris;\n\njulia> (fitResults, cache, report) = MLJ.fit(model, 0, X);\n\njulia> est_classes                 = predict(model, fitResults, X)\n150-element CategoricalArrays.CategoricalArray{Int64,1,UInt32}:\n3\n3\n3\n⋮\n1\n1\n2\n```\n"
":name" = "KMeans"
":human_name" = "k means"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":fitted_params", ":predict", ":transform"]
":hyperparameters" = "`(:n_classes, :dist, :initialisation_strategy, :initial_representatives, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Function\", \"String\", \"Union{Nothing, Matrix{Float64}}\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.DecisionTreeClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.Finite}}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}, AbstractVector{<:Union{Missing, ScientificTypesBase.Finite}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Trees.DecisionTreeClassifier"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```julia\nmutable struct DecisionTreeClassifier <: MLJModelInterface.Probabilistic\n```\n\nA simple Decision Tree model for classification with support for Missing data, from the Beta Machine Learning Toolkit (BetaML).\n\n# Hyperparameters:\n\n  * `max_depth::Int64`: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: `0`, i.e. no limits]\n  * `min_gain::Float64`: The minimum information gain to allow for a node's partition [def: `0`]\n  * `min_records::Int64`: The minimum number of records a node must holds to consider for a partition of it [def: `2`]\n  * `max_features::Int64`: The maximum number of (random) features to consider at each partitioning [def: `0`, i.e. look at all features]\n  * `splitting_criterion::Function`: This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the \"impurity\" of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: `gini`]. Either `gini`, `entropy` or a custom function. It can also be an anonymous function.\n  * `rng::Random.AbstractRNG`: A Random Number Generator to be used in stochastic parts of the code [deafult: `Random.GLOBAL_RNG`]\n\n# Example:\n\n```julia\n\njulia> using MLJ\n\njulia> X, y                        = @load_iris;\n\njulia> modelType                   = @load DecisionTreeClassifier pkg = \"BetaML\" verbosity=0\nBetaML.Trees.DecisionTreeClassifier\n\njulia> model                       = modelType()\nDecisionTreeClassifier(\n  max_depth = 0, \n  min_gain = 0.0, \n  min_records = 2, \n  max_features = 0, \n  splitting_criterion = BetaML.Utils.gini, \n  rng = Random._GLOBAL_RNG())\n\njulia> (fitResults, cache, report) = MLJ.fit(model, 0, X, y);\n\njulia> class_est                   = predict(model, fitResults, X)\n150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float64}:\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n ⋮\n UnivariateFinite{Multiclass{3}}(setosa=>0.0, versicolor=>0.0, virginica=>1.0)\n UnivariateFinite{Multiclass{3}}(setosa=>0.0, versicolor=>0.0, virginica=>1.0)\n UnivariateFinite{Multiclass{3}}(setosa=>0.0, versicolor=>0.0, virginica=>1.0)\n```\n"
":name" = "DecisionTreeClassifier"
":human_name" = "decision tree classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":fit", ":predict"]
":hyperparameters" = "`(:max_depth, :min_gain, :min_records, :max_features, :splitting_criterion, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Float64\", \"Int64\", \"Int64\", \"Function\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.GeneralImputer]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Known}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Known}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Imputation.GeneralImputer"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```julia\nmutable struct GeneralImputer <: MLJModelInterface.Unsupervised\n```\n\nImpute missing values using arbitrary learning models, from the Beta Machine Learning Toolkit (BetaML).\n\nImpute missing values using a vector (one per column) of arbitrary learning models (classifiers/regressors, not necessarily from BetaML) that:\n\n  * implement the interface `m = Model([options])`, `train!(m,X,Y)` and `predict(m,X)`;\n  * accept missing data in the feature matrix.\n\n(default to Random Forests)\n\n# Hyperparameters:\n\n  * `estimators::Union{Nothing, Vector}`: A D-dimensions vector of regressor or classifier models (and eventually their respective options/hyper-parameters) to be used to impute the various columns of the matrix [default: `nothing`, i.e. use random forests].\n  * `recursive_passages::Int64`: Define the times to go trough the various columns to impute their data. Useful when there are data to impute on multiple columns. The order of the first passage is given by the decreasing number of missing values per column, the other passages are random [default: `1`].\n  * `rng::Random.AbstractRNG`: A Random Number Generator to be used in stochastic parts of the code [deafult: `Random.GLOBAL_RNG`]\n\n# Example :\n\n```julia\njulia> using MLJ\n\njulia> X = [\"a\" 10.5;\"a\" missing; \"b\" 8; \"b\" 15; \"c\" 40; missing missing; \"c\" 38; missing -2.3; \"c\" -2.4] |> table ;\n\njulia> modelType                   = @load GeneralImputer pkg = \"BetaML\" verbosity=0\nGeneralImputer\n\njulia> model                       = modelType(estimators=[BetaML.DecisionTreeEstimator(),BetaML.RandomForestEstimator(n_trees=40)],recursive_passages=2)\nGeneralImputer(\n  estimators = BetaMLSupervisedModel[DecisionTreeEstimator - A Decision Tree model (unfitted), RandomForestEstimator - A 40 trees Random Forest model (unfitted)], \n  recursive_passages = 2, \n  rng = Random._GLOBAL_RNG())\n\njulia> (fitResults, cache, report) = MLJ.fit(model, 0, X);\n\njulia> X_full                      = transform(model, fitResults, X) |> MLJ.matrix\n9×2 Matrix{Any}:\n \"a\"  10.5\n \"a\"  10.5\n \"b\"   8\n \"b\"  15\n \"c\"  40\n \"a\"  10.5\n \"c\"  38\n \"c\"  -2.3\n \"c\"  -2.4\n```\n"
":name" = "GeneralImputer"
":human_name" = "general imputer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":transform"]
":hyperparameters" = "`(:estimators, :recursive_passages, :rng)`"
":hyperparameter_types" = "`(\"Union{Nothing, Vector}\", \"Int64\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.NeuralNetworkClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:Union{ScientificTypesBase.Count, ScientificTypesBase.Finite}}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}, AbstractVector{<:Union{ScientificTypesBase.Count, ScientificTypesBase.Finite}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Nn.NeuralNetworkClassifier"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```julia\nmutable struct NeuralNetworkClassifier <: MLJModelInterface.Probabilistic\n```\n\nA simple but flexible Feedforward Neural Network, from the Beta Machine Learning Toolkit (BetaML) for classification  problems.\n\n# Parameters:\n\n  * `layers`: Array of layer objects [def: `nothing`, i.e. basic network]. See `subtypes(BetaML.AbstractLayer)` for supported layers. The last \"softmax\" layer is automatically added.\n  * `loss`: Loss (cost) function [def: `crossentropy`]. Should always assume y and ŷ as matrices.\n\n    !!! warning\n        If you change the parameter `loss`, you need to either provide its derivative on the parameter `dloss` or use autodiff with `dloss=nothing`.\n\n  * `dloss`: Derivative of the loss function [def: `dcrossentropy`, i.e. the derivative of the cross-entropy]. Use `nothing` for autodiff.\n  * `epochs`: Number of epochs, i.e. passages trough the whole training sample [def: `1000`]\n  * `batch_size`: Size of each individual batch [def: `32`]\n  * `opt_alg`: The optimisation algorithm to update the gradient at each batch [def: `BetaML.ADAM()`]\n  * `shuffle`: Whether to randomly shuffle the data at each iteration (epoch) [def: `true`]\n  * `descr`: An optional title and/or description for this model\n  * `cb`: A call back function to provide information during training [def: `BetaML.fitting_info`\n  * `categories`: The categories to represent as columns. [def: `nothing`, i.e. unique training values].\n  * `handle_unknown`: How to handle categories not seens in training or not present in the provided `categories` array? \"error\" (default) rises an error, \"infrequent\" adds a specific column for these categories.\n  * `other_categories_name`: Which value during prediction to assign to this \"other\" category (i.e. categories not seen on training or not present in the provided `categories` array? [def: `nothing`, i.e. typemax(Int64) for integer vectors and \"other\" for other types]. This setting is active only if `handle_unknown=\"infrequent\"` and in that case it MUST be specified if Y is neither integer or strings\n  * `rng`: Random Number Generator [deafult: `Random.GLOBAL_RNG`]\n\n# Notes:\n\n  * data must be numerical\n  * the label should be a *n-records* by *n-dimensions* matrix (e.g. a one-hot-encoded data for classification), where the output columns should be interpreted as the probabilities for each categories.\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> X, y                        = @load_iris;\n\njulia> modelType                   = @load NeuralNetworkClassifier pkg = \"BetaML\"\n[ Info: For silent loading, specify `verbosity=0`. \nimport BetaML ✔\nBetaML.Nn.NeuralNetworkClassifier\n\njulia> layers                      = [BetaML.DenseLayer(4,8,f=BetaML.relu),BetaML.DenseLayer(8,8,f=BetaML.relu),BetaML.DenseLayer(8,3,f=BetaML.relu),BetaML.VectorFunctionLayer(3,f=BetaML.softmax)];\n\njulia> model                       = modelType(layers=layers,opt_alg=BetaML.ADAM())\nNeuralNetworkClassifier(\n  layers = BetaML.Nn.AbstractLayer[BetaML.Nn.DenseLayer([-0.13065425957999977 0.3006718045454293 -0.14208182654389845 -0.010396909703178414; 0.048520032692515036 -0.015206389893573924 0.10185996867206404 0.3322496808168578; … ; -0.35259614611009477 0.6482620436066895 0.008337847389667918 -0.12305204287019345; 0.4658422589725906 0.6934957957952972 -0.3085357878320247 0.20222661286207866], [0.36174111580772195, -0.35269496628536656, 0.26811746239579826, 0.5528187653581791, -0.3510634981562191, 0.10825967870150688, 0.3022797568475024, 0.4981155176339185], BetaML.Utils.relu, nothing), BetaML.Nn.DenseLayer([-0.10421417572899494 -0.35499611903472195 … -0.3335182269175171 -0.3985778486065036; 0.23543572035878935 0.59952318489473 … 0.2795331413389591 -0.5720523377542953; … ; 0.2647745208772335 -0.3248093104701972 … 0.3974038426324087 -0.08540125672267229; 0.5192880535413722 0.484381279307307 … 0.5908202412047914 0.3565865691496263], [-0.43847147676332937, -0.0792557647479405, 0.28527379769156247, 0.472161396182901, 0.5499454540456155, -0.24120815998677952, 0.07292491907243237, 0.6046011380800786], BetaML.Utils.relu, nothing), BetaML.Nn.DenseLayer([0.07404458231451261 -0.6297338418338474 … -0.5203349840135756 0.2659245561353357; -0.03739230431842255 -0.7175051212845613 … 0.7131622720546834 -0.6340542706678468; -0.14453639566110688 0.38900994015838364 … 0.5074513955919556 0.34154609716155104], [-0.39346454660088837, -0.3091008284310222, -0.03586152622920202], BetaML.Utils.relu, nothing), BetaML.Nn.VectorFunctionLayer{0}(fill(NaN), 3, 3, BetaML.Utils.softmax, nothing, nothing)], \n  loss = BetaML.Utils.crossentropy, \n  dloss = BetaML.Utils.dcrossentropy, \n  epochs = 100, \n  batch_size = 32, \n  opt_alg = BetaML.Nn.ADAM(BetaML.Nn.var\"#69#72\"(), 1.0, 0.9, 0.999, 1.0e-8, BetaML.Nn.Learnable[], BetaML.Nn.Learnable[]), \n  shuffle = true, \n  descr = \"\", \n  cb = BetaML.Nn.fitting_info, \n  categories = nothing, \n  handle_unknown = \"error\", \n  other_categories_name = nothing, \n  rng = Random._GLOBAL_RNG())\n\njulia> (fitResults, cache, report) = MLJ.fit(model, 0, X, y);\n\njulia> est_classes                 = predict(model, fitResults, X)\n150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt8, Float64}:\n UnivariateFinite{Multiclass{3}}(setosa=>0.57, versicolor=>0.215, virginica=>0.215)\n UnivariateFinite{Multiclass{3}}(setosa=>0.565, versicolor=>0.217, virginica=>0.217)\n ⋮\n UnivariateFinite{Multiclass{3}}(setosa=>0.255, versicolor=>0.255, virginica=>0.49)\n UnivariateFinite{Multiclass{3}}(setosa=>0.254, versicolor=>0.254, virginica=>0.492)\n UnivariateFinite{Multiclass{3}}(setosa=>0.263, versicolor=>0.263, virginica=>0.473)\n```\n"
":name" = "NeuralNetworkClassifier"
":human_name" = "neural network classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":fit", ":predict"]
":hyperparameters" = "`(:layers, :loss, :dloss, :epochs, :batch_size, :opt_alg, :shuffle, :descr, :cb, :categories, :handle_unknown, :other_categories_name, :rng)`"
":hyperparameter_types" = "`(\"Union{Nothing, Vector{BetaML.Nn.AbstractLayer}}\", \"Union{Nothing, Function}\", \"Union{Nothing, Function}\", \"Int64\", \"Int64\", \"BetaML.Nn.OptimisationAlgorithm\", \"Bool\", \"String\", \"Function\", \"Union{Nothing, Vector}\", \"String\", \"Any\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.SimpleImputer]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.Imputation.SimpleImputer"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```julia\nmutable struct SimpleImputer <: MLJModelInterface.Unsupervised\n```\n\nImpute missing values using feature (column) mean, with optional record normalisation (using l-`norm` norms), from the Beta Machine Learning Toolkit (BetaML).\n\n# Hyperparameters:\n\n  * `statistic::Function`: The descriptive statistic of the column (feature) to use as imputed value [def: `mean`]\n  * `norm::Union{Nothing, Int64}`: Normalise the feature mean by l-`norm` norm of the records [default: `nothing`]. Use it (e.g. `norm=1` to use the l-1 norm) if the records are highly heterogeneus (e.g. quantity exports of different countries).\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> X = [1 10.5;1.5 missing; 1.8 8; 1.7 15; 3.2 40; missing missing; 3.3 38; missing -2.3; 5.2 -2.4] |> table ;\n\njulia> modelType                   = @load SimpleImputer pkg = \"BetaML\" verbosity=0\nBetaML.Imputation.SimpleImputer\n\njulia> model                       = modelType(norm=1)\nSimpleImputer(\n  statistic = Statistics.mean, \n  norm = 1)\n\njulia> (fitResults, cache, report) = MLJ.fit(model, 0, X);\n\njulia> X_full                      = transform(model, fitResults, X) |> MLJ.matrix\n9×2 Matrix{Float64}:\n 1.0        10.5\n 1.5         0.295466\n 1.8         8.0\n 1.7        15.0\n 3.2        40.0\n 0.280952    1.69524\n 3.3        38.0\n 0.0750839  -2.3\n 5.2        -2.4\n```\n"
":name" = "SimpleImputer"
":human_name" = "simple imputer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":transform"]
":hyperparameters" = "`(:statistic, :norm)`"
":hyperparameter_types" = "`(\"Function\", \"Union{Nothing, Int64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[BetaML.GaussianMixtureClusterer]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`AbstractArray{<:ScientificTypesBase.Multiclass}`"
":target_scitype" = "`AbstractArray{<:ScientificTypesBase.Multiclass}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractArray{<:ScientificTypesBase.Multiclass}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}`"
":is_pure_julia" = "`true`"
":package_name" = "BetaML"
":package_license" = "MIT"
":load_path" = "BetaML.GMM.GaussianMixtureClusterer"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```julia\nmutable struct GaussianMixtureClusterer <: MLJModelInterface.Unsupervised\n```\n\nA Expectation-Maximisation clustering algorithm with customisable mixtures, from the Beta Machine Learning Toolkit (BetaML).\n\n# Hyperparameters:\n\n  * `n_classes::Int64`: Number of mixtures (latent classes) to consider [def: 3]\n  * `initial_probmixtures::AbstractVector{Float64}`: Initial probabilities of the categorical distribution (n_classes x 1) [default: `[]`]\n  * `mixtures::Union{Type, Vector{<:BetaML.GMM.AbstractMixture}}`: An array (of length `n_classes``) of the mixtures to employ (see the [`?GMM`](@ref GMM) module). Each mixture object can be provided with or without its parameters (e.g. mean and variance for the gaussian ones). Fully qualified mixtures are useful only if the`initialisation*strategy`parameter is  set to \"gived\"` This parameter can also be given symply in term of a _type*. In this case it is automatically extended to a vector of `n_classes``mixtures of the specified type. Note that mixing of different mixture types is not currently supported. [def:`[DiagonalGaussian() for i in 1:n_classes]`]\n  * `tol::Float64`: Tolerance to stop the algorithm [default: 10^(-6)]\n  * `minimum_variance::Float64`: Minimum variance for the mixtures [default: 0.05]\n  * `minimum_covariance::Float64`: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance (see notes).\n  * `initialisation_strategy::String`: The computation method of the vector of the initial mixtures. One of the following:\n\n      * \"grid\": using a grid approach\n      * \"given\": using the mixture provided in the fully qualified `mixtures` parameter\n      * \"kmeans\": use first kmeans (itself initialised with a \"grid\" strategy) to set the initial mixture centers [default]\n\n    Note that currently \"random\" and \"shuffle\" initialisations are not supported in gmm-based algorithms.\n  * `maximum_iterations::Int64`: Maximum number of iterations [def: `typemax(Int64)`, i.e. ∞]\n  * `rng::Random.AbstractRNG`: Random Number Generator [deafult: `Random.GLOBAL_RNG`]\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> modelType                   = @load GaussianMixtureClusterer pkg = \"BetaML\"\n[ Info: For silent loading, specify `verbosity=0`. \nimport BetaML ✔\nBetaML.GMM.GaussianMixtureClusterer\n\njulia> model                       = modelType()\nGaussianMixtureClusterer(\n  n_classes = 3, \n  initial_probmixtures = Float64[], \n  mixtures = BetaML.GMM.DiagonalGaussian{Float64}[BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing)], \n  tol = 1.0e-6, \n  minimum_variance = 0.05, \n  minimum_covariance = 0.0, \n  initialisation_strategy = \"kmeans\", \n  maximum_iterations = 9223372036854775807, \n  rng = Random._GLOBAL_RNG())\n\njulia> X, y                        = @load_iris;\n\njulia> (fitResults, cache, report) = MLJ.fit(model, 0, X);\n\njulia> est_classes                 = predict(model, fitResults, X)\n150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, Int64, UInt32, Float64}:\n UnivariateFinite{Multiclass{3}}(1=>1.0, 2=>4.17e-15, 3=>2.1900000000000003e-31)\n UnivariateFinite{Multiclass{3}}(1=>1.0, 2=>1.25e-13, 3=>5.87e-31)\n UnivariateFinite{Multiclass{3}}(1=>1.0, 2=>4.5e-15, 3=>1.55e-32)\n ⋮\n UnivariateFinite{Multiclass{3}}(1=>5.39e-25, 2=>0.0167, 3=>0.983)\n UnivariateFinite{Multiclass{3}}(1=>7.5e-29, 2=>0.000106, 3=>1.0)\n UnivariateFinite{Multiclass{3}}(1=>1.6e-20, 2=>0.594, 3=>0.406)\n\njulia> \n```\n"
":name" = "GaussianMixtureClusterer"
":human_name" = "gaussian mixture clusterer"
":is_supervised" = "`false`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:n_classes, :initial_probmixtures, :mixtures, :tol, :minimum_variance, :minimum_covariance, :initialisation_strategy, :maximum_iterations, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"AbstractVector{Float64}\", \"Union{Type, Vector{<:BetaML.GMM.AbstractMixture}}\", \"Float64\", \"Float64\", \"Float64\", \"String\", \"Int64\", \"Random.AbstractRNG\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[CatBoost.CatBoostRegressor]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}, AbstractVector{<:ScientificTypesBase.Multiclass}}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}, AbstractVector{<:ScientificTypesBase.Multiclass}}}, AbstractMatrix{ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "CatBoost"
":package_license" = "MIT"
":load_path" = "CatBoost.MLJCatBoostInterface.CatBoostRegressor"
":package_uuid" = "e2e10f9a-a85d-4fa9-b6b2-639a32100a12"
":package_url" = "https://github.com/JuliaAI/CatBoost.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nCatBoostRegressor\n```\n\nA model type for constructing a CatBoost regressor, based on [CatBoost.jl](https://github.com/JuliaAI/CatBoost.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nCatBoostRegressor = @load CatBoostRegressor pkg=CatBoost\n```\n\nDo `model = CatBoostRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `CatBoostRegressor(iterations=...)`.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have one of the following element scitypes: `Continuous`, `Count`, `Finite`, `Textual`; check column scitypes with `schema(X)`. `Textual` columns will be passed to catboost as `text_features`, `Multiclass` columns will be passed to catboost as `cat_features`, and `OrderedFactor` columns will be converted to integers.\n  * `y`: the target, which can be any `AbstractVector` whose element scitype is `Continuous`; check the scitype with `scitype(y)`\n\nTrain the machine with `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\nMore details on the catboost hyperparameters, here are the Python docs:  https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier#parameters\n\n# Operations\n\n  * `predict(mach, Xnew)`: probabilistic predictions of the target given new features `Xnew` having the same scitype as `X` above.\n\n# Accessor functions\n\n  * `feature_importances(mach)`: return vector of feature importances, in the form of   `feature::Symbol => importance::Real` pairs\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `model`: The Python CatBoostRegressor model\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `feature_importances`: Vector{Pair{Symbol, Float64}} of feature importances\n\n# Examples\n\n```\nusing CatBoost.MLJCatBoostInterface\nusing MLJ\n\nX = (\n    duration = [1.5, 4.1, 5.0, 6.7], \n    n_phone_calls = [4, 5, 6, 7], \n    department = coerce([\"acc\", \"ops\", \"acc\", \"ops\"], Multiclass), \n)\ny = [2.0, 4.0, 6.0, 7.0]\n\nmodel = CatBoostRegressor(iterations=5)\nmach = machine(model, X, y)\nfit!(mach)\npreds = predict(mach, X)\n```\n\nSee also [catboost](https://github.com/catboost/catboost) and the unwrapped model type [`CatBoost.CatBoostRegressor`](@ref).\n"
":name" = "CatBoostRegressor"
":human_name" = "CatBoost regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":reformat", ":selectrows", ":update", ":feature_importances"]
":hyperparameters" = "`(:iterations, :learning_rate, :depth, :l2_leaf_reg, :model_size_reg, :rsm, :loss_function, :border_count, :feature_border_type, :per_float_feature_quantization, :input_borders, :output_borders, :fold_permutation_block, :nan_mode, :counter_calc_method, :leaf_estimation_iterations, :leaf_estimation_method, :thread_count, :random_seed, :metric_period, :ctr_leaf_count_limit, :store_all_simple_ctr, :max_ctr_complexity, :has_time, :allow_const_label, :target_border, :one_hot_max_size, :random_strength, :custom_metric, :bagging_temperature, :fold_len_multiplier, :used_ram_limit, :gpu_ram_part, :pinned_memory_size, :allow_writing_files, :approx_on_full_history, :boosting_type, :simple_ctr, :combinations_ctr, :per_feature_ctr, :ctr_target_border_count, :task_type, :devices, :bootstrap_type, :subsample, :sampling_frequency, :sampling_unit, :gpu_cat_features_storage, :data_partition, :early_stopping_rounds, :grow_policy, :min_data_in_leaf, :max_leaves, :leaf_estimation_backtracking, :feature_weights, :penalties_coefficient, :model_shrink_rate, :model_shrink_mode, :langevin, :diffusion_temperature, :posterior_sampling, :boost_from_average, :text_processing)`"
":hyperparameter_types" = "`(\"Int64\", \"Float64\", \"Int64\", \"Float64\", \"Float64\", \"Float64\", \"String\", \"Union{Nothing, Int64}\", \"Union{Nothing, String}\", \"Union{Nothing, PythonCall.Py}\", \"Union{Nothing, String}\", \"Union{Nothing, String}\", \"Int64\", \"String\", \"String\", \"Union{Nothing, Int64}\", \"Union{Nothing, String}\", \"Int64\", \"Union{Nothing, Int64}\", \"Int64\", \"Union{Nothing, Int64}\", \"Bool\", \"Union{Nothing, Bool}\", \"Bool\", \"Bool\", \"Union{Nothing, Float64}\", \"Union{Nothing, Int64}\", \"Float64\", \"Union{Nothing, String, PythonCall.Py}\", \"Float64\", \"Float64\", \"Union{Nothing, Int64}\", \"Float64\", \"Int64\", \"Union{Nothing, Bool}\", \"Bool\", \"Union{Nothing, String}\", \"Union{Nothing, PythonCall.Py}\", \"Union{Nothing, PythonCall.Py}\", \"Union{Nothing, PythonCall.Py}\", \"Union{Nothing, Int64}\", \"Union{Nothing, String}\", \"Union{Nothing, String}\", \"Union{Nothing, String}\", \"Union{Nothing, Int64}\", \"String\", \"String\", \"String\", \"Union{Nothing, String}\", \"Union{Nothing, Int64}\", \"String\", \"Int64\", \"Int64\", \"String\", \"Union{Nothing, PythonCall.Py}\", \"Float64\", \"Union{Nothing, Float64}\", \"String\", \"Bool\", \"Float64\", \"Bool\", \"Union{Nothing, Bool}\", \"Union{Nothing, PythonCall.Py}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = ":iterations"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`true`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[CatBoost.CatBoostClassifier]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}, AbstractVector{<:ScientificTypesBase.Multiclass}}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}, AbstractVector{<:ScientificTypesBase.Multiclass}}}, AbstractMatrix{ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "CatBoost"
":package_license" = "MIT"
":load_path" = "CatBoost.MLJCatBoostInterface.CatBoostClassifier"
":package_uuid" = "e2e10f9a-a85d-4fa9-b6b2-639a32100a12"
":package_url" = "https://github.com/JuliaAI/CatBoost.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nCatBoostClassifier\n```\n\nA model type for constructing a CatBoost classifier, based on [CatBoost.jl](https://github.com/JuliaAI/CatBoost.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nCatBoostClassifier = @load CatBoostClassifier pkg=CatBoost\n```\n\nDo `model = CatBoostClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `CatBoostClassifier(iterations=...)`.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have one of the following element scitypes: `Continuous`, `Count`, `Finite`, `Textual`; check column scitypes with `schema(X)`. `Textual` columns will be passed to catboost as `text_features`, `Multiclass` columns will be passed to catboost as `cat_features`, and `OrderedFactor` columns will be converted to integers.\n  * `y`: the target, which can be any `AbstractVector` whose element scitype is `Finite`; check the scitype with `scitype(y)`\n\nTrain the machine with `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\nMore details on the catboost hyperparameters, here are the Python docs:  https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier#parameters\n\n# Operations\n\n  * `predict(mach, Xnew)`: probabilistic predictions of the target given new features `Xnew` having the same scitype as `X` above.\n  * `predict_mode(mach, Xnew)`: returns the mode of each of the prediction above.\n\n# Accessor functions\n\n  * `feature_importances(mach)`: return vector of feature importances, in the form of   `feature::Symbol => importance::Real` pairs\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `model`: The Python CatBoostClassifier model\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `feature_importances`: Vector{Pair{Symbol, Float64}} of feature importances\n\n# Examples\n\n```\nusing CatBoost.MLJCatBoostInterface\nusing MLJ\n\nX = (\n    duration = [1.5, 4.1, 5.0, 6.7], \n    n_phone_calls = [4, 5, 6, 7], \n    department = coerce([\"acc\", \"ops\", \"acc\", \"ops\"], Multiclass), \n)\ny = coerce([0, 0, 1, 1], Multiclass)\n\nmodel = CatBoostClassifier(iterations=5)\nmach = machine(model, X, y)\nfit!(mach)\nprobs = predict(mach, X)\npreds = predict_mode(mach, X)\n```\n\nSee also [catboost](https://github.com/catboost/catboost) and the unwrapped model type [`CatBoost.CatBoostClassifier`](@ref).\n"
":name" = "CatBoostClassifier"
":human_name" = "CatBoost classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":predict_mode", ":reformat", ":selectrows", ":update", ":feature_importances"]
":hyperparameters" = "`(:iterations, :learning_rate, :depth, :l2_leaf_reg, :model_size_reg, :rsm, :loss_function, :border_count, :feature_border_type, :per_float_feature_quantization, :input_borders, :output_borders, :fold_permutation_block, :nan_mode, :counter_calc_method, :leaf_estimation_iterations, :leaf_estimation_method, :thread_count, :random_seed, :metric_period, :ctr_leaf_count_limit, :store_all_simple_ctr, :max_ctr_complexity, :has_time, :allow_const_label, :target_border, :class_weights, :auto_class_weights, :one_hot_max_size, :random_strength, :bagging_temperature, :fold_len_multiplier, :used_ram_limit, :gpu_ram_part, :pinned_memory_size, :allow_writing_files, :approx_on_full_history, :boosting_type, :simple_ctr, :combinations_ctr, :per_feature_ctr, :task_type, :devices, :bootstrap_type, :subsample, :sampling_frequency, :sampling_unit, :gpu_cat_features_storage, :data_partition, :early_stopping_rounds, :grow_policy, :min_data_in_leaf, :max_leaves, :leaf_estimation_backtracking, :feature_weights, :penalties_coefficient, :model_shrink_rate, :model_shrink_mode, :langevin, :diffusion_temperature, :posterior_sampling, :boost_from_average, :text_processing)`"
":hyperparameter_types" = "`(\"Int64\", \"Float64\", \"Int64\", \"Float64\", \"Float64\", \"Float64\", \"Union{Nothing, String}\", \"Union{Nothing, Int64}\", \"Union{Nothing, String}\", \"Union{Nothing, PythonCall.Py}\", \"Union{Nothing, String}\", \"Union{Nothing, String}\", \"Int64\", \"String\", \"String\", \"Union{Nothing, Int64}\", \"Union{Nothing, String}\", \"Int64\", \"Union{Nothing, Int64}\", \"Int64\", \"Union{Nothing, Int64}\", \"Bool\", \"Union{Nothing, Bool}\", \"Bool\", \"Bool\", \"Union{Nothing, Float64}\", \"Union{Nothing, PythonCall.Py}\", \"Union{Nothing, Bool}\", \"Union{Nothing, Int64}\", \"Float64\", \"Float64\", \"Float64\", \"Union{Nothing, Int64}\", \"Float64\", \"Int64\", \"Union{Nothing, Bool}\", \"Bool\", \"Union{Nothing, String}\", \"Union{Nothing, PythonCall.Py}\", \"Union{Nothing, PythonCall.Py}\", \"Union{Nothing, PythonCall.Py}\", \"Union{Nothing, String}\", \"Union{Nothing, String}\", \"Union{Nothing, String}\", \"Union{Nothing, Int64}\", \"String\", \"String\", \"String\", \"Union{Nothing, String}\", \"Union{Nothing, Int64}\", \"String\", \"Int64\", \"Int64\", \"String\", \"Union{Nothing, PythonCall.Py}\", \"Float64\", \"Union{Nothing, Float64}\", \"String\", \"Bool\", \"Float64\", \"Bool\", \"Union{Nothing, Bool}\", \"Union{Nothing, PythonCall.Py}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = ":iterations"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`true`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[NearestNeighborModels.KNNClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "NearestNeighborModels"
":package_license" = "MIT"
":load_path" = "NearestNeighborModels.KNNClassifier"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaAI/NearestNeighborModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nKNNClassifier\n```\n\nA model type for constructing a K-nearest neighbor classifier, based on [NearestNeighborModels.jl](https://github.com/JuliaAI/NearestNeighborModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nKNNClassifier = @load KNNClassifier pkg=NearestNeighborModels\n```\n\nDo `model = KNNClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `KNNClassifier(K=...)`.\n\nKNNClassifier implements [K-Nearest Neighbors classifier](https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm)  which is non-parametric algorithm that predicts a discrete class distribution associated  with a new point by taking a vote over the classes of the k-nearest points. Each neighbor  vote is assigned a weight based on proximity of the neighbor point to the test point  according to a specified distance metric.\n\nFor more information about the weighting kernels, see the paper by Geler et.al  [Comparison of different weighting schemes for the kNN classifier on time-series data](https://perun.pmf.uns.ac.rs/radovanovic/publications/2016-kais-knn-weighting.pdf). \n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nOR\n\n```\nmach = machine(model, X, y, w)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `<:Finite` (`<:Multiclass` or `<:OrderedFactor` will do); check the scitype with `scitype(y)`\n  * `w` is the observation weights which can either be `nothing` (default) or an  `AbstractVector` whose element scitype is `Count` or `Continuous`. This is  different from `weights` kernel which is a model hyperparameter, see below.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `K::Int=5` : number of neighbors\n  * `algorithm::Symbol = :kdtree` : one of `(:kdtree, :brutetree, :balltree)`\n  * `metric::Metric = Euclidean()` : any `Metric` from    [Distances.jl](https://github.com/JuliaStats/Distances.jl) for the    distance between points. For `algorithm = :kdtree` only metrics which are    instances of `Union{Distances.Chebyshev, Distances.Cityblock, Distances.Euclidean, Distances.Minkowski, Distances.WeightedCityblock, Distances.WeightedEuclidean, Distances.WeightedMinkowski}` are supported.\n  * `leafsize::Int = algorithm == 10` : determines the number of points    at which to stop splitting the tree. This option is ignored and always taken as `0`    for `algorithm = :brutetree`, since `brutetree` isn't actually a tree.\n  * `reorder::Bool = true` : if `true` then points which are close in    distance are placed close in memory. In this case, a copy of the original data    will be made so that the original data is left unmodified. Setting this to `true`    can significantly improve performance of the specified `algorithm`    (except `:brutetree`). This option is ignored and always taken as `false` for    `algorithm = :brutetree`.\n  * `weights::KNNKernel=Uniform()` : kernel used in assigning weights to the    k-nearest neighbors for each observation. An instance of one of the types in    `list_kernels()`. User-defined weighting functions can be passed by wrapping the    function in a [`UserDefinedKernel`](@ref) kernel (do `?NearestNeighborModels.UserDefinedKernel` for more    info). If observation weights `w` are passed during machine construction then the    weight assigned to each neighbor vote is the product of the kernel generated    weight for that neighbor and the corresponding observation weight.\n\n# Operations\n\n  * `predict(mach, Xnew)`: Return predictions of the target given features `Xnew`, which should have same scitype as `X` above. Predictions are probabilistic but uncalibrated.\n  * `predict_mode(mach, Xnew)`: Return the modes of the probabilistic predictions returned above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `tree`: An instance of either `KDTree`, `BruteTree` or `BallTree` depending on the  value of the `algorithm` hyperparameter (See hyper-parameters section above).  These are data structures that stores the training data with the view of making  quicker nearest neighbor searches on test data points.\n\n# Examples\n\n```\nusing MLJ\nKNNClassifier = @load KNNClassifier pkg=NearestNeighborModels\nX, y = @load_crabs; # a table and a vector from the crabs dataset\n# view possible kernels\nNearestNeighborModels.list_kernels()\n# KNNClassifier instantiation\nmodel = KNNClassifier(weights = NearestNeighborModels.Inverse())\nmach = machine(model, X, y) |> fit! # wrap model and required data in an MLJ machine and fit\ny_hat = predict(mach, X)\nlabels = predict_mode(mach, X)\n\n```\n\nSee also [`MultitargetKNNClassifier`](@ref)\n"
":name" = "KNNClassifier"
":human_name" = "K-nearest neighbor classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:K, :algorithm, :metric, :leafsize, :reorder, :weights)`"
":hyperparameter_types" = "`(\"Int64\", \"Symbol\", \"Distances.Metric\", \"Int64\", \"Bool\", \"NearestNeighborModels.KNNKernel\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[NearestNeighborModels.MultitargetKNNClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Finite}}`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Finite}}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Finite}}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Density{ScientificTypesBase.Finite}}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "NearestNeighborModels"
":package_license" = "MIT"
":load_path" = "NearestNeighborModels.MultitargetKNNClassifier"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaAI/NearestNeighborModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nMultitargetKNNClassifier\n```\n\nA model type for constructing a multitarget K-nearest neighbor classifier, based on [NearestNeighborModels.jl](https://github.com/JuliaAI/NearestNeighborModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nMultitargetKNNClassifier = @load MultitargetKNNClassifier pkg=NearestNeighborModels\n```\n\nDo `model = MultitargetKNNClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `MultitargetKNNClassifier(K=...)`.\n\nMulti-target K-Nearest Neighbors Classifier (MultitargetKNNClassifier) is a variation of  [`KNNClassifier`](@ref) that assumes the target variable is vector-valued with `Multiclass` or `OrderedFactor` components. (Target data must be presented as a table, however.)\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nOR\n\n```\nmach = machine(model, X, y, w)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n  * y`is the target, which can be any table of responses whose element scitype is either`<:Finite`(`<:Multiclass`or`<:OrderedFactor`will do); check the columns scitypes with`schema(y)`.  Each column of`y` is assumed to belong to a common categorical pool.\n  * `w` is the observation weights which can either be `nothing`(default) or an  `AbstractVector` whose element scitype is `Count` or `Continuous`. This is different  from `weights` kernel which is a model hyperparameter, see below.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `K::Int=5` : number of neighbors\n  * `algorithm::Symbol = :kdtree` : one of `(:kdtree, :brutetree, :balltree)`\n  * `metric::Metric = Euclidean()` : any `Metric` from    [Distances.jl](https://github.com/JuliaStats/Distances.jl) for the    distance between points. For `algorithm = :kdtree` only metrics which are    instances of `Union{Distances.Chebyshev, Distances.Cityblock, Distances.Euclidean, Distances.Minkowski, Distances.WeightedCityblock, Distances.WeightedEuclidean, Distances.WeightedMinkowski}` are supported.\n  * `leafsize::Int = algorithm == 10` : determines the number of points    at which to stop splitting the tree. This option is ignored and always taken as `0`    for `algorithm = :brutetree`, since `brutetree` isn't actually a tree.\n  * `reorder::Bool = true` : if `true` then points which are close in    distance are placed close in memory. In this case, a copy of the original data    will be made so that the original data is left unmodified. Setting this to `true`    can significantly improve performance of the specified `algorithm`    (except `:brutetree`). This option is ignored and always taken as `false` for    `algorithm = :brutetree`.\n  * `weights::KNNKernel=Uniform()` : kernel used in assigning weights to the    k-nearest neighbors for each observation. An instance of one of the types in    `list_kernels()`. User-defined weighting functions can be passed by wrapping the    function in a [`UserDefinedKernel`](@ref) kernel (do `?NearestNeighborModels.UserDefinedKernel` for more    info). If observation weights `w` are passed during machine construction then the    weight assigned to each neighbor vote is the product of the kernel generated    weight for that neighbor and the corresponding observation weight.\n\n  * `output_type::Type{<:MultiUnivariateFinite}=DictTable` : One of    (`ColumnTable`, `DictTable`). The type of table type to use for predictions.   Setting to `ColumnTable` might improve performance for narrow tables while setting to    `DictTable` improves performance for wide tables.\n\n# Operations\n\n  * `predict(mach, Xnew)`: Return predictions of the target given features `Xnew`, which should have same scitype as `X` above. Predictions are either a `ColumnTable` or  `DictTable` of `UnivariateFiniteVector` columns depending on the value set for the  `output_type` parameter discussed above. The probabilistic predictions are uncalibrated.\n  * `predict_mode(mach, Xnew)`: Return the modes of each column of the table of probabilistic  predictions returned above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `tree`: An instance of either `KDTree`, `BruteTree` or `BallTree` depending on the  value of the `algorithm` hyperparameter (See hyper-parameters section above).  These are data structures that stores the training data with the view of making  quicker nearest neighbor searches on test data points.\n\n# Examples\n\n```\nusing MLJ, StableRNGs\n\n# set rng for reproducibility\nrng = StableRNG(10)\n\n# Dataset generation\nn, p = 10, 3\nX = table(randn(rng, n, p)) # feature table\nfruit, color = categorical([\"apple\", \"orange\"]), categorical([\"blue\", \"green\"])\ny = [(fruit = rand(rng, fruit), color = rand(rng, color)) for _ in 1:n] # target_table\n# Each column in y has a common categorical pool as expected\nselectcols(y, :fruit) # categorical array\nselectcols(y, :color) # categorical array\n\n# Load MultitargetKNNClassifier\nMultitargetKNNClassifier = @load MultitargetKNNClassifier pkg=NearestNeighborModels\n\n# view possible kernels\nNearestNeighborModels.list_kernels()\n\n# MultitargetKNNClassifier instantiation\nmodel = MultitargetKNNClassifier(K=3, weights = NearestNeighborModels.Inverse())\n\n# wrap model and required data in an MLJ machine and fit\nmach = machine(model, X, y) |> fit!\n\n# predict\ny_hat = predict(mach, X)\nlabels = predict_mode(mach, X)\n\n```\n\nSee also [`KNNClassifier`](@ref)\n"
":name" = "MultitargetKNNClassifier"
":human_name" = "multitarget K-nearest neighbor classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":predict_mode"]
":hyperparameters" = "`(:K, :algorithm, :metric, :leafsize, :reorder, :weights, :output_type)`"
":hyperparameter_types" = "`(\"Int64\", \"Symbol\", \"Distances.Metric\", \"Int64\", \"Bool\", \"NearestNeighborModels.KNNKernel\", \"Type{<:Union{NamedTuple{names, T} where {N, names, T<:Tuple{Vararg{AbstractVector, N}}}, Union{var\\\"_s3\\\", var\\\"_s4\\\"} where {var\\\"_s3\\\"<:(AbstractDict{<:AbstractString, <:AbstractVector}), var\\\"_s4\\\"<:(AbstractDict{Symbol, <:AbstractVector})}}}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[NearestNeighborModels.MultitargetKNNRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "NearestNeighborModels"
":package_license" = "MIT"
":load_path" = "NearestNeighborModels.MultitargetKNNRegressor"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaAI/NearestNeighborModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nMultitargetKNNRegressor\n```\n\nA model type for constructing a multitarget K-nearest neighbor regressor, based on [NearestNeighborModels.jl](https://github.com/JuliaAI/NearestNeighborModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nMultitargetKNNRegressor = @load MultitargetKNNRegressor pkg=NearestNeighborModels\n```\n\nDo `model = MultitargetKNNRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `MultitargetKNNRegressor(K=...)`.\n\nMulti-target K-Nearest Neighbors regressor (MultitargetKNNRegressor) is a variation of  [`KNNRegressor`](@ref) that assumes the target variable is vector-valued with `Continuous` components. (Target data must be presented as a table, however.)\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nOR\n\n```\nmach = machine(model, X, y, w)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n  * `y` is the target, which can be any table of responses whose element scitype is  `Continuous`; check column scitypes with `schema(y)`.\n  * `w` is the observation weights which can either be `nothing`(default) or an  `AbstractVector` whoose element scitype is `Count` or `Continuous`. This is different  from `weights` kernel which is an hyperparameter to the model, see below.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `K::Int=5` : number of neighbors\n  * `algorithm::Symbol = :kdtree` : one of `(:kdtree, :brutetree, :balltree)`\n  * `metric::Metric = Euclidean()` : any `Metric` from    [Distances.jl](https://github.com/JuliaStats/Distances.jl) for the    distance between points. For `algorithm = :kdtree` only metrics which are    instances of `Union{Distances.Chebyshev, Distances.Cityblock, Distances.Euclidean, Distances.Minkowski, Distances.WeightedCityblock, Distances.WeightedEuclidean, Distances.WeightedMinkowski}` are supported.\n  * `leafsize::Int = algorithm == 10` : determines the number of points    at which to stop splitting the tree. This option is ignored and always taken as `0`    for `algorithm = :brutetree`, since `brutetree` isn't actually a tree.\n  * `reorder::Bool = true` : if `true` then points which are close in    distance are placed close in memory. In this case, a copy of the original data    will be made so that the original data is left unmodified. Setting this to `true`    can significantly improve performance of the specified `algorithm`    (except `:brutetree`). This option is ignored and always taken as `false` for    `algorithm = :brutetree`.\n  * `weights::KNNKernel=Uniform()` : kernel used in assigning weights to the    k-nearest neighbors for each observation. An instance of one of the types in    `list_kernels()`. User-defined weighting functions can be passed by wrapping the    function in a [`UserDefinedKernel`](@ref) kernel (do `?NearestNeighborModels.UserDefinedKernel` for more    info). If observation weights `w` are passed during machine construction then the    weight assigned to each neighbor vote is the product of the kernel generated    weight for that neighbor and the corresponding observation weight.\n\n# Operations\n\n  * `predict(mach, Xnew)`: Return predictions of the target given features `Xnew`, which should have same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `tree`: An instance of either `KDTree`, `BruteTree` or `BallTree` depending on the  value of the `algorithm` hyperparameter (See hyper-parameters section above).  These are data structures that stores the training data with the view of making  quicker nearest neighbor searches on test data points.\n\n# Examples\n\n```\nusing MLJ\n\n# Create Data\nX, y = make_regression(10, 5, n_targets=2)\n\n# load MultitargetKNNRegressor\nMultitargetKNNRegressor = @load MultitargetKNNRegressor pkg=NearestNeighborModels\n\n# view possible kernels\nNearestNeighborModels.list_kernels()\n\n# MutlitargetKNNRegressor instantiation\nmodel = MultitargetKNNRegressor(weights = NearestNeighborModels.Inverse())\n\n# Wrap model and required data in an MLJ machine and fit.\nmach = machine(model, X, y) |> fit! \n\n# Predict\ny_hat = predict(mach, X)\n\n```\n\nSee also [`KNNRegressor`](@ref)\n"
":name" = "MultitargetKNNRegressor"
":human_name" = "multitarget K-nearest neighbor regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:K, :algorithm, :metric, :leafsize, :reorder, :weights)`"
":hyperparameter_types" = "`(\"Int64\", \"Symbol\", \"Distances.Metric\", \"Int64\", \"Bool\", \"NearestNeighborModels.KNNKernel\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[NearestNeighborModels.KNNRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "NearestNeighborModels"
":package_license" = "MIT"
":load_path" = "NearestNeighborModels.KNNRegressor"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaAI/NearestNeighborModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nKNNRegressor\n```\n\nA model type for constructing a K-nearest neighbor regressor, based on [NearestNeighborModels.jl](https://github.com/JuliaAI/NearestNeighborModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nKNNRegressor = @load KNNRegressor pkg=NearestNeighborModels\n```\n\nDo `model = KNNRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `KNNRegressor(K=...)`.\n\nKNNRegressor implements [K-Nearest Neighbors regressor](https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm)  which is non-parametric algorithm that predicts the response associated with a new point  by taking an weighted average of the response of the K-nearest points.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nOR\n\n```\nmach = machine(model, X, y, w)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n  * `y` is the target, which can be any table of responses whose element scitype is    `Continuous`; check the scitype with `scitype(y)`.\n  * `w` is the observation weights which can either be `nothing`(default) or an  `AbstractVector` whoose element scitype is `Count` or `Continuous`. This is different  from `weights` kernel which is an hyperparameter to the model, see below.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `K::Int=5` : number of neighbors\n  * `algorithm::Symbol = :kdtree` : one of `(:kdtree, :brutetree, :balltree)`\n  * `metric::Metric = Euclidean()` : any `Metric` from    [Distances.jl](https://github.com/JuliaStats/Distances.jl) for the    distance between points. For `algorithm = :kdtree` only metrics which are    instances of `Union{Distances.Chebyshev, Distances.Cityblock, Distances.Euclidean, Distances.Minkowski, Distances.WeightedCityblock, Distances.WeightedEuclidean, Distances.WeightedMinkowski}` are supported.\n  * `leafsize::Int = algorithm == 10` : determines the number of points    at which to stop splitting the tree. This option is ignored and always taken as `0`    for `algorithm = :brutetree`, since `brutetree` isn't actually a tree.\n  * `reorder::Bool = true` : if `true` then points which are close in    distance are placed close in memory. In this case, a copy of the original data    will be made so that the original data is left unmodified. Setting this to `true`    can significantly improve performance of the specified `algorithm`    (except `:brutetree`). This option is ignored and always taken as `false` for    `algorithm = :brutetree`.\n  * `weights::KNNKernel=Uniform()` : kernel used in assigning weights to the    k-nearest neighbors for each observation. An instance of one of the types in    `list_kernels()`. User-defined weighting functions can be passed by wrapping the    function in a [`UserDefinedKernel`](@ref) kernel (do `?NearestNeighborModels.UserDefinedKernel` for more    info). If observation weights `w` are passed during machine construction then the    weight assigned to each neighbor vote is the product of the kernel generated    weight for that neighbor and the corresponding observation weight.\n\n# Operations\n\n  * `predict(mach, Xnew)`: Return predictions of the target given features `Xnew`, which should have same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `tree`: An instance of either `KDTree`, `BruteTree` or `BallTree` depending on the  value of the `algorithm` hyperparameter (See hyper-parameters section above).  These are data structures that stores the training data with the view of making  quicker nearest neighbor searches on test data points.\n\n# Examples\n\n```\nusing MLJ\nKNNRegressor = @load KNNRegressor pkg=NearestNeighborModels\nX, y = @load_boston; # loads the crabs dataset from MLJBase\n# view possible kernels\nNearestNeighborModels.list_kernels()\nmodel = KNNRegressor(weights = NearestNeighborModels.Inverse()) #KNNRegressor instantiation\nmach = machine(model, X, y) |> fit! # wrap model and required data in an MLJ machine and fit\ny_hat = predict(mach, X)\n\n```\n\nSee also [`MultitargetKNNRegressor`](@ref)\n"
":name" = "KNNRegressor"
":human_name" = "K-nearest neighbor regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:K, :algorithm, :metric, :leafsize, :reorder, :weights)`"
":hyperparameter_types" = "`(\"Int64\", \"Symbol\", \"Distances.Metric\", \"Int64\", \"Bool\", \"NearestNeighborModels.KNNKernel\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.ProbabilisticSGDClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.ProbabilisticSGDClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nProbabilisticSGDClassifier\n```\n\nA model type for constructing a probabilistic sgd classifier, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nProbabilisticSGDClassifier = @load ProbabilisticSGDClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = ProbabilisticSGDClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`ProbabilisticSGDClassifier(loss=...)`.\n# Hyper-parameters\n\n- `loss = log_loss`\n\n- `penalty = l2`\n\n- `alpha = 0.0001`\n\n- `l1_ratio = 0.15`\n\n- `fit_intercept = true`\n\n- `max_iter = 1000`\n\n- `tol = 0.001`\n\n- `shuffle = true`\n\n- `verbose = 0`\n\n- `epsilon = 0.1`\n\n- `n_jobs = nothing`\n\n- `random_state = nothing`\n\n- `learning_rate = optimal`\n\n- `eta0 = 0.0`\n\n- `power_t = 0.5`\n\n- `early_stopping = false`\n\n- `validation_fraction = 0.1`\n\n- `n_iter_no_change = 5`\n\n- `class_weight = nothing`\n\n- `warm_start = false`\n\n- `average = false`\n\n"
":name" = "ProbabilisticSGDClassifier"
":human_name" = "probabilistic sgd classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:loss, :penalty, :alpha, :l1_ratio, :fit_intercept, :max_iter, :tol, :shuffle, :verbose, :epsilon, :n_jobs, :random_state, :learning_rate, :eta0, :power_t, :early_stopping, :validation_fraction, :n_iter_no_change, :class_weight, :warm_start, :average)`"
":hyperparameter_types" = "`(\"String\", \"String\", \"Float64\", \"Float64\", \"Bool\", \"Int64\", \"Union{Nothing, Float64}\", \"Bool\", \"Int64\", \"Float64\", \"Union{Nothing, Int64}\", \"Any\", \"String\", \"Float64\", \"Float64\", \"Bool\", \"Float64\", \"Int64\", \"Any\", \"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.RidgeCVClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.RidgeCVClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nRidgeCVClassifier\n```\n\nA model type for constructing a ridge regression classifier with built-in cross-validation, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nRidgeCVClassifier = @load RidgeCVClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = RidgeCVClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`RidgeCVClassifier(alphas=...)`.\n# Hyper-parameters\n\n- `alphas = [0.1, 1.0, 10.0]`\n\n- `fit_intercept = true`\n\n- `scoring = nothing`\n\n- `cv = 5`\n\n- `class_weight = nothing`\n\n- `store_cv_values = false`\n\n"
":name" = "RidgeCVClassifier"
":human_name" = "ridge regression classifier with built-in cross-validation"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:alphas, :fit_intercept, :scoring, :cv, :class_weight, :store_cv_values)`"
":hyperparameter_types" = "`(\"AbstractArray{Float64}\", \"Bool\", \"Any\", \"Int64\", \"Any\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.LogisticClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.LogisticClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLogisticClassifier\n```\n\nA model type for constructing a logistic regression classifier, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nLogisticClassifier = @load LogisticClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = LogisticClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LogisticClassifier(penalty=...)`.\n# Hyper-parameters\n\n- `penalty = l2`\n\n- `dual = false`\n\n- `tol = 0.0001`\n\n- `C = 1.0`\n\n- `fit_intercept = true`\n\n- `intercept_scaling = 1.0`\n\n- `class_weight = nothing`\n\n- `random_state = nothing`\n\n- `solver = lbfgs`\n\n- `max_iter = 100`\n\n- `multi_class = auto`\n\n- `verbose = 0`\n\n- `warm_start = false`\n\n- `n_jobs = nothing`\n\n- `l1_ratio = nothing`\n\n"
":name" = "LogisticClassifier"
":human_name" = "logistic regression classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:penalty, :dual, :tol, :C, :fit_intercept, :intercept_scaling, :class_weight, :random_state, :solver, :max_iter, :multi_class, :verbose, :warm_start, :n_jobs, :l1_ratio)`"
":hyperparameter_types" = "`(\"String\", \"Bool\", \"Float64\", \"Float64\", \"Bool\", \"Float64\", \"Any\", \"Any\", \"String\", \"Int64\", \"String\", \"Int64\", \"Bool\", \"Union{Nothing, Int64}\", \"Union{Nothing, Float64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.RandomForestRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.Continuous}}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.RandomForestRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nRandomForestRegressor\n```\n\nA model type for constructing a random forest regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nRandomForestRegressor = @load RandomForestRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = RandomForestRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`RandomForestRegressor(n_estimators=...)`.\n# Hyper-parameters\n\n- `n_estimators = 100`\n\n- `criterion = squared_error`\n\n- `max_depth = nothing`\n\n- `min_samples_split = 2`\n\n- `min_samples_leaf = 1`\n\n- `min_weight_fraction_leaf = 0.0`\n\n- `max_features = 1.0`\n\n- `max_leaf_nodes = nothing`\n\n- `min_impurity_decrease = 0.0`\n\n- `bootstrap = true`\n\n- `oob_score = false`\n\n- `n_jobs = nothing`\n\n- `random_state = nothing`\n\n- `verbose = 0`\n\n- `warm_start = false`\n\n- `ccp_alpha = 0.0`\n\n- `max_samples = nothing`\n\n"
":name" = "RandomForestRegressor"
":human_name" = "random forest regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:n_estimators, :criterion, :max_depth, :min_samples_split, :min_samples_leaf, :min_weight_fraction_leaf, :max_features, :max_leaf_nodes, :min_impurity_decrease, :bootstrap, :oob_score, :n_jobs, :random_state, :verbose, :warm_start, :ccp_alpha, :max_samples)`"
":hyperparameter_types" = "`(\"Int64\", \"String\", \"Union{Nothing, Int64}\", \"Union{Float64, Int64}\", \"Union{Float64, Int64}\", \"Float64\", \"Union{Nothing, Float64, Int64, String}\", \"Union{Nothing, Int64}\", \"Float64\", \"Bool\", \"Bool\", \"Union{Nothing, Int64}\", \"Any\", \"Int64\", \"Bool\", \"Float64\", \"Union{Nothing, Float64, Int64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.ElasticNetCVRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.ElasticNetCVRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nElasticNetCVRegressor\n```\n\nA model type for constructing a elastic net regression with built-in cross-validation, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nElasticNetCVRegressor = @load ElasticNetCVRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = ElasticNetCVRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`ElasticNetCVRegressor(l1_ratio=...)`.\n# Hyper-parameters\n\n- `l1_ratio = 0.5`\n\n- `eps = 0.001`\n\n- `n_alphas = 100`\n\n- `alphas = nothing`\n\n- `fit_intercept = true`\n\n- `precompute = auto`\n\n- `max_iter = 1000`\n\n- `tol = 0.0001`\n\n- `cv = 5`\n\n- `copy_X = true`\n\n- `verbose = 0`\n\n- `n_jobs = nothing`\n\n- `positive = false`\n\n- `random_state = nothing`\n\n- `selection = cyclic`\n\n"
":name" = "ElasticNetCVRegressor"
":human_name" = "elastic net regression with built-in cross-validation"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:l1_ratio, :eps, :n_alphas, :alphas, :fit_intercept, :precompute, :max_iter, :tol, :cv, :copy_X, :verbose, :n_jobs, :positive, :random_state, :selection)`"
":hyperparameter_types" = "`(\"Union{Float64, Vector{Float64}}\", \"Float64\", \"Int64\", \"Any\", \"Bool\", \"Union{Bool, String, AbstractMatrix}\", \"Int64\", \"Float64\", \"Any\", \"Bool\", \"Union{Bool, Int64}\", \"Union{Nothing, Int64}\", \"Bool\", \"Any\", \"String\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.PerceptronClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.PerceptronClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nPerceptronClassifier\n```\n\nA model type for constructing a perceptron classifier, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nPerceptronClassifier = @load PerceptronClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = PerceptronClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`PerceptronClassifier(penalty=...)`.\n# Hyper-parameters\n\n- `penalty = nothing`\n\n- `alpha = 0.0001`\n\n- `fit_intercept = true`\n\n- `max_iter = 1000`\n\n- `tol = 0.001`\n\n- `shuffle = true`\n\n- `verbose = 0`\n\n- `eta0 = 1.0`\n\n- `n_jobs = nothing`\n\n- `random_state = 0`\n\n- `early_stopping = false`\n\n- `validation_fraction = 0.1`\n\n- `n_iter_no_change = 5`\n\n- `class_weight = nothing`\n\n- `warm_start = false`\n\n"
":name" = "PerceptronClassifier"
":human_name" = "perceptron classifier"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:penalty, :alpha, :fit_intercept, :max_iter, :tol, :shuffle, :verbose, :eta0, :n_jobs, :random_state, :early_stopping, :validation_fraction, :n_iter_no_change, :class_weight, :warm_start)`"
":hyperparameter_types" = "`(\"Union{Nothing, String}\", \"Float64\", \"Bool\", \"Int64\", \"Union{Nothing, Float64}\", \"Bool\", \"Int64\", \"Float64\", \"Union{Nothing, Int64}\", \"Any\", \"Bool\", \"Float64\", \"Int64\", \"Any\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.MultiTaskLassoRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.MultiTaskLassoRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nMultiTaskLassoRegressor\n```\n\nA model type for constructing a multi-target lasso regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nMultiTaskLassoRegressor = @load MultiTaskLassoRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = MultiTaskLassoRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`MultiTaskLassoRegressor(alpha=...)`.\n# Hyper-parameters\n\n- `alpha = 1.0`\n\n- `fit_intercept = true`\n\n- `max_iter = 1000`\n\n- `tol = 0.0001`\n\n- `copy_X = true`\n\n- `random_state = nothing`\n\n- `selection = cyclic`\n\n"
":name" = "MultiTaskLassoRegressor"
":human_name" = "multi-target lasso regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:alpha, :fit_intercept, :max_iter, :tol, :copy_X, :random_state, :selection)`"
":hyperparameter_types" = "`(\"Float64\", \"Bool\", \"Int64\", \"Float64\", \"Bool\", \"Any\", \"String\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.LinearRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.LinearRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLinearRegressor\n```\n\nA model type for constructing a ordinary least-squares regressor (OLS), based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nLinearRegressor = @load LinearRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = LinearRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LinearRegressor(fit_intercept=...)`.\n# Hyper-parameters\n\n- `fit_intercept = true`\n\n- `copy_X = true`\n\n- `n_jobs = nothing`\n\n"
":name" = "LinearRegressor"
":human_name" = "ordinary least-squares regressor (OLS)"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:fit_intercept, :copy_X, :n_jobs)`"
":hyperparameter_types" = "`(\"Bool\", \"Bool\", \"Union{Nothing, Int64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.DBSCAN]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.DBSCAN"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nDBSCAN\n```\n\nA model type for constructing a dbscan, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nDBSCAN = @load DBSCAN pkg=MLJScikitLearnInterface\n```\n\nDo `model = DBSCAN()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `DBSCAN(eps=...)`.\n\nDensity-Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density.\n"
":name" = "DBSCAN"
":human_name" = "dbscan"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params"]
":hyperparameters" = "`(:eps, :min_samples, :metric, :algorithm, :leaf_size, :p, :n_jobs)`"
":hyperparameter_types" = "`(\"Float64\", \"Int64\", \"String\", \"String\", \"Int64\", \"Union{Nothing, Float64}\", \"Union{Nothing, Int64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.RidgeRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.RidgeRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nRidgeRegressor\n```\n\nA model type for constructing a ridge regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nRidgeRegressor = @load RidgeRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = RidgeRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`RidgeRegressor(alpha=...)`.\n# Hyper-parameters\n\n- `alpha = 1.0`\n\n- `fit_intercept = true`\n\n- `copy_X = true`\n\n- `max_iter = 1000`\n\n- `tol = 0.0001`\n\n- `solver = auto`\n\n- `random_state = nothing`\n\n"
":name" = "RidgeRegressor"
":human_name" = "ridge regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:alpha, :fit_intercept, :copy_X, :max_iter, :tol, :solver, :random_state)`"
":hyperparameter_types" = "`(\"Union{Float64, Vector{Float64}}\", \"Bool\", \"Bool\", \"Int64\", \"Float64\", \"String\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.LassoLarsICRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.LassoLarsICRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLassoLarsICRegressor\n```\n\nA model type for constructing a Lasso model with LARS using BIC or AIC for model selection, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nLassoLarsICRegressor = @load LassoLarsICRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = LassoLarsICRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LassoLarsICRegressor(criterion=...)`.\n# Hyper-parameters\n\n- `criterion = aic`\n\n- `fit_intercept = true`\n\n- `verbose = false`\n\n- `normalize = false`\n\n- `precompute = auto`\n\n- `max_iter = 500`\n\n- `eps = 2.220446049250313e-16`\n\n- `copy_X = true`\n\n- `positive = false`\n\n"
":name" = "LassoLarsICRegressor"
":human_name" = "Lasso model with LARS using BIC or AIC for model selection"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:criterion, :fit_intercept, :verbose, :normalize, :precompute, :max_iter, :eps, :copy_X, :positive)`"
":hyperparameter_types" = "`(\"String\", \"Bool\", \"Union{Bool, Int64}\", \"Bool\", \"Union{Bool, String, AbstractMatrix}\", \"Int64\", \"Float64\", \"Bool\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.ARDRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.ARDRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nARDRegressor\n```\n\nA model type for constructing a Bayesian ARD regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nARDRegressor = @load ARDRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = ARDRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`ARDRegressor(n_iter=...)`.\n# Hyper-parameters\n\n- `n_iter = 300`\n\n- `tol = 0.001`\n\n- `alpha_1 = 1.0e-6`\n\n- `alpha_2 = 1.0e-6`\n\n- `lambda_1 = 1.0e-6`\n\n- `lambda_2 = 1.0e-6`\n\n- `compute_score = false`\n\n- `threshold_lambda = 10000.0`\n\n- `fit_intercept = true`\n\n- `copy_X = true`\n\n- `verbose = false`\n\n"
":name" = "ARDRegressor"
":human_name" = "Bayesian ARD regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:n_iter, :tol, :alpha_1, :alpha_2, :lambda_1, :lambda_2, :compute_score, :threshold_lambda, :fit_intercept, :copy_X, :verbose)`"
":hyperparameter_types" = "`(\"Int64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Bool\", \"Float64\", \"Bool\", \"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.SVMNuRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.SVMNuRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nSVMNuRegressor\n```\n\nA model type for constructing a nu-support vector regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nSVMNuRegressor = @load SVMNuRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = SVMNuRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`SVMNuRegressor(nu=...)`.\n# Hyper-parameters\n\n- `nu = 0.5`\n\n- `C = 1.0`\n\n- `kernel = rbf`\n\n- `degree = 3`\n\n- `gamma = scale`\n\n- `coef0 = 0.0`\n\n- `shrinking = true`\n\n- `tol = 0.001`\n\n- `cache_size = 200`\n\n- `max_iter = -1`\n\n"
":name" = "SVMNuRegressor"
":human_name" = "nu-support vector regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:nu, :C, :kernel, :degree, :gamma, :coef0, :shrinking, :tol, :cache_size, :max_iter)`"
":hyperparameter_types" = "`(\"Float64\", \"Float64\", \"Union{Function, String}\", \"Int64\", \"Union{Float64, String}\", \"Float64\", \"Any\", \"Float64\", \"Int64\", \"Int64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.RidgeClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.RidgeClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nRidgeClassifier\n```\n\nA model type for constructing a ridge regression classifier, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nRidgeClassifier = @load RidgeClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = RidgeClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`RidgeClassifier(alpha=...)`.\n# Hyper-parameters\n\n- `alpha = 1.0`\n\n- `fit_intercept = true`\n\n- `copy_X = true`\n\n- `max_iter = nothing`\n\n- `tol = 0.001`\n\n- `class_weight = nothing`\n\n- `solver = auto`\n\n- `random_state = nothing`\n\n"
":name" = "RidgeClassifier"
":human_name" = "ridge regression classifier"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:alpha, :fit_intercept, :copy_X, :max_iter, :tol, :class_weight, :solver, :random_state)`"
":hyperparameter_types" = "`(\"Float64\", \"Bool\", \"Bool\", \"Union{Nothing, Int64}\", \"Float64\", \"Any\", \"String\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.SGDRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.SGDRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nSGDRegressor\n```\n\nA model type for constructing a stochastic gradient descent-based regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nSGDRegressor = @load SGDRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = SGDRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`SGDRegressor(loss=...)`.\n# Hyper-parameters\n\n- `loss = squared_error`\n\n- `penalty = l2`\n\n- `alpha = 0.0001`\n\n- `l1_ratio = 0.15`\n\n- `fit_intercept = true`\n\n- `max_iter = 1000`\n\n- `tol = 0.001`\n\n- `shuffle = true`\n\n- `verbose = 0`\n\n- `epsilon = 0.1`\n\n- `random_state = nothing`\n\n- `learning_rate = invscaling`\n\n- `eta0 = 0.01`\n\n- `power_t = 0.25`\n\n- `early_stopping = false`\n\n- `validation_fraction = 0.1`\n\n- `n_iter_no_change = 5`\n\n- `warm_start = false`\n\n- `average = false`\n\n"
":name" = "SGDRegressor"
":human_name" = "stochastic gradient descent-based regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:loss, :penalty, :alpha, :l1_ratio, :fit_intercept, :max_iter, :tol, :shuffle, :verbose, :epsilon, :random_state, :learning_rate, :eta0, :power_t, :early_stopping, :validation_fraction, :n_iter_no_change, :warm_start, :average)`"
":hyperparameter_types" = "`(\"String\", \"String\", \"Float64\", \"Float64\", \"Bool\", \"Int64\", \"Float64\", \"Bool\", \"Union{Bool, Int64}\", \"Float64\", \"Any\", \"String\", \"Float64\", \"Float64\", \"Bool\", \"Float64\", \"Int64\", \"Bool\", \"Union{Bool, Int64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.ComplementNBClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Count}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Count}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.ComplementNBClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nComplementNBClassifier\n```\n\nA model type for constructing a Complement naive Bayes classifier, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nComplementNBClassifier = @load ComplementNBClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = ComplementNBClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `ComplementNBClassifier(alpha=...)`.\n\nSimilar to [`MultinomialNBClassifier`](@ref) but with more robust assumptions. Suited for imbalanced datasets.\n"
":name" = "ComplementNBClassifier"
":human_name" = "Complement naive Bayes classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:alpha, :fit_prior, :class_prior, :norm)`"
":hyperparameter_types" = "`(\"Float64\", \"Bool\", \"Union{Nothing, AbstractVector}\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.HuberRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.HuberRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nHuberRegressor\n```\n\nA model type for constructing a Huber regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nHuberRegressor = @load HuberRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = HuberRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`HuberRegressor(epsilon=...)`.\n# Hyper-parameters\n\n- `epsilon = 1.35`\n\n- `max_iter = 100`\n\n- `alpha = 0.0001`\n\n- `warm_start = false`\n\n- `fit_intercept = true`\n\n- `tol = 1.0e-5`\n\n"
":name" = "HuberRegressor"
":human_name" = "Huber regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:epsilon, :max_iter, :alpha, :warm_start, :fit_intercept, :tol)`"
":hyperparameter_types" = "`(\"Float64\", \"Int64\", \"Float64\", \"Bool\", \"Bool\", \"Float64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.SVMNuClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.SVMNuClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nSVMNuClassifier\n```\n\nA model type for constructing a nu-support vector classifier, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nSVMNuClassifier = @load SVMNuClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = SVMNuClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`SVMNuClassifier(nu=...)`.\n# Hyper-parameters\n\n- `nu = 0.5`\n\n- `kernel = rbf`\n\n- `degree = 3`\n\n- `gamma = scale`\n\n- `coef0 = 0.0`\n\n- `shrinking = true`\n\n- `tol = 0.001`\n\n- `cache_size = 200`\n\n- `max_iter = -1`\n\n- `decision_function_shape = ovr`\n\n- `random_state = nothing`\n\n"
":name" = "SVMNuClassifier"
":human_name" = "nu-support vector classifier"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:nu, :kernel, :degree, :gamma, :coef0, :shrinking, :tol, :cache_size, :max_iter, :decision_function_shape, :random_state)`"
":hyperparameter_types" = "`(\"Float64\", \"Union{Function, String}\", \"Int64\", \"Union{Float64, String}\", \"Float64\", \"Bool\", \"Float64\", \"Int64\", \"Int64\", \"String\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.GradientBoostingClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.GradientBoostingClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nGradientBoostingClassifier\n```\n\nA model type for constructing a gradient boosting classifier, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nGradientBoostingClassifier = @load GradientBoostingClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = GradientBoostingClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`GradientBoostingClassifier(loss=...)`.\n# Hyper-parameters\n\n- `loss = log_loss`\n\n- `learning_rate = 0.1`\n\n- `n_estimators = 100`\n\n- `subsample = 1.0`\n\n- `criterion = friedman_mse`\n\n- `min_samples_split = 2`\n\n- `min_samples_leaf = 1`\n\n- `min_weight_fraction_leaf = 0.0`\n\n- `max_depth = 3`\n\n- `min_impurity_decrease = 0.0`\n\n- `init = nothing`\n\n- `random_state = nothing`\n\n- `max_features = nothing`\n\n- `verbose = 0`\n\n- `max_leaf_nodes = nothing`\n\n- `warm_start = false`\n\n- `validation_fraction = 0.1`\n\n- `n_iter_no_change = nothing`\n\n- `tol = 0.0001`\n\n"
":name" = "GradientBoostingClassifier"
":human_name" = "gradient boosting classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:loss, :learning_rate, :n_estimators, :subsample, :criterion, :min_samples_split, :min_samples_leaf, :min_weight_fraction_leaf, :max_depth, :min_impurity_decrease, :init, :random_state, :max_features, :verbose, :max_leaf_nodes, :warm_start, :validation_fraction, :n_iter_no_change, :tol)`"
":hyperparameter_types" = "`(\"String\", \"Float64\", \"Int64\", \"Float64\", \"String\", \"Union{Float64, Int64}\", \"Union{Float64, Int64}\", \"Float64\", \"Int64\", \"Float64\", \"Any\", \"Any\", \"Union{Nothing, Float64, Int64, String}\", \"Int64\", \"Union{Nothing, Int64}\", \"Bool\", \"Float64\", \"Union{Nothing, Int64}\", \"Float64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.GaussianProcessRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.GaussianProcessRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nGaussianProcessRegressor\n```\n\nA model type for constructing a Gaussian process regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nGaussianProcessRegressor = @load GaussianProcessRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = GaussianProcessRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`GaussianProcessRegressor(kernel=...)`.\n# Hyper-parameters\n\n- `kernel = nothing`\n\n- `alpha = 1.0e-10`\n\n- `optimizer = fmin_l_bfgs_b`\n\n- `n_restarts_optimizer = 0`\n\n- `normalize_y = false`\n\n- `copy_X_train = true`\n\n- `random_state = nothing`\n\n"
":name" = "GaussianProcessRegressor"
":human_name" = "Gaussian process regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:kernel, :alpha, :optimizer, :n_restarts_optimizer, :normalize_y, :copy_X_train, :random_state)`"
":hyperparameter_types" = "`(\"Any\", \"Union{Float64, AbstractArray}\", \"Any\", \"Int64\", \"Bool\", \"Bool\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.SVMLinearRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.SVMLinearRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nSVMLinearRegressor\n```\n\nA model type for constructing a linear support vector regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nSVMLinearRegressor = @load SVMLinearRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = SVMLinearRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`SVMLinearRegressor(epsilon=...)`.\n# Hyper-parameters\n\n- `epsilon = 0.0`\n\n- `tol = 0.0001`\n\n- `C = 1.0`\n\n- `loss = epsilon_insensitive`\n\n- `fit_intercept = true`\n\n- `intercept_scaling = 1.0`\n\n- `dual = true`\n\n- `random_state = nothing`\n\n- `max_iter = 1000`\n\n"
":name" = "SVMLinearRegressor"
":human_name" = "linear support vector regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:epsilon, :tol, :C, :loss, :fit_intercept, :intercept_scaling, :dual, :random_state, :max_iter)`"
":hyperparameter_types" = "`(\"Float64\", \"Float64\", \"Float64\", \"String\", \"Bool\", \"Float64\", \"Bool\", \"Any\", \"Int64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.LarsRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.LarsRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLarsRegressor\n```\n\nA model type for constructing a least angle regressor (LARS), based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nLarsRegressor = @load LarsRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = LarsRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LarsRegressor(fit_intercept=...)`.\n# Hyper-parameters\n\n- `fit_intercept = true`\n\n- `verbose = false`\n\n- `normalize = false`\n\n- `precompute = auto`\n\n- `n_nonzero_coefs = 500`\n\n- `eps = 2.220446049250313e-16`\n\n- `copy_X = true`\n\n- `fit_path = true`\n\n"
":name" = "LarsRegressor"
":human_name" = "least angle regressor (LARS)"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:fit_intercept, :verbose, :normalize, :precompute, :n_nonzero_coefs, :eps, :copy_X, :fit_path)`"
":hyperparameter_types" = "`(\"Bool\", \"Union{Bool, Int64}\", \"Bool\", \"Union{Bool, String, AbstractMatrix}\", \"Int64\", \"Float64\", \"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.MeanShift]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Multiclass}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.MeanShift"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nMeanShift\n```\n\nA model type for constructing a mean shift, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nMeanShift = @load MeanShift pkg=MLJScikitLearnInterface\n```\n\nDo `model = MeanShift()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `MeanShift(bandwidth=...)`.\n\nMean shift clustering using a flat kernel. Mean shift clustering aims to discover \"blobs\" in a smooth density of samples. It is a centroid-based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids.\"\n"
":name" = "MeanShift"
":human_name" = "mean shift"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:bandwidth, :seeds, :bin_seeding, :min_bin_freq, :cluster_all, :n_jobs)`"
":hyperparameter_types" = "`(\"Union{Nothing, Float64}\", \"Union{Nothing, AbstractArray}\", \"Bool\", \"Int64\", \"Bool\", \"Union{Nothing, Int64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.AdaBoostRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.AdaBoostRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nAdaBoostRegressor\n```\n\nA model type for constructing a AdaBoost ensemble regression, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nAdaBoostRegressor = @load AdaBoostRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = AdaBoostRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`AdaBoostRegressor(estimator=...)`.\n# Hyper-parameters\n\n- `estimator = nothing`\n\n- `n_estimators = 50`\n\n- `learning_rate = 1.0`\n\n- `loss = linear`\n\n- `random_state = nothing`\n\n"
":name" = "AdaBoostRegressor"
":human_name" = "AdaBoost ensemble regression"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:estimator, :n_estimators, :learning_rate, :loss, :random_state)`"
":hyperparameter_types" = "`(\"Any\", \"Int64\", \"Float64\", \"String\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.AffinityPropagation]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Multiclass}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.AffinityPropagation"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nAffinityPropagation\n```\n\nA model type for constructing a Affinity Propagation Clustering of data, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nAffinityPropagation = @load AffinityPropagation pkg=MLJScikitLearnInterface\n```\n\nDo `model = AffinityPropagation()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`AffinityPropagation(damping=...)`.\n# Hyper-parameters\n\n- `damping = 0.5`\n\n- `max_iter = 200`\n\n- `convergence_iter = 15`\n\n- `copy = true`\n\n- `preference = nothing`\n\n- `affinity = euclidean`\n\n- `verbose = false`\n\n"
":name" = "AffinityPropagation"
":human_name" = "Affinity Propagation Clustering of data"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:damping, :max_iter, :convergence_iter, :copy, :preference, :affinity, :verbose)`"
":hyperparameter_types" = "`(\"Float64\", \"Int64\", \"Int64\", \"Bool\", \"Any\", \"String\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.MultiTaskLassoCVRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.MultiTaskLassoCVRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nMultiTaskLassoCVRegressor\n```\n\nA model type for constructing a multi-target lasso regressor with built-in cross-validation, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nMultiTaskLassoCVRegressor = @load MultiTaskLassoCVRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = MultiTaskLassoCVRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`MultiTaskLassoCVRegressor(eps=...)`.\n# Hyper-parameters\n\n- `eps = 0.001`\n\n- `n_alphas = 100`\n\n- `alphas = nothing`\n\n- `fit_intercept = true`\n\n- `max_iter = 300`\n\n- `tol = 0.0001`\n\n- `copy_X = true`\n\n- `cv = 5`\n\n- `verbose = false`\n\n- `n_jobs = 1`\n\n- `random_state = nothing`\n\n- `selection = cyclic`\n\n"
":name" = "MultiTaskLassoCVRegressor"
":human_name" = "multi-target lasso regressor with built-in cross-validation"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:eps, :n_alphas, :alphas, :fit_intercept, :max_iter, :tol, :copy_X, :cv, :verbose, :n_jobs, :random_state, :selection)`"
":hyperparameter_types" = "`(\"Float64\", \"Int64\", \"Any\", \"Bool\", \"Int64\", \"Float64\", \"Bool\", \"Any\", \"Union{Bool, Int64}\", \"Union{Nothing, Int64}\", \"Any\", \"String\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.OrthogonalMatchingPursuitRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.OrthogonalMatchingPursuitRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nOrthogonalMatchingPursuitRegressor\n```\n\nA model type for constructing a orthogonal matching pursuit regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nOrthogonalMatchingPursuitRegressor = @load OrthogonalMatchingPursuitRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = OrthogonalMatchingPursuitRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`OrthogonalMatchingPursuitRegressor(n_nonzero_coefs=...)`.\n# Hyper-parameters\n\n- `n_nonzero_coefs = nothing`\n\n- `tol = nothing`\n\n- `fit_intercept = true`\n\n- `normalize = false`\n\n- `precompute = auto`\n\n"
":name" = "OrthogonalMatchingPursuitRegressor"
":human_name" = "orthogonal matching pursuit regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:n_nonzero_coefs, :tol, :fit_intercept, :normalize, :precompute)`"
":hyperparameter_types" = "`(\"Union{Nothing, Int64}\", \"Union{Nothing, Float64}\", \"Bool\", \"Bool\", \"Union{Bool, String, AbstractMatrix}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.RidgeCVRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.RidgeCVRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nRidgeCVRegressor\n```\n\nA model type for constructing a ridge regressor with built-in cross-validation, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nRidgeCVRegressor = @load RidgeCVRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = RidgeCVRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`RidgeCVRegressor(alphas=...)`.\n# Hyper-parameters\n\n- `alphas = (0.1, 1.0, 10.0)`\n\n- `fit_intercept = true`\n\n- `scoring = nothing`\n\n- `cv = 5`\n\n- `gcv_mode = nothing`\n\n- `store_cv_values = false`\n\n"
":name" = "RidgeCVRegressor"
":human_name" = "ridge regressor with built-in cross-validation"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:alphas, :fit_intercept, :scoring, :cv, :gcv_mode, :store_cv_values)`"
":hyperparameter_types" = "`(\"Any\", \"Bool\", \"Any\", \"Any\", \"Union{Nothing, String}\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.PassiveAggressiveClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.PassiveAggressiveClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nPassiveAggressiveClassifier\n```\n\nA model type for constructing a passive aggressive classifier, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nPassiveAggressiveClassifier = @load PassiveAggressiveClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = PassiveAggressiveClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`PassiveAggressiveClassifier(C=...)`.\n# Hyper-parameters\n\n- `C = 1.0`\n\n- `fit_intercept = true`\n\n- `max_iter = 100`\n\n- `tol = 0.001`\n\n- `early_stopping = false`\n\n- `validation_fraction = 0.1`\n\n- `n_iter_no_change = 5`\n\n- `shuffle = true`\n\n- `verbose = 0`\n\n- `loss = hinge`\n\n- `n_jobs = nothing`\n\n- `random_state = 0`\n\n- `warm_start = false`\n\n- `class_weight = nothing`\n\n- `average = false`\n\n"
":name" = "PassiveAggressiveClassifier"
":human_name" = "passive aggressive classifier"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:C, :fit_intercept, :max_iter, :tol, :early_stopping, :validation_fraction, :n_iter_no_change, :shuffle, :verbose, :loss, :n_jobs, :random_state, :warm_start, :class_weight, :average)`"
":hyperparameter_types" = "`(\"Float64\", \"Bool\", \"Int64\", \"Float64\", \"Bool\", \"Float64\", \"Int64\", \"Bool\", \"Int64\", \"String\", \"Union{Nothing, Int64}\", \"Any\", \"Bool\", \"Any\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.SVMRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.SVMRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nSVMRegressor\n```\n\nA model type for constructing a epsilon-support vector regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nSVMRegressor = @load SVMRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = SVMRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`SVMRegressor(kernel=...)`.\n# Hyper-parameters\n\n- `kernel = rbf`\n\n- `degree = 3`\n\n- `gamma = scale`\n\n- `coef0 = 0.0`\n\n- `tol = 0.001`\n\n- `C = 1.0`\n\n- `epsilon = 0.1`\n\n- `shrinking = true`\n\n- `cache_size = 200`\n\n- `max_iter = -1`\n\n"
":name" = "SVMRegressor"
":human_name" = "epsilon-support vector regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:kernel, :degree, :gamma, :coef0, :tol, :C, :epsilon, :shrinking, :cache_size, :max_iter)`"
":hyperparameter_types" = "`(\"Union{Function, String}\", \"Int64\", \"Union{Float64, String}\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Any\", \"Int64\", \"Int64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.BernoulliNBClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Count}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Count}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.BernoulliNBClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nBernoulliNBClassifier\n```\n\nA model type for constructing a Bernoulli naive Bayes classifier, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nBernoulliNBClassifier = @load BernoulliNBClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = BernoulliNBClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `BernoulliNBClassifier(alpha=...)`.\n\nBinomial naive bayes classifier. It is suitable for classification with binary features; features will be binarized based on the `binarize` keyword (unless it's `nothing` in which case the features are assumed to be binary).\n"
":name" = "BernoulliNBClassifier"
":human_name" = "Bernoulli naive Bayes classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:alpha, :binarize, :fit_prior, :class_prior)`"
":hyperparameter_types" = "`(\"Float64\", \"Union{Nothing, Float64}\", \"Bool\", \"Union{Nothing, AbstractVector}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.GaussianNBClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.GaussianNBClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nGaussianNBClassifier\n```\n\nA model type for constructing a Gaussian naive Bayes classifier, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nGaussianNBClassifier = @load GaussianNBClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = GaussianNBClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`GaussianNBClassifier(priors=...)`.\n# Hyper-parameters\n\n- `priors = nothing`\n\n- `var_smoothing = 1.0e-9`\n\n"
":name" = "GaussianNBClassifier"
":human_name" = "Gaussian naive Bayes classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:priors, :var_smoothing)`"
":hyperparameter_types" = "`(\"Union{Nothing, AbstractVector{Float64}}\", \"Float64\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.ExtraTreesClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.ExtraTreesClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nExtraTreesClassifier\n```\n\nA model type for constructing a extra trees classifier, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nExtraTreesClassifier = @load ExtraTreesClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = ExtraTreesClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `ExtraTreesClassifier(n_estimators=...)`.\n\nExtra trees classifier, fits a number of randomized decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n"
":name" = "ExtraTreesClassifier"
":human_name" = "extra trees classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:n_estimators, :criterion, :max_depth, :min_samples_split, :min_samples_leaf, :min_weight_fraction_leaf, :max_features, :max_leaf_nodes, :min_impurity_decrease, :bootstrap, :oob_score, :n_jobs, :random_state, :verbose, :warm_start, :class_weight)`"
":hyperparameter_types" = "`(\"Int64\", \"String\", \"Union{Nothing, Int64}\", \"Union{Float64, Int64}\", \"Union{Float64, Int64}\", \"Float64\", \"Union{Nothing, Float64, Int64, String}\", \"Union{Nothing, Int64}\", \"Float64\", \"Bool\", \"Bool\", \"Union{Nothing, Int64}\", \"Any\", \"Int64\", \"Bool\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.KMeans]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Multiclass}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.KMeans"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nKMeans\n```\n\nA model type for constructing a k means, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nKMeans = @load KMeans pkg=MLJScikitLearnInterface\n```\n\nDo `model = KMeans()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `KMeans(n_clusters=...)`.\n\nK-Means algorithm: find K centroids corresponding to K clusters in the data.\n"
":name" = "KMeans"
":human_name" = "k means"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":transform"]
":hyperparameters" = "`(:n_clusters, :n_init, :max_iter, :tol, :verbose, :random_state, :copy_x, :algorithm, :init)`"
":hyperparameter_types" = "`(\"Int64\", \"Union{Int64, String}\", \"Int64\", \"Float64\", \"Int64\", \"Any\", \"Bool\", \"String\", \"Union{String, AbstractArray}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.MultiTaskElasticNetCVRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.MultiTaskElasticNetCVRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nMultiTaskElasticNetCVRegressor\n```\n\nA model type for constructing a multi-target elastic net regressor with built-in cross-validation, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nMultiTaskElasticNetCVRegressor = @load MultiTaskElasticNetCVRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = MultiTaskElasticNetCVRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`MultiTaskElasticNetCVRegressor(l1_ratio=...)`.\n# Hyper-parameters\n\n- `l1_ratio = 0.5`\n\n- `eps = 0.001`\n\n- `n_alphas = 100`\n\n- `alphas = nothing`\n\n- `fit_intercept = true`\n\n- `max_iter = 1000`\n\n- `tol = 0.0001`\n\n- `cv = 5`\n\n- `copy_X = true`\n\n- `verbose = 0`\n\n- `n_jobs = nothing`\n\n- `random_state = nothing`\n\n- `selection = cyclic`\n\n"
":name" = "MultiTaskElasticNetCVRegressor"
":human_name" = "multi-target elastic net regressor with built-in cross-validation"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:l1_ratio, :eps, :n_alphas, :alphas, :fit_intercept, :max_iter, :tol, :cv, :copy_X, :verbose, :n_jobs, :random_state, :selection)`"
":hyperparameter_types" = "`(\"Union{Float64, Vector{Float64}}\", \"Float64\", \"Int64\", \"Any\", \"Bool\", \"Int64\", \"Float64\", \"Any\", \"Bool\", \"Union{Bool, Int64}\", \"Union{Nothing, Int64}\", \"Any\", \"String\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.LassoLarsCVRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.LassoLarsCVRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLassoLarsCVRegressor\n```\n\nA model type for constructing a Lasso model fit with least angle regression (LARS) with built-in cross-validation, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nLassoLarsCVRegressor = @load LassoLarsCVRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = LassoLarsCVRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LassoLarsCVRegressor(fit_intercept=...)`.\n# Hyper-parameters\n\n- `fit_intercept = true`\n\n- `verbose = false`\n\n- `max_iter = 500`\n\n- `normalize = false`\n\n- `precompute = auto`\n\n- `cv = 5`\n\n- `max_n_alphas = 1000`\n\n- `n_jobs = nothing`\n\n- `eps = 2.220446049250313e-16`\n\n- `copy_X = true`\n\n- `positive = false`\n\n"
":name" = "LassoLarsCVRegressor"
":human_name" = "Lasso model fit with least angle regression (LARS) with built-in cross-validation"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:fit_intercept, :verbose, :max_iter, :normalize, :precompute, :cv, :max_n_alphas, :n_jobs, :eps, :copy_X, :positive)`"
":hyperparameter_types" = "`(\"Bool\", \"Union{Bool, Int64}\", \"Int64\", \"Bool\", \"Union{Bool, String, AbstractMatrix}\", \"Any\", \"Int64\", \"Union{Nothing, Int64}\", \"Float64\", \"Bool\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.OrthogonalMatchingPursuitCVRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.OrthogonalMatchingPursuitCVRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nOrthogonalMatchingPursuitCVRegressor\n```\n\nA model type for constructing a orthogonal ,atching pursuit (OMP) model with built-in cross-validation, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nOrthogonalMatchingPursuitCVRegressor = @load OrthogonalMatchingPursuitCVRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = OrthogonalMatchingPursuitCVRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`OrthogonalMatchingPursuitCVRegressor(copy=...)`.\n# Hyper-parameters\n\n- `copy = true`\n\n- `fit_intercept = true`\n\n- `normalize = false`\n\n- `max_iter = nothing`\n\n- `cv = 5`\n\n- `n_jobs = 1`\n\n- `verbose = false`\n\n"
":name" = "OrthogonalMatchingPursuitCVRegressor"
":human_name" = "orthogonal ,atching pursuit (OMP) model with built-in cross-validation"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:copy, :fit_intercept, :normalize, :max_iter, :cv, :n_jobs, :verbose)`"
":hyperparameter_types" = "`(\"Bool\", \"Bool\", \"Bool\", \"Union{Nothing, Int64}\", \"Any\", \"Union{Nothing, Int64}\", \"Union{Bool, Int64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.AdaBoostClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.AdaBoostClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nAdaBoostClassifier\n```\n\nA model type for constructing a ada boost classifier, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nAdaBoostClassifier = @load AdaBoostClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = AdaBoostClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`AdaBoostClassifier(estimator=...)`.\n# Hyper-parameters\n\n- `estimator = nothing`\n\n- `n_estimators = 50`\n\n- `learning_rate = 1.0`\n\n- `algorithm = SAMME.R`\n\n- `random_state = nothing`\n\n"
":name" = "AdaBoostClassifier"
":human_name" = "ada boost classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:estimator, :n_estimators, :learning_rate, :algorithm, :random_state)`"
":hyperparameter_types" = "`(\"Any\", \"Int64\", \"Float64\", \"String\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.PassiveAggressiveRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.PassiveAggressiveRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nPassiveAggressiveRegressor\n```\n\nA model type for constructing a passive aggressive regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nPassiveAggressiveRegressor = @load PassiveAggressiveRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = PassiveAggressiveRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`PassiveAggressiveRegressor(C=...)`.\n# Hyper-parameters\n\n- `C = 1.0`\n\n- `fit_intercept = true`\n\n- `max_iter = 1000`\n\n- `tol = 0.0001`\n\n- `early_stopping = false`\n\n- `validation_fraction = 0.1`\n\n- `n_iter_no_change = 5`\n\n- `shuffle = true`\n\n- `verbose = 0`\n\n- `loss = epsilon_insensitive`\n\n- `epsilon = 0.1`\n\n- `random_state = nothing`\n\n- `warm_start = false`\n\n- `average = false`\n\n"
":name" = "PassiveAggressiveRegressor"
":human_name" = "passive aggressive regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:C, :fit_intercept, :max_iter, :tol, :early_stopping, :validation_fraction, :n_iter_no_change, :shuffle, :verbose, :loss, :epsilon, :random_state, :warm_start, :average)`"
":hyperparameter_types" = "`(\"Float64\", \"Bool\", \"Int64\", \"Float64\", \"Bool\", \"Float64\", \"Int64\", \"Bool\", \"Union{Bool, Int64}\", \"String\", \"Float64\", \"Any\", \"Bool\", \"Union{Bool, Int64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.BayesianRidgeRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.BayesianRidgeRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nBayesianRidgeRegressor\n```\n\nA model type for constructing a Bayesian ridge regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nBayesianRidgeRegressor = @load BayesianRidgeRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = BayesianRidgeRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`BayesianRidgeRegressor(n_iter=...)`.\n# Hyper-parameters\n\n- `n_iter = 300`\n\n- `tol = 0.001`\n\n- `alpha_1 = 1.0e-6`\n\n- `alpha_2 = 1.0e-6`\n\n- `lambda_1 = 1.0e-6`\n\n- `lambda_2 = 1.0e-6`\n\n- `compute_score = false`\n\n- `fit_intercept = true`\n\n- `copy_X = true`\n\n- `verbose = false`\n\n"
":name" = "BayesianRidgeRegressor"
":human_name" = "Bayesian ridge regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:n_iter, :tol, :alpha_1, :alpha_2, :lambda_1, :lambda_2, :compute_score, :fit_intercept, :copy_X, :verbose)`"
":hyperparameter_types" = "`(\"Int64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Bool\", \"Bool\", \"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.RANSACRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.RANSACRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nRANSACRegressor\n```\n\nA model type for constructing a ransac regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nRANSACRegressor = @load RANSACRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = RANSACRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`RANSACRegressor(estimator=...)`.\n# Hyper-parameters\n\n- `estimator = nothing`\n\n- `min_samples = 5`\n\n- `residual_threshold = nothing`\n\n- `is_data_valid = nothing`\n\n- `is_model_valid = nothing`\n\n- `max_trials = 100`\n\n- `max_skips = 9223372036854775807`\n\n- `stop_n_inliers = 9223372036854775807`\n\n- `stop_score = Inf`\n\n- `stop_probability = 0.99`\n\n- `loss = absolute_error`\n\n- `random_state = nothing`\n\n"
":name" = "RANSACRegressor"
":human_name" = "ransac regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:estimator, :min_samples, :residual_threshold, :is_data_valid, :is_model_valid, :max_trials, :max_skips, :stop_n_inliers, :stop_score, :stop_probability, :loss, :random_state)`"
":hyperparameter_types" = "`(\"Any\", \"Union{Float64, Int64}\", \"Union{Nothing, Float64}\", \"Any\", \"Any\", \"Int64\", \"Int64\", \"Int64\", \"Float64\", \"Float64\", \"Union{Function, String}\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.BaggingClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.BaggingClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nBaggingClassifier\n```\n\nA model type for constructing a bagging ensemble classifier, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nBaggingClassifier = @load BaggingClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = BaggingClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`BaggingClassifier(estimator=...)`.\n# Hyper-parameters\n\n- `estimator = nothing`\n\n- `n_estimators = 10`\n\n- `max_samples = 1.0`\n\n- `max_features = 1.0`\n\n- `bootstrap = true`\n\n- `bootstrap_features = false`\n\n- `oob_score = false`\n\n- `warm_start = false`\n\n- `n_jobs = nothing`\n\n- `random_state = nothing`\n\n- `verbose = 0`\n\n"
":name" = "BaggingClassifier"
":human_name" = "bagging ensemble classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:estimator, :n_estimators, :max_samples, :max_features, :bootstrap, :bootstrap_features, :oob_score, :warm_start, :n_jobs, :random_state, :verbose)`"
":hyperparameter_types" = "`(\"Any\", \"Int64\", \"Union{Float64, Int64}\", \"Union{Float64, Int64}\", \"Bool\", \"Bool\", \"Bool\", \"Bool\", \"Union{Nothing, Int64}\", \"Any\", \"Int64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.GaussianProcessClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.GaussianProcessClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nGaussianProcessClassifier\n```\n\nA model type for constructing a Gaussian process classifier, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nGaussianProcessClassifier = @load GaussianProcessClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = GaussianProcessClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`GaussianProcessClassifier(kernel=...)`.\n# Hyper-parameters\n\n- `kernel = nothing`\n\n- `optimizer = fmin_l_bfgs_b`\n\n- `n_restarts_optimizer = 0`\n\n- `copy_X_train = true`\n\n- `random_state = nothing`\n\n- `max_iter_predict = 100`\n\n- `warm_start = false`\n\n- `multi_class = one_vs_rest`\n\n"
":name" = "GaussianProcessClassifier"
":human_name" = "Gaussian process classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:kernel, :optimizer, :n_restarts_optimizer, :copy_X_train, :random_state, :max_iter_predict, :warm_start, :multi_class)`"
":hyperparameter_types" = "`(\"Any\", \"Any\", \"Int64\", \"Bool\", \"Any\", \"Int64\", \"Bool\", \"String\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.OPTICS]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.OPTICS"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nOPTICS\n```\n\nA model type for constructing a optics, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nOPTICS = @load OPTICS pkg=MLJScikitLearnInterface\n```\n\nDo `model = OPTICS()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `OPTICS(min_samples=...)`.\n\nOPTICS (Ordering Points To Identify the Clustering Structure), closely related to [`DBSCAN'](@ref), finds core sample of high density and expands clusters from them. Unlike DBSCAN, keeps cluster hierarchy for a variable neighborhood radius. Better suited for usage on large datasets than the current sklearn implementation of DBSCAN.\n"
":name" = "OPTICS"
":human_name" = "optics"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params"]
":hyperparameters" = "`(:min_samples, :max_eps, :metric, :p, :cluster_method, :eps, :xi, :predecessor_correction, :min_cluster_size, :algorithm, :leaf_size, :n_jobs)`"
":hyperparameter_types" = "`(\"Union{Float64, Int64}\", \"Float64\", \"String\", \"Int64\", \"String\", \"Union{Nothing, Float64}\", \"Float64\", \"Bool\", \"Union{Nothing, Float64, Int64}\", \"String\", \"Int64\", \"Union{Nothing, Int64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.KNeighborsRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.KNeighborsRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nKNeighborsRegressor\n```\n\nA model type for constructing a K-nearest neighbors regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nKNeighborsRegressor = @load KNeighborsRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = KNeighborsRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`KNeighborsRegressor(n_neighbors=...)`.\n# Hyper-parameters\n\n- `n_neighbors = 5`\n\n- `weights = uniform`\n\n- `algorithm = auto`\n\n- `leaf_size = 30`\n\n- `p = 2`\n\n- `metric = minkowski`\n\n- `metric_params = nothing`\n\n- `n_jobs = nothing`\n\n"
":name" = "KNeighborsRegressor"
":human_name" = "K-nearest neighbors regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:n_neighbors, :weights, :algorithm, :leaf_size, :p, :metric, :metric_params, :n_jobs)`"
":hyperparameter_types" = "`(\"Int64\", \"Union{Function, String}\", \"String\", \"Int64\", \"Int64\", \"Any\", \"Any\", \"Union{Nothing, Int64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.MiniBatchKMeans]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Multiclass}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.MiniBatchKMeans"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nMiniBatchKMeans\n```\n\nA model type for constructing a Mini-Batch K-Means clustering., based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nMiniBatchKMeans = @load MiniBatchKMeans pkg=MLJScikitLearnInterface\n```\n\nDo `model = MiniBatchKMeans()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`MiniBatchKMeans(n_clusters=...)`.\n# Hyper-parameters\n\n- `n_clusters = 8`\n\n- `max_iter = 100`\n\n- `batch_size = 100`\n\n- `verbose = 0`\n\n- `compute_labels = true`\n\n- `random_state = nothing`\n\n- `tol = 0.0`\n\n- `max_no_improvement = 10`\n\n- `init_size = nothing`\n\n- `n_init = 3`\n\n- `init = k-means++`\n\n- `reassignment_ratio = 0.01`\n\n"
":name" = "MiniBatchKMeans"
":human_name" = "Mini-Batch K-Means clustering."
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":transform"]
":hyperparameters" = "`(:n_clusters, :max_iter, :batch_size, :verbose, :compute_labels, :random_state, :tol, :max_no_improvement, :init_size, :n_init, :init, :reassignment_ratio)`"
":hyperparameter_types" = "`(\"Int64\", \"Int64\", \"Int64\", \"Int64\", \"Bool\", \"Any\", \"Float64\", \"Int64\", \"Union{Nothing, Int64}\", \"Union{Int64, String}\", \"Union{String, AbstractArray}\", \"Float64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.LassoCVRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.LassoCVRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLassoCVRegressor\n```\n\nA model type for constructing a lasso regressor with built-in cross-validation, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nLassoCVRegressor = @load LassoCVRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = LassoCVRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LassoCVRegressor(eps=...)`.\n# Hyper-parameters\n\n- `eps = 0.001`\n\n- `n_alphas = 100`\n\n- `alphas = nothing`\n\n- `fit_intercept = true`\n\n- `precompute = auto`\n\n- `max_iter = 1000`\n\n- `tol = 0.0001`\n\n- `copy_X = true`\n\n- `cv = 5`\n\n- `verbose = false`\n\n- `n_jobs = nothing`\n\n- `positive = false`\n\n- `random_state = nothing`\n\n- `selection = cyclic`\n\n"
":name" = "LassoCVRegressor"
":human_name" = "lasso regressor with built-in cross-validation"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:eps, :n_alphas, :alphas, :fit_intercept, :precompute, :max_iter, :tol, :copy_X, :cv, :verbose, :n_jobs, :positive, :random_state, :selection)`"
":hyperparameter_types" = "`(\"Float64\", \"Int64\", \"Any\", \"Bool\", \"Union{Bool, String, AbstractMatrix}\", \"Int64\", \"Float64\", \"Bool\", \"Any\", \"Union{Bool, Int64}\", \"Union{Nothing, Int64}\", \"Bool\", \"Any\", \"String\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.DummyRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.DummyRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nDummyRegressor\n```\n\nA model type for constructing a dummy regressor, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nDummyRegressor = @load DummyRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = DummyRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `DummyRegressor(strategy=...)`.\n\nDummyRegressor is a regressor that makes predictions using simple rules.\n"
":name" = "DummyRegressor"
":human_name" = "dummy regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:strategy, :constant, :quantile)`"
":hyperparameter_types" = "`(\"String\", \"Any\", \"Float64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.LassoLarsRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.LassoLarsRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLassoLarsRegressor\n```\n\nA model type for constructing a Lasso model fit with least angle regression (LARS), based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nLassoLarsRegressor = @load LassoLarsRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = LassoLarsRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LassoLarsRegressor(alpha=...)`.\n# Hyper-parameters\n\n- `alpha = 1.0`\n\n- `fit_intercept = true`\n\n- `verbose = false`\n\n- `normalize = false`\n\n- `precompute = auto`\n\n- `max_iter = 500`\n\n- `eps = 2.220446049250313e-16`\n\n- `copy_X = true`\n\n- `fit_path = true`\n\n- `positive = false`\n\n"
":name" = "LassoLarsRegressor"
":human_name" = "Lasso model fit with least angle regression (LARS)"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:alpha, :fit_intercept, :verbose, :normalize, :precompute, :max_iter, :eps, :copy_X, :fit_path, :positive)`"
":hyperparameter_types" = "`(\"Float64\", \"Bool\", \"Union{Bool, Int64}\", \"Bool\", \"Union{Bool, String, AbstractMatrix}\", \"Int64\", \"Float64\", \"Bool\", \"Bool\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.LarsCVRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.LarsCVRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLarsCVRegressor\n```\n\nA model type for constructing a least angle regressor with built-in cross-validation, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nLarsCVRegressor = @load LarsCVRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = LarsCVRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LarsCVRegressor(fit_intercept=...)`.\n# Hyper-parameters\n\n- `fit_intercept = true`\n\n- `verbose = false`\n\n- `max_iter = 500`\n\n- `normalize = false`\n\n- `precompute = auto`\n\n- `cv = 5`\n\n- `max_n_alphas = 1000`\n\n- `n_jobs = nothing`\n\n- `eps = 2.220446049250313e-16`\n\n- `copy_X = true`\n\n"
":name" = "LarsCVRegressor"
":human_name" = "least angle regressor with built-in cross-validation"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:fit_intercept, :verbose, :max_iter, :normalize, :precompute, :cv, :max_n_alphas, :n_jobs, :eps, :copy_X)`"
":hyperparameter_types" = "`(\"Bool\", \"Union{Bool, Int64}\", \"Int64\", \"Bool\", \"Union{Bool, String, AbstractMatrix}\", \"Any\", \"Int64\", \"Union{Nothing, Int64}\", \"Float64\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.KNeighborsClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.KNeighborsClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nKNeighborsClassifier\n```\n\nA model type for constructing a K-nearest neighbors classifier, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nKNeighborsClassifier = @load KNeighborsClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = KNeighborsClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`KNeighborsClassifier(n_neighbors=...)`.\n# Hyper-parameters\n\n- `n_neighbors = 5`\n\n- `weights = uniform`\n\n- `algorithm = auto`\n\n- `leaf_size = 30`\n\n- `p = 2`\n\n- `metric = minkowski`\n\n- `metric_params = nothing`\n\n- `n_jobs = nothing`\n\n"
":name" = "KNeighborsClassifier"
":human_name" = "K-nearest neighbors classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:n_neighbors, :weights, :algorithm, :leaf_size, :p, :metric, :metric_params, :n_jobs)`"
":hyperparameter_types" = "`(\"Int64\", \"Union{Function, String}\", \"String\", \"Int64\", \"Int64\", \"Any\", \"Any\", \"Union{Nothing, Int64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.SVMLinearClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.SVMLinearClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nSVMLinearClassifier\n```\n\nA model type for constructing a linear support vector classifier, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nSVMLinearClassifier = @load SVMLinearClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = SVMLinearClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`SVMLinearClassifier(penalty=...)`.\n# Hyper-parameters\n\n- `penalty = l2`\n\n- `loss = squared_hinge`\n\n- `dual = true`\n\n- `tol = 0.0001`\n\n- `C = 1.0`\n\n- `multi_class = ovr`\n\n- `fit_intercept = true`\n\n- `intercept_scaling = 1.0`\n\n- `random_state = nothing`\n\n- `max_iter = 1000`\n\n"
":name" = "SVMLinearClassifier"
":human_name" = "linear support vector classifier"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:penalty, :loss, :dual, :tol, :C, :multi_class, :fit_intercept, :intercept_scaling, :random_state, :max_iter)`"
":hyperparameter_types" = "`(\"String\", \"String\", \"Bool\", \"Float64\", \"Float64\", \"String\", \"Bool\", \"Float64\", \"Any\", \"Int64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.FeatureAgglomeration]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.FeatureAgglomeration"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nFeatureAgglomeration\n```\n\nA model type for constructing a feature agglomeration, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nFeatureAgglomeration = @load FeatureAgglomeration pkg=MLJScikitLearnInterface\n```\n\nDo `model = FeatureAgglomeration()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `FeatureAgglomeration(n_clusters=...)`.\n\nSimilar to [`AgglomerativeClustering`](@ref), but recursively merges features instead of samples.\"\n"
":name" = "FeatureAgglomeration"
":human_name" = "feature agglomeration"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":inverse_transform", ":transform"]
":hyperparameters" = "`(:n_clusters, :memory, :connectivity, :affinity, :compute_full_tree, :linkage, :distance_threshold)`"
":hyperparameter_types" = "`(\"Int64\", \"Any\", \"Any\", \"Any\", \"Union{Bool, String}\", \"String\", \"Union{Nothing, Float64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.DummyClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.DummyClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nDummyClassifier\n```\n\nA model type for constructing a dummy classifier, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nDummyClassifier = @load DummyClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = DummyClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `DummyClassifier(strategy=...)`.\n\nDummyClassifier is a classifier that makes predictions using simple rules.\n"
":name" = "DummyClassifier"
":human_name" = "dummy classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:strategy, :constant, :random_state)`"
":hyperparameter_types" = "`(\"String\", \"Any\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.BaggingRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.BaggingRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nBaggingRegressor\n```\n\nA model type for constructing a bagging ensemble regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nBaggingRegressor = @load BaggingRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = BaggingRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`BaggingRegressor(estimator=...)`.\n# Hyper-parameters\n\n- `estimator = nothing`\n\n- `n_estimators = 10`\n\n- `max_samples = 1.0`\n\n- `max_features = 1.0`\n\n- `bootstrap = true`\n\n- `bootstrap_features = false`\n\n- `oob_score = false`\n\n- `warm_start = false`\n\n- `n_jobs = nothing`\n\n- `random_state = nothing`\n\n- `verbose = 0`\n\n"
":name" = "BaggingRegressor"
":human_name" = "bagging ensemble regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:estimator, :n_estimators, :max_samples, :max_features, :bootstrap, :bootstrap_features, :oob_score, :warm_start, :n_jobs, :random_state, :verbose)`"
":hyperparameter_types" = "`(\"Any\", \"Int64\", \"Union{Float64, Int64}\", \"Union{Float64, Int64}\", \"Bool\", \"Bool\", \"Bool\", \"Bool\", \"Union{Nothing, Int64}\", \"Any\", \"Int64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.BayesianQDA]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.BayesianQDA"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nBayesianQDA\n```\n\nA model type for constructing a Bayesian quadratic discriminant analysis, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nBayesianQDA = @load BayesianQDA pkg=MLJScikitLearnInterface\n```\n\nDo `model = BayesianQDA()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`BayesianQDA(priors=...)`.\n# Hyper-parameters\n\n- `priors = nothing`\n\n- `reg_param = 0.0`\n\n- `store_covariance = false`\n\n- `tol = 0.0001`\n\n"
":name" = "BayesianQDA"
":human_name" = "Bayesian quadratic discriminant analysis"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:priors, :reg_param, :store_covariance, :tol)`"
":hyperparameter_types" = "`(\"Union{Nothing, AbstractVector}\", \"Float64\", \"Bool\", \"Float64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.BayesianLDA]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.BayesianLDA"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nBayesianLDA\n```\n\nA model type for constructing a Bayesian linear discriminant analysis, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nBayesianLDA = @load BayesianLDA pkg=MLJScikitLearnInterface\n```\n\nDo `model = BayesianLDA()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`BayesianLDA(solver=...)`.\n# Hyper-parameters\n\n- `solver = svd`\n\n- `shrinkage = nothing`\n\n- `priors = nothing`\n\n- `n_components = nothing`\n\n- `store_covariance = false`\n\n- `tol = 0.0001`\n\n- `covariance_estimator = nothing`\n\n"
":name" = "BayesianLDA"
":human_name" = "Bayesian linear discriminant analysis"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:solver, :shrinkage, :priors, :n_components, :store_covariance, :tol, :covariance_estimator)`"
":hyperparameter_types" = "`(\"String\", \"Union{Nothing, Float64, String}\", \"Union{Nothing, AbstractVector}\", \"Union{Nothing, Int64}\", \"Bool\", \"Float64\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.SGDClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.SGDClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nSGDClassifier\n```\n\nA model type for constructing a sgd classifier, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nSGDClassifier = @load SGDClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = SGDClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`SGDClassifier(loss=...)`.\n# Hyper-parameters\n\n- `loss = hinge`\n\n- `penalty = l2`\n\n- `alpha = 0.0001`\n\n- `l1_ratio = 0.15`\n\n- `fit_intercept = true`\n\n- `max_iter = 1000`\n\n- `tol = 0.001`\n\n- `shuffle = true`\n\n- `verbose = 0`\n\n- `epsilon = 0.1`\n\n- `n_jobs = nothing`\n\n- `random_state = nothing`\n\n- `learning_rate = optimal`\n\n- `eta0 = 0.0`\n\n- `power_t = 0.5`\n\n- `early_stopping = false`\n\n- `validation_fraction = 0.1`\n\n- `n_iter_no_change = 5`\n\n- `class_weight = nothing`\n\n- `warm_start = false`\n\n- `average = false`\n\n"
":name" = "SGDClassifier"
":human_name" = "sgd classifier"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:loss, :penalty, :alpha, :l1_ratio, :fit_intercept, :max_iter, :tol, :shuffle, :verbose, :epsilon, :n_jobs, :random_state, :learning_rate, :eta0, :power_t, :early_stopping, :validation_fraction, :n_iter_no_change, :class_weight, :warm_start, :average)`"
":hyperparameter_types" = "`(\"String\", \"String\", \"Float64\", \"Float64\", \"Bool\", \"Int64\", \"Union{Nothing, Float64}\", \"Bool\", \"Int64\", \"Float64\", \"Union{Nothing, Int64}\", \"Any\", \"String\", \"Float64\", \"Float64\", \"Bool\", \"Float64\", \"Int64\", \"Any\", \"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.TheilSenRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.TheilSenRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nTheilSenRegressor\n```\n\nA model type for constructing a Theil-Sen regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nTheilSenRegressor = @load TheilSenRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = TheilSenRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`TheilSenRegressor(fit_intercept=...)`.\n# Hyper-parameters\n\n- `fit_intercept = true`\n\n- `copy_X = true`\n\n- `max_subpopulation = 10000`\n\n- `n_subsamples = nothing`\n\n- `max_iter = 300`\n\n- `tol = 0.001`\n\n- `random_state = nothing`\n\n- `n_jobs = nothing`\n\n- `verbose = false`\n\n"
":name" = "TheilSenRegressor"
":human_name" = "Theil-Sen regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:fit_intercept, :copy_X, :max_subpopulation, :n_subsamples, :max_iter, :tol, :random_state, :n_jobs, :verbose)`"
":hyperparameter_types" = "`(\"Bool\", \"Bool\", \"Int64\", \"Union{Nothing, Int64}\", \"Int64\", \"Float64\", \"Any\", \"Union{Nothing, Int64}\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.SpectralClustering]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.SpectralClustering"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nSpectralClustering\n```\n\nA model type for constructing a spectral clustering, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nSpectralClustering = @load SpectralClustering pkg=MLJScikitLearnInterface\n```\n\nDo `model = SpectralClustering()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `SpectralClustering(n_clusters=...)`.\n\nApply clustering to a projection of the normalized Laplacian.  In practice spectral clustering is very useful when the structure of the individual clusters is highly non-convex or more generally when a measure of the center and spread of the cluster is not a suitable description of the complete cluster. For instance when clusters are nested circles on the 2D plane.\n"
":name" = "SpectralClustering"
":human_name" = "spectral clustering"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params"]
":hyperparameters" = "`(:n_clusters, :eigen_solver, :random_state, :n_init, :gamma, :affinity, :n_neighbors, :eigen_tol, :assign_labels, :n_jobs)`"
":hyperparameter_types" = "`(\"Int64\", \"Union{Nothing, String}\", \"Any\", \"Int64\", \"Float64\", \"String\", \"Int64\", \"Float64\", \"String\", \"Union{Nothing, Int64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.Birch]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Multiclass}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.Birch"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nBirch\n```\n\nA model type for constructing a birch, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nBirch = @load Birch pkg=MLJScikitLearnInterface\n```\n\nDo `model = Birch()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `Birch(threshold=...)`.\n\nMemory-efficient, online-learning algorithm provided as an alternative to MiniBatchKMeans. Note: noisy samples are given the label -1.\n"
":name" = "Birch"
":human_name" = "birch"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":transform"]
":hyperparameters" = "`(:threshold, :branching_factor, :n_clusters, :compute_labels, :copy)`"
":hyperparameter_types" = "`(\"Float64\", \"Int64\", \"Int64\", \"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.AgglomerativeClustering]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.AgglomerativeClustering"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nAgglomerativeClustering\n```\n\nA model type for constructing a agglomerative clustering, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nAgglomerativeClustering = @load AgglomerativeClustering pkg=MLJScikitLearnInterface\n```\n\nDo `model = AgglomerativeClustering()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `AgglomerativeClustering(n_clusters=...)`.\n\nRecursively merges the pair of clusters that minimally increases a given linkage distance. Note: there is no `predict` or `transform`. Instead, inspect the `fitted_params`.\n"
":name" = "AgglomerativeClustering"
":human_name" = "agglomerative clustering"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params"]
":hyperparameters" = "`(:n_clusters, :affinity, :memory, :connectivity, :compute_full_tree, :linkage, :distance_threshold)`"
":hyperparameter_types" = "`(\"Int64\", \"String\", \"Any\", \"Any\", \"Union{Bool, String}\", \"String\", \"Union{Nothing, Float64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.ElasticNetRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.ElasticNetRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nElasticNetRegressor\n```\n\nA model type for constructing a elastic net regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nElasticNetRegressor = @load ElasticNetRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = ElasticNetRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`ElasticNetRegressor(alpha=...)`.\n# Hyper-parameters\n\n- `alpha = 1.0`\n\n- `l1_ratio = 0.5`\n\n- `fit_intercept = true`\n\n- `precompute = false`\n\n- `max_iter = 1000`\n\n- `copy_X = true`\n\n- `tol = 0.0001`\n\n- `warm_start = false`\n\n- `positive = false`\n\n- `random_state = nothing`\n\n- `selection = cyclic`\n\n"
":name" = "ElasticNetRegressor"
":human_name" = "elastic net regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:alpha, :l1_ratio, :fit_intercept, :precompute, :max_iter, :copy_X, :tol, :warm_start, :positive, :random_state, :selection)`"
":hyperparameter_types" = "`(\"Float64\", \"Float64\", \"Bool\", \"Union{Bool, AbstractMatrix}\", \"Int64\", \"Bool\", \"Float64\", \"Bool\", \"Bool\", \"Any\", \"String\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.RandomForestClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.Continuous}}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.RandomForestClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nRandomForestClassifier\n```\n\nA model type for constructing a random forest classifier, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nRandomForestClassifier = @load RandomForestClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = RandomForestClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`RandomForestClassifier(n_estimators=...)`.\n# Hyper-parameters\n\n- `n_estimators = 100`\n\n- `criterion = gini`\n\n- `max_depth = nothing`\n\n- `min_samples_split = 2`\n\n- `min_samples_leaf = 1`\n\n- `min_weight_fraction_leaf = 0.0`\n\n- `max_features = sqrt`\n\n- `max_leaf_nodes = nothing`\n\n- `min_impurity_decrease = 0.0`\n\n- `bootstrap = true`\n\n- `oob_score = false`\n\n- `n_jobs = nothing`\n\n- `random_state = nothing`\n\n- `verbose = 0`\n\n- `warm_start = false`\n\n- `class_weight = nothing`\n\n- `ccp_alpha = 0.0`\n\n- `max_samples = nothing`\n\n"
":name" = "RandomForestClassifier"
":human_name" = "random forest classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:n_estimators, :criterion, :max_depth, :min_samples_split, :min_samples_leaf, :min_weight_fraction_leaf, :max_features, :max_leaf_nodes, :min_impurity_decrease, :bootstrap, :oob_score, :n_jobs, :random_state, :verbose, :warm_start, :class_weight, :ccp_alpha, :max_samples)`"
":hyperparameter_types" = "`(\"Int64\", \"String\", \"Union{Nothing, Int64}\", \"Union{Float64, Int64}\", \"Union{Float64, Int64}\", \"Float64\", \"Union{Nothing, Float64, Int64, String}\", \"Union{Nothing, Int64}\", \"Float64\", \"Bool\", \"Bool\", \"Union{Nothing, Int64}\", \"Any\", \"Int64\", \"Bool\", \"Any\", \"Float64\", \"Union{Nothing, Float64, Int64}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.LogisticCVClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.LogisticCVClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLogisticCVClassifier\n```\n\nA model type for constructing a logistic regression classifier with built-in cross-validation, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nLogisticCVClassifier = @load LogisticCVClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = LogisticCVClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LogisticCVClassifier(Cs=...)`.\n# Hyper-parameters\n\n- `Cs = 10`\n\n- `fit_intercept = true`\n\n- `cv = 5`\n\n- `dual = false`\n\n- `penalty = l2`\n\n- `scoring = nothing`\n\n- `solver = lbfgs`\n\n- `tol = 0.0001`\n\n- `max_iter = 100`\n\n- `class_weight = nothing`\n\n- `n_jobs = nothing`\n\n- `verbose = 0`\n\n- `refit = true`\n\n- `intercept_scaling = 1.0`\n\n- `multi_class = auto`\n\n- `random_state = nothing`\n\n- `l1_ratios = nothing`\n\n"
":name" = "LogisticCVClassifier"
":human_name" = "logistic regression classifier with built-in cross-validation"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:Cs, :fit_intercept, :cv, :dual, :penalty, :scoring, :solver, :tol, :max_iter, :class_weight, :n_jobs, :verbose, :refit, :intercept_scaling, :multi_class, :random_state, :l1_ratios)`"
":hyperparameter_types" = "`(\"Union{Int64, AbstractVector{Float64}}\", \"Bool\", \"Any\", \"Bool\", \"String\", \"Any\", \"String\", \"Float64\", \"Int64\", \"Any\", \"Union{Nothing, Int64}\", \"Int64\", \"Bool\", \"Float64\", \"String\", \"Any\", \"Union{Nothing, AbstractVector{Float64}}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.MultiTaskElasticNetRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.MultiTaskElasticNetRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nMultiTaskElasticNetRegressor\n```\n\nA model type for constructing a multi-target elastic net regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nMultiTaskElasticNetRegressor = @load MultiTaskElasticNetRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = MultiTaskElasticNetRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`MultiTaskElasticNetRegressor(alpha=...)`.\n# Hyper-parameters\n\n- `alpha = 1.0`\n\n- `l1_ratio = 0.5`\n\n- `fit_intercept = true`\n\n- `copy_X = true`\n\n- `max_iter = 1000`\n\n- `tol = 0.0001`\n\n- `warm_start = false`\n\n- `random_state = nothing`\n\n- `selection = cyclic`\n\n"
":name" = "MultiTaskElasticNetRegressor"
":human_name" = "multi-target elastic net regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:alpha, :l1_ratio, :fit_intercept, :copy_X, :max_iter, :tol, :warm_start, :random_state, :selection)`"
":hyperparameter_types" = "`(\"Float64\", \"Union{Float64, Vector{Float64}}\", \"Bool\", \"Bool\", \"Int64\", \"Float64\", \"Bool\", \"Any\", \"String\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.ExtraTreesRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.ExtraTreesRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nExtraTreesRegressor\n```\n\nA model type for constructing a extra trees regressor, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nExtraTreesRegressor = @load ExtraTreesRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = ExtraTreesRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `ExtraTreesRegressor(n_estimators=...)`.\n\nExtra trees regressor, fits a number of randomized decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n"
":name" = "ExtraTreesRegressor"
":human_name" = "extra trees regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:n_estimators, :criterion, :max_depth, :min_samples_split, :min_samples_leaf, :min_weight_fraction_leaf, :max_features, :max_leaf_nodes, :min_impurity_decrease, :bootstrap, :oob_score, :n_jobs, :random_state, :verbose, :warm_start)`"
":hyperparameter_types" = "`(\"Int64\", \"String\", \"Union{Nothing, Int64}\", \"Union{Float64, Int64}\", \"Union{Float64, Int64}\", \"Float64\", \"Union{Nothing, Float64, Int64, String}\", \"Union{Nothing, Int64}\", \"Float64\", \"Bool\", \"Bool\", \"Union{Nothing, Int64}\", \"Any\", \"Int64\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.LassoRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.LassoRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLassoRegressor\n```\n\nA model type for constructing a lasso regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nLassoRegressor = @load LassoRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = LassoRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LassoRegressor(alpha=...)`.\n# Hyper-parameters\n\n- `alpha = 1.0`\n\n- `fit_intercept = true`\n\n- `precompute = false`\n\n- `copy_X = true`\n\n- `max_iter = 1000`\n\n- `tol = 0.0001`\n\n- `warm_start = false`\n\n- `positive = false`\n\n- `random_state = nothing`\n\n- `selection = cyclic`\n\n"
":name" = "LassoRegressor"
":human_name" = "lasso regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:alpha, :fit_intercept, :precompute, :copy_X, :max_iter, :tol, :warm_start, :positive, :random_state, :selection)`"
":hyperparameter_types" = "`(\"Float64\", \"Bool\", \"Union{Bool, AbstractMatrix}\", \"Bool\", \"Int64\", \"Float64\", \"Bool\", \"Bool\", \"Any\", \"String\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.MultinomialNBClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Count}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Count}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.MultinomialNBClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nMultinomialNBClassifier\n```\n\nA model type for constructing a multinomial naive Bayes classifier, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nMultinomialNBClassifier = @load MultinomialNBClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = MultinomialNBClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `MultinomialNBClassifier(alpha=...)`.\n\nMultinomial naive bayes classifier. It is suitable for classification with discrete features (e.g. word counts for text classification).\n"
":name" = "MultinomialNBClassifier"
":human_name" = "multinomial naive Bayes classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:alpha, :fit_prior, :class_prior)`"
":hyperparameter_types" = "`(\"Float64\", \"Bool\", \"Union{Nothing, AbstractVector}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.GradientBoostingRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.GradientBoostingRegressor"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nGradientBoostingRegressor\n```\n\nA model type for constructing a gradient boosting ensemble regression, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nGradientBoostingRegressor = @load GradientBoostingRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = GradientBoostingRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`GradientBoostingRegressor(loss=...)`.\n# Hyper-parameters\n\n- `loss = squared_error`\n\n- `learning_rate = 0.1`\n\n- `n_estimators = 100`\n\n- `subsample = 1.0`\n\n- `criterion = friedman_mse`\n\n- `min_samples_split = 2`\n\n- `min_samples_leaf = 1`\n\n- `min_weight_fraction_leaf = 0.0`\n\n- `max_depth = 3`\n\n- `min_impurity_decrease = 0.0`\n\n- `init = nothing`\n\n- `random_state = nothing`\n\n- `max_features = nothing`\n\n- `alpha = 0.9`\n\n- `verbose = 0`\n\n- `max_leaf_nodes = nothing`\n\n- `warm_start = false`\n\n- `validation_fraction = 0.1`\n\n- `n_iter_no_change = nothing`\n\n- `tol = 0.0001`\n\n"
":name" = "GradientBoostingRegressor"
":human_name" = "gradient boosting ensemble regression"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:loss, :learning_rate, :n_estimators, :subsample, :criterion, :min_samples_split, :min_samples_leaf, :min_weight_fraction_leaf, :max_depth, :min_impurity_decrease, :init, :random_state, :max_features, :alpha, :verbose, :max_leaf_nodes, :warm_start, :validation_fraction, :n_iter_no_change, :tol)`"
":hyperparameter_types" = "`(\"String\", \"Float64\", \"Int64\", \"Float64\", \"String\", \"Union{Float64, Int64}\", \"Union{Float64, Int64}\", \"Float64\", \"Int64\", \"Float64\", \"Any\", \"Any\", \"Union{Nothing, Float64, Int64, String}\", \"Float64\", \"Int64\", \"Union{Nothing, Int64}\", \"Bool\", \"Float64\", \"Union{Nothing, Int64}\", \"Float64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJScikitLearnInterface.SVMClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "MLJScikitLearnInterface"
":package_license" = "BSD"
":load_path" = "MLJScikitLearnInterface.SVMClassifier"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nSVMClassifier\n```\n\nA model type for constructing a C-support vector classifier, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nSVMClassifier = @load SVMClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = SVMClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`SVMClassifier(C=...)`.\n# Hyper-parameters\n\n- `C = 1.0`\n\n- `kernel = rbf`\n\n- `degree = 3`\n\n- `gamma = scale`\n\n- `coef0 = 0.0`\n\n- `shrinking = true`\n\n- `tol = 0.001`\n\n- `cache_size = 200`\n\n- `max_iter = -1`\n\n- `decision_function_shape = ovr`\n\n- `random_state = nothing`\n\n"
":name" = "SVMClassifier"
":human_name" = "C-support vector classifier"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:C, :kernel, :degree, :gamma, :coef0, :shrinking, :tol, :cache_size, :max_iter, :decision_function_shape, :random_state)`"
":hyperparameter_types" = "`(\"Float64\", \"Union{Function, String}\", \"Int64\", \"Union{Float64, String}\", \"Float64\", \"Bool\", \"Float64\", \"Int64\", \"Int64\", \"String\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionNeighbors.ABODDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "OutlierDetectionNeighbors"
":package_license" = "MIT"
":load_path" = "OutlierDetectionNeighbors.ABODDetector"
":package_uuid" = "51249a0a-cb36-4849-8e04-30c7f8d311bb"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionNeighbors.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nABODDetector(k = 5,\n             metric = Euclidean(),\n             algorithm = :kdtree,\n             static = :auto,\n             leafsize = 10,\n             reorder = true,\n             parallel = false,\n             enhanced = false)\n```\n\nDetermine outliers based on the angles to its nearest neighbors. This implements the `FastABOD` variant described in the paper, that is, it uses the variance of angles to its nearest neighbors, not to the whole dataset, see [1]. \n\n*Notice:* The scores are inverted, to conform to our notion that higher scores describe higher outlierness.\n\n## Parameters\n\n```\nk::Integer\n```\n\nNumber of neighbors (must be greater than 0).\n\n```\nmetric::Metric\n```\n\nThis is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric.\n\n```\nalgorithm::Symbol\n```\n\nOne of `(:kdtree, :balltree)`. In a `kdtree`, points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A *brutetree* linearly searches all points in a brute force fashion and works with any Metric. A *balltree* recursively splits points into groups bounded by hyper-spheres and works with any Metric.\n\n```\nstatic::Union{Bool, Symbol}\n```\n\nOne of `(true, false, :auto)`. Whether the input data for fitting and transform should be statically or dynamically allocated. If `true`, the data is statically allocated. If `false`, the data is dynamically allocated. If `:auto`, the data is dynamically allocated if the product of all dimensions except the last is greater than 100.\n\n```\nleafsize::Int\n```\n\nDetermines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points.\n\n```\nreorder::Bool\n```\n\nWhile building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true.\n\n```\nparallel::Bool\n```\n\nParallelize `score` and `predict` using all threads available. The number of threads can be set with the `JULIA_NUM_THREADS` environment variable. Note: `fit` is not parallel.\n\n```\nenhanced::Bool\n```\n\nWhen `enhanced=true`, it uses the enhanced ABOD (EABOD) adaptation proposed by [2].\n\n## Examples\n\n```julia\nusing OutlierDetection: ABODDetector, fit, transform\ndetector = ABODDetector()\nX = rand(10, 100)\nmodel, result = fit(detector, X; verbosity=0)\ntest_scores = transform(detector, model, X)\n```\n\n## References\n\n[1] Kriegel, Hans-Peter; S hubert, Matthias; Zimek, Arthur (2008): Angle-based outlier detection in high-dimensional data.\n\n[2] Li, Xiaojie; Lv, Jian Cheng; Cheng, Dongdong (2015): Angle-Based Outlier Detection Algorithm with More Stable Relationships.\n"
":name" = "ABODDetector"
":human_name" = "abod detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:k, :metric, :algorithm, :static, :leafsize, :reorder, :parallel, :enhanced)`"
":hyperparameter_types" = "`(\"Integer\", \"Distances.Metric\", \"Symbol\", \"Union{Bool, Symbol}\", \"Integer\", \"Bool\", \"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionNeighbors.DNNDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "OutlierDetectionNeighbors"
":package_license" = "MIT"
":load_path" = "OutlierDetectionNeighbors.DNNDetector"
":package_uuid" = "51249a0a-cb36-4849-8e04-30c7f8d311bb"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionNeighbors.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nDNNDetector(d = 0,\n            metric = Euclidean(),\n            algorithm = :kdtree,\n            leafsize = 10,\n            reorder = true,\n            parallel = false)\n```\n\nAnomaly score based on the number of neighbors in a hypersphere of radius `d`. Knorr et al. [1] directly converted the resulting outlier scores to labels, thus this implementation does not fully reflect the approach from the paper.\n\n## Parameters\n\n```\nd::Real\n```\n\nThe hypersphere radius used to calculate the global density of an instance.\n\n```\nmetric::Metric\n```\n\nThis is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric.\n\n```\nalgorithm::Symbol\n```\n\nOne of `(:kdtree, :balltree)`. In a `kdtree`, points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A *brutetree* linearly searches all points in a brute force fashion and works with any Metric. A *balltree* recursively splits points into groups bounded by hyper-spheres and works with any Metric.\n\n```\nstatic::Union{Bool, Symbol}\n```\n\nOne of `(true, false, :auto)`. Whether the input data for fitting and transform should be statically or dynamically allocated. If `true`, the data is statically allocated. If `false`, the data is dynamically allocated. If `:auto`, the data is dynamically allocated if the product of all dimensions except the last is greater than 100.\n\n```\nleafsize::Int\n```\n\nDetermines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points.\n\n```\nreorder::Bool\n```\n\nWhile building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true.\n\n```\nparallel::Bool\n```\n\nParallelize `score` and `predict` using all threads available. The number of threads can be set with the `JULIA_NUM_THREADS` environment variable. Note: `fit` is not parallel.\n\n## Examples\n\n```julia\nusing OutlierDetection: DNNDetector, fit, transform\ndetector = DNNDetector()\nX = rand(10, 100)\nmodel, result = fit(detector, X; verbosity=0)\ntest_scores = transform(detector, model, X)\n```\n\n## References\n\n[1] Knorr, Edwin M.; Ng, Raymond T. (1998): Algorithms for Mining Distance-Based Outliers in Large Datasets.\n"
":name" = "DNNDetector"
":human_name" = "dnn detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:metric, :algorithm, :static, :leafsize, :reorder, :parallel, :d)`"
":hyperparameter_types" = "`(\"Distances.Metric\", \"Symbol\", \"Union{Bool, Symbol}\", \"Integer\", \"Bool\", \"Bool\", \"Real\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionNeighbors.LOFDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "OutlierDetectionNeighbors"
":package_license" = "MIT"
":load_path" = "OutlierDetectionNeighbors.LOFDetector"
":package_uuid" = "51249a0a-cb36-4849-8e04-30c7f8d311bb"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionNeighbors.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLOFDetector(k = 5,\n            metric = Euclidean(),\n            algorithm = :kdtree,\n            leafsize = 10,\n            reorder = true,\n            parallel = false)\n```\n\nCalculate an anomaly score based on the density of an instance in comparison to its neighbors. This algorithm introduced the notion of local outliers and was developed by Breunig et al., see [1].\n\n## Parameters\n\n```\nk::Integer\n```\n\nNumber of neighbors (must be greater than 0).\n\n```\nmetric::Metric\n```\n\nThis is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric.\n\n```\nalgorithm::Symbol\n```\n\nOne of `(:kdtree, :balltree)`. In a `kdtree`, points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A *brutetree* linearly searches all points in a brute force fashion and works with any Metric. A *balltree* recursively splits points into groups bounded by hyper-spheres and works with any Metric.\n\n```\nstatic::Union{Bool, Symbol}\n```\n\nOne of `(true, false, :auto)`. Whether the input data for fitting and transform should be statically or dynamically allocated. If `true`, the data is statically allocated. If `false`, the data is dynamically allocated. If `:auto`, the data is dynamically allocated if the product of all dimensions except the last is greater than 100.\n\n```\nleafsize::Int\n```\n\nDetermines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points.\n\n```\nreorder::Bool\n```\n\nWhile building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true.\n\n```\nparallel::Bool\n```\n\nParallelize `score` and `predict` using all threads available. The number of threads can be set with the `JULIA_NUM_THREADS` environment variable. Note: `fit` is not parallel.\n\n## Examples\n\n```julia\nusing OutlierDetection: LOFDetector, fit, transform\ndetector = LOFDetector()\nX = rand(10, 100)\nmodel, result = fit(detector, X; verbosity=0)\ntest_scores = transform(detector, model, X)\n```\n\n## References\n\n[1] Breunig, Markus M.; Kriegel, Hans-Peter; Ng, Raymond T.; Sander, Jörg (2000): LOF: Identifying Density-Based Local Outliers.\n"
":name" = "LOFDetector"
":human_name" = "lof detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:k, :metric, :algorithm, :static, :leafsize, :reorder, :parallel)`"
":hyperparameter_types" = "`(\"Integer\", \"Distances.Metric\", \"Symbol\", \"Union{Bool, Symbol}\", \"Integer\", \"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionNeighbors.KNNDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "OutlierDetectionNeighbors"
":package_license" = "MIT"
":load_path" = "OutlierDetectionNeighbors.KNNDetector"
":package_uuid" = "51249a0a-cb36-4849-8e04-30c7f8d311bb"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionNeighbors.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nKNNDetector(k=5,\n            metric=Euclidean,\n            algorithm=:kdtree,\n            leafsize=10,\n            reorder=true,\n            reduction=:maximum)\n```\n\nCalculate the anomaly score of an instance based on the distance to its k-nearest neighbors.\n\n## Parameters\n\n```\nk::Integer\n```\n\nNumber of neighbors (must be greater than 0).\n\n```\nmetric::Metric\n```\n\nThis is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric.\n\n```\nalgorithm::Symbol\n```\n\nOne of `(:kdtree, :balltree)`. In a `kdtree`, points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A *brutetree* linearly searches all points in a brute force fashion and works with any Metric. A *balltree* recursively splits points into groups bounded by hyper-spheres and works with any Metric.\n\n```\nstatic::Union{Bool, Symbol}\n```\n\nOne of `(true, false, :auto)`. Whether the input data for fitting and transform should be statically or dynamically allocated. If `true`, the data is statically allocated. If `false`, the data is dynamically allocated. If `:auto`, the data is dynamically allocated if the product of all dimensions except the last is greater than 100.\n\n```\nleafsize::Int\n```\n\nDetermines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points.\n\n```\nreorder::Bool\n```\n\nWhile building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true.\n\n```\nparallel::Bool\n```\n\nParallelize `score` and `predict` using all threads available. The number of threads can be set with the `JULIA_NUM_THREADS` environment variable. Note: `fit` is not parallel.\n\n```\nreduction::Symbol\n```\n\nOne of `(:maximum, :median, :mean)`. (`reduction=:maximum`) was proposed by [1]. Angiulli et al. [2] proposed sum to reduce the distances, but mean has been implemented for numerical stability.\n\n## Examples\n\n```julia\nusing OutlierDetection: KNNDetector, fit, transform\ndetector = KNNDetector()\nX = rand(10, 100)\nmodel, result = fit(detector, X; verbosity=0)\ntest_scores = transform(detector, model, X)\n```\n\n## References\n\n[1] Ramaswamy, Sridhar; Rastogi, Rajeev; Shim, Kyuseok (2000): Efficient Algorithms for Mining Outliers from Large Data Sets.\n\n[2] Angiulli, Fabrizio; Pizzuti, Clara (2002): Fast Outlier Detection in High Dimensional Spaces.\n"
":name" = "KNNDetector"
":human_name" = "knn detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:k, :metric, :algorithm, :static, :leafsize, :reorder, :parallel, :reduction)`"
":hyperparameter_types" = "`(\"Integer\", \"Distances.Metric\", \"Symbol\", \"Union{Bool, Symbol}\", \"Integer\", \"Bool\", \"Bool\", \"Symbol\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionNeighbors.COFDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "OutlierDetectionNeighbors"
":package_license" = "MIT"
":load_path" = "OutlierDetectionNeighbors.COFDetector"
":package_uuid" = "51249a0a-cb36-4849-8e04-30c7f8d311bb"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionNeighbors.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nCOFDetector(k = 5,\n            metric = Euclidean(),\n            algorithm = :kdtree,\n            leafsize = 10,\n            reorder = true,\n            parallel = false)\n```\n\nLocal outlier density based on chaining distance between graphs of neighbors, as described in [1].\n\n## Parameters\n\n```\nk::Integer\n```\n\nNumber of neighbors (must be greater than 0).\n\n```\nmetric::Metric\n```\n\nThis is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric.\n\n```\nalgorithm::Symbol\n```\n\nOne of `(:kdtree, :balltree)`. In a `kdtree`, points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A *brutetree* linearly searches all points in a brute force fashion and works with any Metric. A *balltree* recursively splits points into groups bounded by hyper-spheres and works with any Metric.\n\n```\nstatic::Union{Bool, Symbol}\n```\n\nOne of `(true, false, :auto)`. Whether the input data for fitting and transform should be statically or dynamically allocated. If `true`, the data is statically allocated. If `false`, the data is dynamically allocated. If `:auto`, the data is dynamically allocated if the product of all dimensions except the last is greater than 100.\n\n```\nleafsize::Int\n```\n\nDetermines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points.\n\n```\nreorder::Bool\n```\n\nWhile building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true.\n\n```\nparallel::Bool\n```\n\nParallelize `score` and `predict` using all threads available. The number of threads can be set with the `JULIA_NUM_THREADS` environment variable. Note: `fit` is not parallel.\n\n## Examples\n\n```julia\nusing OutlierDetection: COFDetector, fit, transform\ndetector = COFDetector()\nX = rand(10, 100)\nmodel, result = fit(detector, X; verbosity=0)\ntest_scores = transform(detector, model, X)\n```\n\n## References\n\n[1] Tang, Jian; Chen, Zhixiang; Fu, Ada Wai-Chee; Cheung, David Wai-Lok (2002): Enhancing Effectiveness of Outlier Detections for Low Density Patterns.\n"
":name" = "COFDetector"
":human_name" = "cof detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:k, :metric, :algorithm, :static, :leafsize, :reorder, :parallel)`"
":hyperparameter_types" = "`(\"Integer\", \"Distances.Metric\", \"Symbol\", \"Union{Bool, Symbol}\", \"Integer\", \"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[PartialLeastSquaresRegressor.KPLSRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "PartialLeastSquaresRegressor"
":package_license" = "MIT"
":load_path" = "PartialLeastSquaresRegressor.KPLSRegressor"
":package_uuid" = "f4b1acfe-f311-436c-bb79-8483f53c17d5"
":package_url" = "https://github.com/lalvim/PartialLeastSquaresRegressor.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "A Kernel Partial Least Squares Regressor. A Kernel PLS2 NIPALS algorithms. Can be used mainly for regression."
":name" = "KPLSRegressor"
":human_name" = "kpls regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":predict"]
":hyperparameters" = "`(:n_factors, :kernel, :width)`"
":hyperparameter_types" = "`(\"Integer\", \"String\", \"Real\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[PartialLeastSquaresRegressor.PLSRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "PartialLeastSquaresRegressor"
":package_license" = "MIT"
":load_path" = "PartialLeastSquaresRegressor.PLSRegressor"
":package_uuid" = "f4b1acfe-f311-436c-bb79-8483f53c17d5"
":package_url" = "https://github.com/lalvim/PartialLeastSquaresRegressor.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "A Partial Least Squares Regressor. Contains PLS1, PLS2 (multi target) algorithms. Can be used mainly for regression."
":name" = "PLSRegressor"
":human_name" = "pls regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":predict"]
":hyperparameters" = "`(:n_factors,)`"
":hyperparameter_types" = "`(\"Int64\",)`"
":hyperparameter_ranges" = "`(nothing,)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJLinearModels.QuantileRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJLinearModels"
":package_license" = "MIT"
":load_path" = "MLJLinearModels.QuantileRegressor"
":package_uuid" = "6ee0df7b-362f-4a72-a706-9e79364fb692"
":package_url" = "https://github.com/alan-turing-institute/MLJLinearModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nQuantileRegressor\n```\n\nA model type for constructing a quantile regressor, based on [MLJLinearModels.jl](https://github.com/alan-turing-institute/MLJLinearModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nQuantileRegressor = @load QuantileRegressor pkg=MLJLinearModels\n```\n\nDo `model = QuantileRegressor()` to construct an instance with default hyper-parameters.\n\nThis model coincides with [`RobustRegressor`](@ref), with the exception that the robust loss, `rho`, is fixed to `QuantileRho(delta)`, where `delta` is a new hyperparameter.\n\nDifferent solver options exist, as indicated under \"Hyperparameters\" below. \n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns have `Continuous` scitype; check column scitypes with `schema(X)`\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `Continuous`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyperparameters\n\n  * `delta::Real`: parameterizes the `QuantileRho` function (indicating the quantile to use     with default `0.5` for the median regression) Default: 0.5\n  * `lambda::Real`: strength of the regularizer if `penalty` is `:l2` or `:l1`.     Strength of the L2 regularizer if `penalty` is `:en`. Default: 1.0\n  * `gamma::Real`: strength of the L1 regularizer if `penalty` is `:en`. Default: 0.0\n  * `penalty::Union{String, Symbol}`: the penalty to use, either `:l2`, `:l1`, `:en` (elastic net) or `:none`. Default: :l2\n  * `fit_intercept::Bool`: whether to fit the intercept or not. Default: true\n  * `penalize_intercept::Bool`: whether to penalize the intercept. Default: false\n  * `scale_penalty_with_samples::Bool`: whether to scale the penalty with the number of observations. Default: true\n  * `solver::Union{Nothing, MLJLinearModels.Solver}`: some instance of `MLJLinearModels.S` where `S` is one of: `LBFGS`, `IWLSCG`, if `penalty = :l2`, and `ProxGrad` otherwise.\n\n    If `solver = nothing` (default) then `LBFGS()` is used, if `penalty = :l2`, and otherwise `ProxGrad(accel=true)` (FISTA) is used.\n\n    Solver aliases: `FISTA(; kwargs...) = ProxGrad(accel=true, kwargs...)`, `ISTA(; kwargs...) = ProxGrad(accel=false, kwargs...)` Default: nothing\n\n## Example\n\n```\nusing MLJ\nX, y = make_regression()\nmach = fit!(machine(QuantileRegressor(), X, y))\npredict(mach, X)\nfitted_params(mach)\n```\n\nSee also [`RobustRegressor`](@ref), [`HuberRegressor`](@ref).\n"
":name" = "QuantileRegressor"
":human_name" = "quantile regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":fitted_params", ":predict", ":QuantileRegressor"]
":hyperparameters" = "`(:delta, :lambda, :gamma, :penalty, :fit_intercept, :penalize_intercept, :scale_penalty_with_samples, :solver)`"
":hyperparameter_types" = "`(\"Real\", \"Real\", \"Real\", \"Union{String, Symbol}\", \"Bool\", \"Bool\", \"Bool\", \"Union{Nothing, MLJLinearModels.Solver}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJLinearModels.LogisticClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJLinearModels"
":package_license" = "MIT"
":load_path" = "MLJLinearModels.LogisticClassifier"
":package_uuid" = "6ee0df7b-362f-4a72-a706-9e79364fb692"
":package_url" = "https://github.com/alan-turing-institute/MLJLinearModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLogisticClassifier\n```\n\nA model type for constructing a logistic classifier, based on [MLJLinearModels.jl](https://github.com/alan-turing-institute/MLJLinearModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nLogisticClassifier = @load LogisticClassifier pkg=MLJLinearModels\n```\n\nDo `model = LogisticClassifier()` to construct an instance with default hyper-parameters.\n\nThis model is more commonly known as \"logistic regression\". It is a standard classifier for both binary and multiclass classification.  The objective function applies either a logistic loss (binary target) or multinomial (softmax) loss, and has a mixed L1/L2 penalty:\n\n$L(y, Xθ) + n⋅λ|θ|₂²/2 + n⋅γ|θ|₁$.\n\nHere $L$ is either `MLJLinearModels.LogisticLoss` or `MLJLinearModels.MultiClassLoss`, $λ$ and $γ$ indicate the strength of the L2 (resp. L1) regularization components and $n$ is the number of training observations.\n\nWith `scale_penalty_with_samples = false` the objective function is instead\n\n$L(y, Xθ) + λ|θ|₂²/2 + γ|θ|₁$.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns have `Continuous` scitype; check column scitypes with `schema(X)`\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `<:OrderedFactor` or `<:Multiclass`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyperparameters\n\n  * `lambda::Real`: strength of the regularizer if `penalty` is `:l2` or `:l1` and strength of the L2     regularizer if `penalty` is `:en`. Default: eps()\n  * `gamma::Real`: strength of the L1 regularizer if `penalty` is `:en`. Default: 0.0\n  * `penalty::Union{String, Symbol}`: the penalty to use, either `:l2`, `:l1`, `:en` (elastic net) or `:none`. Default: :l2\n  * `fit_intercept::Bool`: whether to fit the intercept or not. Default: true\n  * `penalize_intercept::Bool`: whether to penalize the intercept. Default: false\n  * `scale_penalty_with_samples::Bool`: whether to scale the penalty with the number of samples. Default: true\n  * `solver::Union{Nothing, MLJLinearModels.Solver}`: some instance of `MLJLinearModels.S` where `S` is one of: `LBFGS`, `Newton`, `NewtonCG`, `ProxGrad`; but subject to the following restrictions:\n\n      * If `penalty = :l2`, `ProxGrad` is disallowed. Otherwise, `ProxyGrad` is the only option.\n      * Unless `scitype(y) <: Finite{2}` (binary target) `Newton` is disallowed.\n\n    If `solver = nothing` (default) then `ProxGrad(accel=true)` (FISTA) is used, unless `gamma = 0`, in which case `LBFGS()` is used.\n\n    Solver aliases: `FISTA(; kwargs...) = ProxGrad(accel=true, kwargs...)`, `ISTA(; kwargs...) = ProxGrad(accel=false, kwargs...)` Default: nothing\n\n## Example\n\n```\nusing MLJ\nX, y = make_blobs(centers = 2)\nmach = fit!(machine(LogisticClassifier(), X, y))\npredict(mach, X)\nfitted_params(mach)\n```\n\nSee also [`MultinomialClassifier`](@ref).\n"
":name" = "LogisticClassifier"
":human_name" = "logistic classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":fit", ":fitted_params", ":predict", ":LogisticClassifier"]
":hyperparameters" = "`(:lambda, :gamma, :penalty, :fit_intercept, :penalize_intercept, :scale_penalty_with_samples, :solver)`"
":hyperparameter_types" = "`(\"Real\", \"Real\", \"Union{String, Symbol}\", \"Bool\", \"Bool\", \"Bool\", \"Union{Nothing, MLJLinearModels.Solver}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJLinearModels.MultinomialClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJLinearModels"
":package_license" = "MIT"
":load_path" = "MLJLinearModels.MultinomialClassifier"
":package_uuid" = "6ee0df7b-362f-4a72-a706-9e79364fb692"
":package_url" = "https://github.com/alan-turing-institute/MLJLinearModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nMultinomialClassifier\n```\n\nA model type for constructing a multinomial classifier, based on [MLJLinearModels.jl](https://github.com/alan-turing-institute/MLJLinearModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nMultinomialClassifier = @load MultinomialClassifier pkg=MLJLinearModels\n```\n\nDo `model = MultinomialClassifier()` to construct an instance with default hyper-parameters.\n\nThis model coincides with [`LogisticClassifier`](@ref), except certain optimizations possible in the special binary case will not be applied. Its hyperparameters are identical.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns have `Continuous` scitype; check column scitypes with `schema(X)`\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `<:OrderedFactor` or `<:Multiclass`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyperparameters\n\n  * `lambda::Real`: strength of the regularizer if `penalty` is `:l2` or `:l1`.     Strength of the L2 regularizer if `penalty` is `:en`. Default: eps()\n  * `gamma::Real`: strength of the L1 regularizer if `penalty` is `:en`. Default: 0.0\n  * `penalty::Union{String, Symbol}`: the penalty to use, either `:l2`, `:l1`, `:en` (elastic net) or `:none`. Default: :l2\n  * `fit_intercept::Bool`: whether to fit the intercept or not. Default: true\n  * `penalize_intercept::Bool`: whether to penalize the intercept. Default: false\n  * `scale_penalty_with_samples::Bool`: whether to scale the penalty with the number of samples. Default: true\n  * `solver::Union{Nothing, MLJLinearModels.Solver}`: some instance of `MLJLinearModels.S` where `S` is one of: `LBFGS`, `NewtonCG`, `ProxGrad`; but subject to the following restrictions:\n\n      * If `penalty = :l2`, `ProxGrad` is disallowed. Otherwise, `ProxyGrad` is the only option.\n      * Unless `scitype(y) <: Finite{2}` (binary target) `Newton` is disallowed.\n\n    If `solver = nothing` (default) then `ProxGrad(accel=true)` (FISTA) is used, unless `gamma = 0`, in which case `LBFGS()` is used.\n\n    Solver aliases: `FISTA(; kwargs...) = ProxGrad(accel=true, kwargs...)`, `ISTA(; kwargs...) = ProxGrad(accel=false, kwargs...)` Default: nothing\n\n## Example\n\n```\nusing MLJ\nX, y = make_blobs(centers = 3)\nmach = fit!(machine(MultinomialClassifier(), X, y))\npredict(mach, X)\nfitted_params(mach)\n```\n\nSee also [`LogisticClassifier`](@ref).\n"
":name" = "MultinomialClassifier"
":human_name" = "multinomial classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":fit", ":fitted_params", ":predict", ":MultinomialClassifier"]
":hyperparameters" = "`(:lambda, :gamma, :penalty, :fit_intercept, :penalize_intercept, :scale_penalty_with_samples, :solver)`"
":hyperparameter_types" = "`(\"Real\", \"Real\", \"Union{String, Symbol}\", \"Bool\", \"Bool\", \"Bool\", \"Union{Nothing, MLJLinearModels.Solver}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJLinearModels.LADRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJLinearModels"
":package_license" = "MIT"
":load_path" = "MLJLinearModels.LADRegressor"
":package_uuid" = "6ee0df7b-362f-4a72-a706-9e79364fb692"
":package_url" = "https://github.com/alan-turing-institute/MLJLinearModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLADRegressor\n```\n\nA model type for constructing a lad regressor, based on [MLJLinearModels.jl](https://github.com/alan-turing-institute/MLJLinearModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nLADRegressor = @load LADRegressor pkg=MLJLinearModels\n```\n\nDo `model = LADRegressor()` to construct an instance with default hyper-parameters.\n\nLeast absolute deviation regression is a linear model with objective function\n\n$∑ρ(Xθ - y) + n⋅λ|θ|₂² + n⋅γ|θ|₁$\n\nwhere $ρ$ is the absolute loss and $n$ is the number of observations.\n\nIf `scale_penalty_with_samples = false` the objective function is instead\n\n$∑ρ(Xθ - y) + λ|θ|₂² + γ|θ|₁$.\n\nDifferent solver options exist, as indicated under \"Hyperparameters\" below. \n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns have `Continuous` scitype; check column scitypes with `schema(X)`\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `Continuous`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyperparameters\n\nSee also `RobustRegressor`.\n\n## Parameters\n\n  * `lambda::Real`: strength of the regularizer if `penalty` is `:l2` or `:l1`.     Strength of the L2 regularizer if `penalty` is `:en`. Default: 1.0\n  * `gamma::Real`: strength of the L1 regularizer if `penalty` is `:en`. Default: 0.0\n  * `penalty::Union{String, Symbol}`: the penalty to use, either `:l2`, `:l1`, `:en` (elastic net) or `:none`. Default: :l2\n  * `fit_intercept::Bool`: whether to fit the intercept or not. Default: true\n  * `penalize_intercept::Bool`: whether to penalize the intercept. Default: false\n  * `scale_penalty_with_samples::Bool`: whether to scale the penalty with the number of observations. Default: true\n  * `solver::Union{Nothing, MLJLinearModels.Solver}`: some instance of `MLJLinearModels.S` where `S` is one of: `LBFGS`, `IWLSCG`, if `penalty = :l2`, and `ProxGrad` otherwise.\n\n    If `solver = nothing` (default) then `LBFGS()` is used, if `penalty = :l2`, and otherwise `ProxGrad(accel=true)` (FISTA) is used.\n\n    Solver aliases: `FISTA(; kwargs...) = ProxGrad(accel=true, kwargs...)`, `ISTA(; kwargs...) = ProxGrad(accel=false, kwargs...)` Default: nothing\n\n## Example\n\n```\nusing MLJ\nX, y = make_regression()\nmach = fit!(machine(LADRegressor(), X, y))\npredict(mach, X)\nfitted_params(mach)\n```\n"
":name" = "LADRegressor"
":human_name" = "least absolute deviation regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":fitted_params", ":predict", ":LADRegressor"]
":hyperparameters" = "`(:lambda, :gamma, :penalty, :fit_intercept, :penalize_intercept, :scale_penalty_with_samples, :solver)`"
":hyperparameter_types" = "`(\"Real\", \"Real\", \"Union{String, Symbol}\", \"Bool\", \"Bool\", \"Bool\", \"Union{Nothing, MLJLinearModels.Solver}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJLinearModels.RidgeRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJLinearModels"
":package_license" = "MIT"
":load_path" = "MLJLinearModels.RidgeRegressor"
":package_uuid" = "6ee0df7b-362f-4a72-a706-9e79364fb692"
":package_url" = "https://github.com/alan-turing-institute/MLJLinearModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nRidgeRegressor\n```\n\nA model type for constructing a ridge regressor, based on [MLJLinearModels.jl](https://github.com/alan-turing-institute/MLJLinearModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nRidgeRegressor = @load RidgeRegressor pkg=MLJLinearModels\n```\n\nDo `model = RidgeRegressor()` to construct an instance with default hyper-parameters.\n\nRidge regression is a linear model with objective function\n\n$|Xθ - y|₂²/2 + n⋅λ|θ|₂²/2$\n\nwhere $n$ is the number of observations.\n\nIf `scale_penalty_with_samples = false` then the objective function is instead\n\n$|Xθ - y|₂²/2 + λ|θ|₂²/2$.\n\nDifferent solver options exist, as indicated under \"Hyperparameters\" below. \n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns have `Continuous` scitype; check column scitypes with `schema(X)`\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `Continuous`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyperparameters\n\n  * `lambda::Real`: strength of the L2 regularization. Default: 1.0\n  * `fit_intercept::Bool`: whether to fit the intercept or not. Default: true\n  * `penalize_intercept::Bool`: whether to penalize the intercept. Default: false\n  * `scale_penalty_with_samples::Bool`: whether to scale the penalty with the number of observations. Default: true\n  * `solver::Union{Nothing, MLJLinearModels.Solver}`: any instance of `MLJLinearModels.Analytical`. Use `Analytical()` for Cholesky and `CG()=Analytical(iteration=true)` for conjugate-gradient. If `solver = nothing` (default) then `Analytical()` is used.  Default: nothing\n\n## Example\n\n```\nusing MLJ\nX, y = make_regression()\nmach = fit!(machine(RidgeRegressor(), X, y))\npredict(mach, X)\nfitted_params(mach)\n```\n\nSee also [`ElasticNetRegressor`](@ref).\n"
":name" = "RidgeRegressor"
":human_name" = "ridge regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":fitted_params", ":predict", ":RidgeRegressor"]
":hyperparameters" = "`(:lambda, :fit_intercept, :penalize_intercept, :scale_penalty_with_samples, :solver)`"
":hyperparameter_types" = "`(\"Real\", \"Bool\", \"Bool\", \"Bool\", \"Union{Nothing, MLJLinearModels.Solver}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJLinearModels.RobustRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJLinearModels"
":package_license" = "MIT"
":load_path" = "MLJLinearModels.RobustRegressor"
":package_uuid" = "6ee0df7b-362f-4a72-a706-9e79364fb692"
":package_url" = "https://github.com/alan-turing-institute/MLJLinearModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nRobustRegressor\n```\n\nA model type for constructing a robust regressor, based on [MLJLinearModels.jl](https://github.com/alan-turing-institute/MLJLinearModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nRobustRegressor = @load RobustRegressor pkg=MLJLinearModels\n```\n\nDo `model = RobustRegressor()` to construct an instance with default hyper-parameters.\n\nRobust regression is a linear model with objective function\n\n$∑ρ(Xθ - y) + n⋅λ|θ|₂² + n⋅γ|θ|₁$\n\nwhere $ρ$ is a robust loss function (e.g. the Huber function) and $n$ is the number of observations.\n\nIf `scale_penalty_with_samples = false` the objective function is instead\n\n$∑ρ(Xθ - y) + λ|θ|₂² + γ|θ|₁$.\n\nDifferent solver options exist, as indicated under \"Hyperparameters\" below. \n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns have `Continuous` scitype; check column scitypes with `schema(X)`\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `Continuous`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyperparameters\n\n  * `rho::MLJLinearModels.RobustRho`: the type of robust loss, which can be any instance of     `MLJLinearModels.L` where `L` is one of: `AndrewsRho`,     `BisquareRho`, `FairRho`, `HuberRho`, `LogisticRho`,     `QuantileRho`, `TalwarRho`, `HuberRho`, `TalwarRho`.  Default: HuberRho(0.1)\n  * `lambda::Real`: strength of the regularizer if `penalty` is `:l2` or `:l1`.     Strength of the L2 regularizer if `penalty` is `:en`. Default: 1.0\n  * `gamma::Real`: strength of the L1 regularizer if `penalty` is `:en`. Default: 0.0\n  * `penalty::Union{String, Symbol}`: the penalty to use, either `:l2`, `:l1`, `:en` (elastic net) or `:none`. Default: :l2\n  * `fit_intercept::Bool`: whether to fit the intercept or not. Default: true\n  * `penalize_intercept::Bool`: whether to penalize the intercept. Default: false\n  * `scale_penalty_with_samples::Bool`: whether to scale the penalty with the number of observations. Default: true\n  * `solver::Union{Nothing, MLJLinearModels.Solver}`: some instance of `MLJLinearModels.S` where `S` is one of: `LBFGS`, `IWLSCG`, `Newton`, `NewtonCG`, if `penalty = :l2`, and `ProxGrad` otherwise.\n\n    If `solver = nothing` (default) then `LBFGS()` is used, if `penalty = :l2`, and otherwise `ProxGrad(accel=true)` (FISTA) is used.\n\n    Solver aliases: `FISTA(; kwargs...) = ProxGrad(accel=true, kwargs...)`, `ISTA(; kwargs...) = ProxGrad(accel=false, kwargs...)` Default: nothing\n\n## Example\n\n```\nusing MLJ\nX, y = make_regression()\nmach = fit!(machine(RobustRegressor(), X, y))\npredict(mach, X)\nfitted_params(mach)\n```\n\nSee also [`HuberRegressor`](@ref), [`QuantileRegressor`](@ref).\n"
":name" = "RobustRegressor"
":human_name" = "robust regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":fitted_params", ":predict", ":RobustRegressor"]
":hyperparameters" = "`(:rho, :lambda, :gamma, :penalty, :fit_intercept, :penalize_intercept, :scale_penalty_with_samples, :solver)`"
":hyperparameter_types" = "`(\"MLJLinearModels.RobustRho\", \"Real\", \"Real\", \"Union{String, Symbol}\", \"Bool\", \"Bool\", \"Bool\", \"Union{Nothing, MLJLinearModels.Solver}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJLinearModels.ElasticNetRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJLinearModels"
":package_license" = "MIT"
":load_path" = "MLJLinearModels.ElasticNetRegressor"
":package_uuid" = "6ee0df7b-362f-4a72-a706-9e79364fb692"
":package_url" = "https://github.com/alan-turing-institute/MLJLinearModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nElasticNetRegressor\n```\n\nA model type for constructing a elastic net regressor, based on [MLJLinearModels.jl](https://github.com/alan-turing-institute/MLJLinearModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nElasticNetRegressor = @load ElasticNetRegressor pkg=MLJLinearModels\n```\n\nDo `model = ElasticNetRegressor()` to construct an instance with default hyper-parameters.\n\nElastic net is a linear model with objective function\n\n$|Xθ - y|₂²/2 + n⋅λ|θ|₂²/2 + n⋅γ|θ|₁$\n\nwhere $n$ is the number of observations.\n\nIf  `scale_penalty_with_samples = false` the objective function is instead\n\n$|Xθ - y|₂²/2 + λ|θ|₂²/2 + γ|θ|₁$.\n\nDifferent solver options exist, as indicated under \"Hyperparameters\" below. \n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns have `Continuous` scitype; check column scitypes with `schema(X)`\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `Continuous`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyperparameters\n\n  * `lambda::Real`: strength of the L2 regularization. Default: 1.0\n  * `gamma::Real`: strength of the L1 regularization. Default: 0.0\n  * `fit_intercept::Bool`: whether to fit the intercept or not. Default: true\n  * `penalize_intercept::Bool`: whether to penalize the intercept. Default: false\n  * `scale_penalty_with_samples::Bool`: whether to scale the penalty with the number of observations. Default: true\n  * `solver::Union{Nothing, MLJLinearModels.Solver}`: any instance of `MLJLinearModels.ProxGrad`.\n\n    If `solver=nothing` (default) then `ProxGrad(accel=true)` (FISTA) is used.\n\n    Solver aliases: `FISTA(; kwargs...) = ProxGrad(accel=true, kwargs...)`, `ISTA(; kwargs...) = ProxGrad(accel=false, kwargs...)`.  Default: nothing\n\n## Example\n\n```\nusing MLJ\nX, y = make_regression()\nmach = fit!(machine(ElasticNetRegressor(), X, y))\npredict(mach, X)\nfitted_params(mach)\n```\n\nSee also [`LassoRegressor`](@ref).\n"
":name" = "ElasticNetRegressor"
":human_name" = "elastic net regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":fitted_params", ":predict", ":ElasticNetRegressor"]
":hyperparameters" = "`(:lambda, :gamma, :fit_intercept, :penalize_intercept, :scale_penalty_with_samples, :solver)`"
":hyperparameter_types" = "`(\"Real\", \"Real\", \"Bool\", \"Bool\", \"Bool\", \"Union{Nothing, MLJLinearModels.Solver}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJLinearModels.LinearRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJLinearModels"
":package_license" = "MIT"
":load_path" = "MLJLinearModels.LinearRegressor"
":package_uuid" = "6ee0df7b-362f-4a72-a706-9e79364fb692"
":package_url" = "https://github.com/alan-turing-institute/MLJLinearModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLinearRegressor\n```\n\nA model type for constructing a linear regressor, based on [MLJLinearModels.jl](https://github.com/alan-turing-institute/MLJLinearModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nLinearRegressor = @load LinearRegressor pkg=MLJLinearModels\n```\n\nDo `model = LinearRegressor()` to construct an instance with default hyper-parameters.\n\nThis model provides standard linear regression with objective function\n\n$|Xθ - y|₂²/2$\n\nDifferent solver options exist, as indicated under \"Hyperparameters\" below. \n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns have `Continuous` scitype; check column scitypes with `schema(X)`\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `Continuous`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyperparameters\n\n  * `fit_intercept::Bool`: whether to fit the intercept or not. Default: true\n  * `solver::Union{Nothing, MLJLinearModels.Solver}`: \"any instance of `MLJLinearModels.Analytical`. Use `Analytical()` for Cholesky and `CG()=Analytical(iterative=true)` for conjugate-gradient.\n\n    If `solver = nothing` (default) then `Analytical()` is used.  Default: nothing\n\n## Example\n\n```\nusing MLJ\nX, y = make_regression()\nmach = fit!(machine(LinearRegressor(), X, y))\npredict(mach, X)\nfitted_params(mach)\n```\n"
":name" = "LinearRegressor"
":human_name" = "linear regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":fitted_params", ":predict", ":LinearRegressor"]
":hyperparameters" = "`(:fit_intercept, :solver)`"
":hyperparameter_types" = "`(\"Bool\", \"Union{Nothing, MLJLinearModels.Solver}\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJLinearModels.LassoRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJLinearModels"
":package_license" = "MIT"
":load_path" = "MLJLinearModels.LassoRegressor"
":package_uuid" = "6ee0df7b-362f-4a72-a706-9e79364fb692"
":package_url" = "https://github.com/alan-turing-institute/MLJLinearModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLassoRegressor\n```\n\nA model type for constructing a lasso regressor, based on [MLJLinearModels.jl](https://github.com/alan-turing-institute/MLJLinearModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nLassoRegressor = @load LassoRegressor pkg=MLJLinearModels\n```\n\nDo `model = LassoRegressor()` to construct an instance with default hyper-parameters.\n\nLasso regression is a linear model with objective function\n\n$|Xθ - y|₂²/2 + n⋅λ|θ|₁$\n\nwhere $n$ is the number of observations.\n\nIf `scale_penalty_with_samples = false` the objective function is\n\n$|Xθ - y|₂²/2 + λ|θ|₁$.\n\nDifferent solver options exist, as indicated under \"Hyperparameters\" below. \n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns have `Continuous` scitype; check column scitypes with `schema(X)`\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `Continuous`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyperparameters\n\n  * `lambda::Real`: strength of the L1 regularization. Default: 1.0\n  * `fit_intercept::Bool`: whether to fit the intercept or not. Default: true\n  * `penalize_intercept::Bool`: whether to penalize the intercept. Default: false\n  * `scale_penalty_with_samples::Bool`: whether to scale the penalty with the number of observations. Default: true\n  * `solver::Union{Nothing, MLJLinearModels.Solver}`: any instance of `MLJLinearModels.ProxGrad`. If `solver=nothing` (default) then `ProxGrad(accel=true)` (FISTA) is used. Solver aliases: `FISTA(; kwargs...) = ProxGrad(accel=true, kwargs...)`, `ISTA(; kwargs...) = ProxGrad(accel=false, kwargs...)`.  Default: nothing\n\n## Example\n\n```\nusing MLJ\nX, y = make_regression()\nmach = fit!(machine(LassoRegressor(), X, y))\npredict(mach, X)\nfitted_params(mach)\n```\n\nSee also [`ElasticNetRegressor`](@ref).\n"
":name" = "LassoRegressor"
":human_name" = "lasso regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":fitted_params", ":predict", ":LassoRegressor"]
":hyperparameters" = "`(:lambda, :fit_intercept, :penalize_intercept, :scale_penalty_with_samples, :solver)`"
":hyperparameter_types" = "`(\"Real\", \"Bool\", \"Bool\", \"Bool\", \"Union{Nothing, MLJLinearModels.Solver}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJLinearModels.HuberRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJLinearModels"
":package_license" = "MIT"
":load_path" = "MLJLinearModels.HuberRegressor"
":package_uuid" = "6ee0df7b-362f-4a72-a706-9e79364fb692"
":package_url" = "https://github.com/alan-turing-institute/MLJLinearModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nHuberRegressor\n```\n\nA model type for constructing a huber regressor, based on [MLJLinearModels.jl](https://github.com/alan-turing-institute/MLJLinearModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nHuberRegressor = @load HuberRegressor pkg=MLJLinearModels\n```\n\nDo `model = HuberRegressor()` to construct an instance with default hyper-parameters.\n\nThis model coincides with [`RobustRegressor`](@ref), with the exception that the robust loss, `rho`, is fixed to `HuberRho(delta)`, where `delta` is a new hyperparameter.\n\nDifferent solver options exist, as indicated under \"Hyperparameters\" below. \n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns have `Continuous` scitype; check column scitypes with `schema(X)`\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `Continuous`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyperparameters\n\n  * `delta::Real`: parameterizes the `HuberRho` function (radius of the ball within which the loss     is a quadratic loss) Default: 0.5\n  * `lambda::Real`: strength of the regularizer if `penalty` is `:l2` or `:l1`.     Strength of the L2 regularizer if `penalty` is `:en`. Default: 1.0\n  * `gamma::Real`: strength of the L1 regularizer if `penalty` is `:en`. Default: 0.0\n  * `penalty::Union{String, Symbol}`: the penalty to use, either `:l2`, `:l1`, `:en` (elastic net) or `:none`. Default: :l2\n  * `fit_intercept::Bool`: whether to fit the intercept or not. Default: true\n  * `penalize_intercept::Bool`: whether to penalize the intercept. Default: false\n  * `scale_penalty_with_samples::Bool`: whether to scale the penalty with the number of observations. Default: true\n  * `solver::Union{Nothing, MLJLinearModels.Solver}`: some instance of `MLJLinearModels.S` where `S` is one of: `LBFGS`, `IWLSCG`, `Newton`, `NewtonCG`, if `penalty = :l2`, and `ProxGrad` otherwise.\n\n    If `solver = nothing` (default) then `LBFGS()` is used, if `penalty = :l2`, and otherwise `ProxGrad(accel=true)` (FISTA) is used.\n\n    Solver aliases: `FISTA(; kwargs...) = ProxGrad(accel=true, kwargs...)`, `ISTA(; kwargs...) = ProxGrad(accel=false, kwargs...)` Default: nothing\n\n## Example\n\n```\nusing MLJ\nX, y = make_regression()\nmach = fit!(machine(HuberRegressor(), X, y))\npredict(mach, X)\nfitted_params(mach)\n```\n\nSee also [`RobustRegressor`](@ref), [`QuantileRegressor`](@ref).\n"
":name" = "HuberRegressor"
":human_name" = "huber regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":fitted_params", ":predict", ":HuberRegressor"]
":hyperparameters" = "`(:delta, :lambda, :gamma, :penalty, :fit_intercept, :penalize_intercept, :scale_penalty_with_samples, :solver)`"
":hyperparameter_types" = "`(\"Real\", \"Real\", \"Real\", \"Union{String, Symbol}\", \"Bool\", \"Bool\", \"Bool\", \"Union{Nothing, MLJLinearModels.Solver}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[ParallelKMeans.KMeans]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`AbstractArray{<:ScientificTypesBase.Multiclass}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "ParallelKMeans"
":package_license" = "MIT"
":load_path" = "ParallelKMeans.KMeans"
":package_uuid" = "42b8e9d4-006b-409a-8472-7f34b3fb58af"
":package_url" = "https://github.com/PyDataBlog/ParallelKMeans.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Parallel & lightning fast implementation of all available variants of the KMeans clustering algorithm\n                             in native Julia. Compatible with Julia 1.3+"
":name" = "KMeans"
":human_name" = "k means"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":transform"]
":hyperparameters" = "`(:algo, :k_init, :k, :tol, :max_iters, :copy, :threads, :rng, :weights, :init)`"
":hyperparameter_types" = "`(\"Union{Symbol, ParallelKMeans.AbstractKMeansAlg}\", \"String\", \"Int64\", \"Float64\", \"Int64\", \"Bool\", \"Int64\", \"Union{Int64, Random.AbstractRNG}\", \"Any\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[NaiveBayes.GaussianNBClassifier]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "NaiveBayes"
":package_license" = "unknown"
":load_path" = "MLJNaiveBayesInterface.GaussianNBClassifier"
":package_uuid" = "9bbee03b-0db5-5f46-924f-b5c9c21b8c60"
":package_url" = "https://github.com/dfdx/NaiveBayes.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nGaussianNBClassifier\n```\n\nA model type for constructing a Gaussian naive Bayes classifier, based on [NaiveBayes.jl](https://github.com/dfdx/NaiveBayes.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nGaussianNBClassifier = @load GaussianNBClassifier pkg=NaiveBayes\n```\n\nDo `model = GaussianNBClassifier()` to construct an instance with default hyper-parameters. \n\nGiven each class taken on by the target variable `y`, it is supposed that the conditional probability distribution for the input variables `X` is a multivariate Gaussian. The mean and covariance of these Gaussian distributions are estimated using maximum likelihood, and a probability distribution for `y` given `X` is deduced by applying Bayes' rule. The required marginal for `y` is estimated using class frequency in the training data.\n\n**Important.** The name \"naive Bayes classifier\" is perhaps misleading. Since we are learning the full multivariate Gaussian distributions for `X` given `y`, we are not applying the usual naive Bayes independence condition, which would amount to forcing the covariance matrix to be diagonal.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check the column scitypes with `schema(X)`\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `Finite`; check the scitype with `schema(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given new features `Xnew`, which should have the same scitype as `X` above. Predictions are probabilistic.\n  * `predict_mode(mach, Xnew)`: Return the mode of above predictions.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `c_counts`: A dictionary containing the observed count of each input class.\n  * `c_stats`: A dictionary containing observed statistics on each input class. Each class is represented by a `DataStats` object, with the following fields:\n\n      * `n_vars`: The number of variables used to describe the class's behavior.\n      * `n_obs`: The number of times the class is observed.\n      * `obs_axis`: The axis along which the observations were computed.\n  * `gaussians`: A per class dictionary of Gaussians, each representing the distribution of the class. Represented with type `Distributions.MvNormal` from the Distributions.jl package.\n  * `n_obs`: The total number of observations in the training data.\n\n# Examples\n\n```\nusing MLJ\nGaussianNB = @load GaussianNBClassifier pkg=NaiveBayes\n\nX, y = @load_iris\nclf = GaussianNB()\nmach = machine(clf, X, y) |> fit!\n\nfitted_params(mach)\n\npreds = predict(mach, X) # probabilistic predictions\npreds[1]\npredict_mode(mach, X) # point predictions\n```\n\nSee also [`MultinomialNBClassifier`](@ref)\n"
":name" = "GaussianNBClassifier"
":human_name" = "Gaussian naive Bayes classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`()`"
":hyperparameter_types" = "`()`"
":hyperparameter_ranges" = "`()`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[NaiveBayes.MultinomialNBClassifier]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Count}}, AbstractMatrix{<:ScientificTypesBase.Count}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Count}}, AbstractMatrix{<:ScientificTypesBase.Count}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "NaiveBayes"
":package_license" = "unknown"
":load_path" = "MLJNaiveBayesInterface.MultinomialNBClassifier"
":package_uuid" = "9bbee03b-0db5-5f46-924f-b5c9c21b8c60"
":package_url" = "https://github.com/dfdx/NaiveBayes.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nMultinomialNBClassifier\n```\n\nA model type for constructing a multinomial naive Bayes classifier, based on [NaiveBayes.jl](https://github.com/dfdx/NaiveBayes.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nMultinomialNBClassifier = @load MultinomialNBClassifier pkg=NaiveBayes\n```\n\nDo `model = MultinomialNBClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `MultinomialNBClassifier(alpha=...)`.\n\nThe [multinomial naive Bayes classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Multinomial_naive_Bayes) is often applied when input features consist of a counts (scitype `Count`) and when observations for a fixed target class are generated from a multinomial distribution with fixed probability vector, but whose sample length varies from observation to observation. For example, features might represent word counts in text documents being classified by sentiment.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Count`; check the column scitypes with `schema(X)`.\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `Finite`; check the scitype with `schema(y)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `alpha=1`: Lindstone smoothing in estimation of multinomial probability vectors from training histograms (default corresponds to Laplacian smoothing).\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given new features `Xnew`, which should have the same scitype as `X` above.\n  * `predict_mode(mach, Xnew)`: Return the mode of above predictions.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `c_counts`: A dictionary containing the observed count of each input class.\n  * `x_counts`: A dictionary containing the categorical counts of each input class.\n  * `x_totals`: The sum of each count (input feature), ungrouped.\n  * `n_obs`: The total number of observations in the training data.\n\n# Examples\n\n```\nusing MLJ\nimport TextAnalysis\n\nCountTransformer = @load CountTransformer pkg=MLJText\nMultinomialNBClassifier = @load MultinomialNBClassifier pkg=NaiveBayes\n\ntokenized_docs = TextAnalysis.tokenize.([\n    \"I am very mad. You never listen.\",\n    \"You seem to be having trouble? Can I help you?\",\n    \"Our boss is mad at me. I hope he dies.\",\n    \"His boss wants to help me. She is nice.\",\n    \"Thank you for your help. It is nice working with you.\",\n    \"Never do that again! I am so mad. \",\n])\n\nsentiment = [\n    \"negative\",\n    \"positive\",\n    \"negative\",\n    \"positive\",\n    \"positive\",\n    \"negative\",\n]\n\nmach1 = machine(CountTransformer(), tokenized_docs) |> fit!\n\n# matrix of counts:\nX = transform(mach1, tokenized_docs)\n\n# to ensure scitype(y) <: AbstractVector{<:OrderedFactor}:\ny = coerce(sentiment, OrderedFactor)\n\nclassifier = MultinomialNBClassifier()\nmach2 = machine(classifier, X, y)\nfit!(mach2, rows=1:4)\n\n# probabilistic predictions:\ny_prob = predict(mach2, rows=5:6) # distributions\npdf.(y_prob, \"positive\") # probabilities for \"positive\"\nlog_loss(y_prob, y[5:6])\n\n# point predictions:\nyhat = mode.(y_prob) # or `predict_mode(mach2, rows=5:6)`\n```\n\nSee also [`GaussianNBClassifier`](@ref)\n"
":name" = "MultinomialNBClassifier"
":human_name" = "multinomial naive Bayes classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:alpha,)`"
":hyperparameter_types" = "`(\"Int64\",)`"
":hyperparameter_ranges" = "`(nothing,)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MultivariateStats.LDA]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MultivariateStats"
":package_license" = "MIT"
":load_path" = "MLJMultivariateStatsInterface.LDA"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaStats/MultivariateStats.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLDA\n```\n\nA model type for constructing a linear discriminant analysis model, based on [MultivariateStats.jl](https://github.com/JuliaStats/MultivariateStats.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nLDA = @load LDA pkg=MultivariateStats\n```\n\nDo `model = LDA()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `LDA(method=...)`.\n\n[Multiclass linear discriminant analysis](https://en.wikipedia.org/wiki/Linear_discriminant_analysis) learns a projection in a space of features to a lower dimensional space, in a way that attempts to preserve as much as possible the degree to which the classes of a discrete target variable can be discriminated. This can be used either for dimension reduction of the features (see `transform` below) or for probabilistic classification of the target (see `predict` below).\n\nIn the case of prediction, the class probability for a new observation reflects the proximity of that observation to training observations associated with that class, and how far away the observation is from observations associated with other classes. Specifically, the distances, in the transformed (projected) space, of a new observation, from the centroid of each target class, is computed; the resulting vector of distances, multiplied by minus one, is passed to a softmax function to obtain a class probability prediction. Here \"distance\" is computed using a user-specified distance function.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `OrderedFactor` or `Multiclass`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `method::Symbol=:gevd`: The solver, one of `:gevd` or `:whiten` methods.\n  * `cov_w::StatsBase.SimpleCovariance()`: An estimator for the within-class covariance (used in computing the within-class scatter matrix, `Sw`). Any robust estimator from `CovarianceEstimation.jl` can be used.\n  * `cov_b::StatsBase.SimpleCovariance()`: The same as `cov_w` but for the between-class covariance (used in computing the between-class scatter matrix, `Sb`).\n  * `outdim::Int=0`: The output dimension, i.e dimension of the transformed space, automatically set to `min(indim, nclasses-1)` if equal to 0.\n  * `regcoef::Float64=1e-6`: The regularization coefficient. A positive value `regcoef*eigmax(Sw)` where `Sw` is the within-class scatter matrix, is added to the diagonal of `Sw` to improve numerical stability. This can be useful if using the standard covariance estimator.\n  * `dist=Distances.SqEuclidean()`: The distance metric to use when performing classification (to compare the distance between a new point and centroids in the transformed space); must be a subtype of `Distances.SemiMetric` from Distances.jl, e.g., `Distances.CosineDist`.\n\n# Operations\n\n  * `transform(mach, Xnew)`: Return a lower dimensional projection of the input `Xnew`, which should have the same scitype as `X` above.\n  * `predict(mach, Xnew)`: Return predictions of the target given features `Xnew` having the same scitype as `X` above. Predictions are probabilistic but uncalibrated.\n  * `predict_mode(mach, Xnew)`: Return the modes of the probabilistic predictions returned above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `classes`: The classes seen during model fitting.\n  * `projection_matrix`: The learned projection matrix, of size `(indim, outdim)`, where `indim` and `outdim` are the input and output dimensions respectively (See Report section below).\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `indim`: The dimension of the input space i.e the number of training features.\n  * `outdim`: The dimension of the transformed space the model is projected to.\n  * `mean`: The mean of the untransformed training data. A vector of length `indim`.\n  * `nclasses`: The number of classes directly observed in the training data (which can be less than the total number of classes in the class pool).\n  * `class_means`: The class-specific means of the training data. A matrix of size `(indim, nclasses)` with the ith column being the class-mean of the ith class in `classes` (See fitted params section above).\n  * `class_weights`: The weights (class counts) of each class. A vector of length `nclasses` with the ith element being the class weight of the ith class in `classes`. (See fitted params section above.)\n  * `Sb`: The between class scatter matrix.\n  * `Sw`: The within class scatter matrix.\n\n# Examples\n\n```\nusing MLJ\n\nLDA = @load LDA pkg=MultivariateStats\n\nX, y = @load_iris # a table and a vector\n\nmodel = LDA()\nmach = machine(model, X, y) |> fit!\n\nXproj = transform(mach, X)\ny_hat = predict(mach, X)\nlabels = predict_mode(mach, X)\n\n```\n\nSee also [`BayesianLDA`](@ref), [`SubspaceLDA`](@ref), [`BayesianSubspaceLDA`](@ref)\n"
":name" = "LDA"
":human_name" = "linear discriminant analysis model"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":transform"]
":hyperparameters" = "`(:method, :cov_w, :cov_b, :outdim, :regcoef, :dist)`"
":hyperparameter_types" = "`(\"Symbol\", \"StatsBase.CovarianceEstimator\", \"StatsBase.CovarianceEstimator\", \"Int64\", \"Float64\", \"Distances.SemiMetric\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MultivariateStats.MultitargetLinearRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MultivariateStats"
":package_license" = "MIT"
":load_path" = "MLJMultivariateStatsInterface.MultitargetLinearRegressor"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaStats/MultivariateStats.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nMultitargetLinearRegressor\n```\n\nA model type for constructing a multitarget linear regressor, based on [MultivariateStats.jl](https://github.com/JuliaStats/MultivariateStats.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nMultitargetLinearRegressor = @load MultitargetLinearRegressor pkg=MultivariateStats\n```\n\nDo `model = MultitargetLinearRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `MultitargetLinearRegressor(bias=...)`.\n\n`MultitargetLinearRegressor` assumes the target variable is vector-valued with continuous components.  It trains a linear prediction function using the least squares algorithm. Options exist to specify a bias term.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype    `Continuous`; check column scitypes with `schema(X)`.\n  * `y` is the target, which can be any table of responses whose element scitype is    `Continuous`; check the scitype with `scitype(y)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `bias=true`: Include the bias term if true, otherwise fit without bias term.\n\n# Operations\n\n  * `predict(mach, Xnew)`: Return predictions of the target given new features `Xnew`,    which should have the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `coefficients`: The linear coefficients determined by the model.\n  * `intercept`: The intercept determined by the model.\n\n# Examples\n\n```\nusing MLJ\nusing DataFrames\n\nLinearRegressor = @load MultitargetLinearRegressor pkg=MultivariateStats\nlinear_regressor = LinearRegressor()\n\nX, y = make_regression(100, 9; n_targets = 2) # a table and a table (synthetic data)\n\nmach = machine(linear_regressor, X, y) |> fit!\n\nXnew, _ = make_regression(3, 9)\nyhat = predict(mach, Xnew) # new predictions\n```\n\nSee also [`LinearRegressor`](@ref), [`RidgeRegressor`](@ref), [`MultitargetRidgeRegressor`](@ref)\n"
":name" = "MultitargetLinearRegressor"
":human_name" = "multitarget linear regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:bias,)`"
":hyperparameter_types" = "`(\"Bool\",)`"
":hyperparameter_ranges" = "`(nothing,)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MultivariateStats.BayesianSubspaceLDA]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MultivariateStats"
":package_license" = "MIT"
":load_path" = "MLJMultivariateStatsInterface.BayesianSubspaceLDA"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaStats/MultivariateStats.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nBayesianSubspaceLDA\n```\n\nA model type for constructing a Bayesian subspace LDA model, based on [MultivariateStats.jl](https://github.com/JuliaStats/MultivariateStats.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nBayesianSubspaceLDA = @load BayesianSubspaceLDA pkg=MultivariateStats\n```\n\nDo `model = BayesianSubspaceLDA()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `BayesianSubspaceLDA(normalize=...)`.\n\nThe Bayesian multiclass subspace linear discriminant analysis algorithm learns a projection matrix as described in [`SubspaceLDA`](@ref). The posterior class probability distribution is derived as in [`BayesianLDA`](@ref).\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `OrderedFactor` or `Multiclass`; check the scitype with `scitype(y)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `normalize=true`: Option to normalize the between class variance for the number of observations in each class, one of `true` or `false`.\n\n`outdim`: the ouput dimension, automatically set to `min(indim, nclasses-1)` if equal   to `0`. If a non-zero `outdim` is passed, then the actual output dimension used is   `min(rank, outdim)` where `rank` is the rank of the within-class covariance matrix.\n\n  * `priors::Union{Nothing, UnivariateFinite{<:Any, <:Any, <:Any, <:Real}, Dict{<:Any, <:Real}} = nothing`: For use in prediction with Bayes rule. If `priors = nothing` then `priors` are estimated from the class proportions in the training data. Otherwise it requires a `Dict` or `UnivariateFinite` object specifying the classes with non-zero probabilities in the training target.\n\n# Operations\n\n  * `transform(mach, Xnew)`: Return a lower dimensional projection of the input `Xnew`, which should have the same scitype as `X` above.\n  * `predict(mach, Xnew)`: Return predictions of the target given features `Xnew`, which should have same scitype as `X` above. Predictions are probabilistic but uncalibrated.\n  * `predict_mode(mach, Xnew)`: Return the modes of the probabilistic predictions returned above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `classes`: The classes seen during model fitting.\n  * `projection_matrix`: The learned projection matrix, of size `(indim, outdim)`, where `indim` and `outdim` are the input and output dimensions respectively (See Report section below).\n  * `priors`: The class priors for classification. As inferred from training target `y`, if not user-specified. A `UnivariateFinite` object with levels consistent with `levels(y)`.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `indim`: The dimension of the input space i.e the number of training features.\n  * `outdim`: The dimension of the transformed space the model is projected to.\n  * `mean`: The overall mean of the training data.\n  * `nclasses`: The number of classes directly observed in the training data (which can be less than the total number of classes in the class pool).\n\n`class_means`: The class-specific means of the training data. A matrix of size   `(indim, nclasses)` with the ith column being the class-mean of the ith class in   `classes` (See fitted params section above).\n\n  * `class_weights`: The weights (class counts) of each class. A vector of length `nclasses` with the ith element being the class weight of the ith class in `classes`. (See fitted params section above.)\n  * `explained_variance_ratio`: The ratio of explained variance to total variance. Each dimension corresponds to an eigenvalue.\n\n# Examples\n\n```\nusing MLJ\n\nBayesianSubspaceLDA = @load BayesianSubspaceLDA pkg=MultivariateStats\n\nX, y = @load_iris # a table and a vector\n\nmodel = BayesianSubspaceLDA()\nmach = machine(model, X, y) |> fit!\n\nXproj = transform(mach, X)\ny_hat = predict(mach, X)\nlabels = predict_mode(mach, X)\n```\n\nSee also [`LDA`](@ref), [`BayesianLDA`](@ref), [`SubspaceLDA`](@ref)\n"
":name" = "BayesianSubspaceLDA"
":human_name" = "Bayesian subspace LDA model"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":transform"]
":hyperparameters" = "`(:normalize, :outdim, :priors)`"
":hyperparameter_types" = "`(\"Bool\", \"Int64\", \"Union{Nothing, Dict{<:Any, <:Real}, CategoricalDistributions.UnivariateFinite{<:Any, <:Any, <:Any, <:Real}}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MultivariateStats.FactorAnalysis]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "MultivariateStats"
":package_license" = "MIT"
":load_path" = "MLJMultivariateStatsInterface.FactorAnalysis"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaStats/MultivariateStats.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nFactorAnalysis\n```\n\nA model type for constructing a factor analysis model, based on [MultivariateStats.jl](https://github.com/JuliaStats/MultivariateStats.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nFactorAnalysis = @load FactorAnalysis pkg=MultivariateStats\n```\n\nDo `model = FactorAnalysis()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `FactorAnalysis(method=...)`.\n\nFactor analysis is a linear-Gaussian latent variable model that is closely related to probabilistic PCA. In contrast to the probabilistic PCA model, the covariance of conditional distribution of the observed variable given the latent variable is diagonal rather than isotropic.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns   are of scitype `Continuous`; check column scitypes with `schema(X)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `method::Symbol=:cm`: Method to use to solve the problem, one of `:ml`, `:em`, `:bayes`.\n  * `maxoutdim=0`: Controls the the dimension (number of columns) of the output,   `outdim`. Specifically, `outdim = min(n, indim, maxoutdim)`, where `n` is the number of   observations and `indim` the input dimension.\n  * `maxiter::Int=1000`: Maximum number of iterations.\n  * `tol::Real=1e-6`: Convergence tolerance.\n  * `eta::Real=tol`: Variance lower bound.\n  * `mean::Union{Nothing, Real, Vector{Float64}}=nothing`: If `nothing`, centering will be   computed and applied; if set to `0` no centering is applied (data is assumed   pre-centered); if a vector, the centering is done with that vector.\n\n# Operations\n\n  * `transform(mach, Xnew)`: Return a lower dimensional projection of the input `Xnew`, which should have the same scitype as `X` above.\n  * `inverse_transform(mach, Xsmall)`: For a dimension-reduced table `Xsmall`, such as returned by `transform`, reconstruct a table, having same the number of columns as the original training data `X`, that transforms to `Xsmall`. Mathematically, `inverse_transform` is a right-inverse for the PCA projection map, whose image is orthogonal to the kernel of that map. In particular, if `Xsmall = transform(mach, Xnew)`, then `inverse_transform(Xsmall)` is only an approximation to `Xnew`.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `projection`: Returns the projection matrix, which has size `(indim, outdim)`, where `indim` and `outdim` are the number of features of the input and ouput respectively. Each column of the projection matrix corresponds to a factor.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `indim`: Dimension (number of columns) of the training data and new data to be transformed.\n  * `outdim`: Dimension of transformed data (number of factors).\n  * `variance`: The variance of the factors.\n  * `covariance_matrix`: The estimated covariance matrix.\n  * `mean`: The mean of the untransformed training data, of length `indim`.\n  * `loadings`: The factor loadings. A matrix of size (`indim`, `outdim`) where `indim` and `outdim` are as defined above.\n\n# Examples\n\n```\nusing MLJ\n\nFactorAnalysis = @load FactorAnalysis pkg=MultivariateStats\n\nX, y = @load_iris # a table and a vector\n\nmodel = FactorAnalysis(maxoutdim=2)\nmach = machine(model, X) |> fit!\n\nXproj = transform(mach, X)\n```\n\nSee also [`KernelPCA`](@ref), [`ICA`](@ref), [`PPCA`](@ref), [`PCA`](@ref)\n"
":name" = "FactorAnalysis"
":human_name" = "factor analysis model"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":inverse_transform", ":transform"]
":hyperparameters" = "`(:method, :maxoutdim, :maxiter, :tol, :eta, :mean)`"
":hyperparameter_types" = "`(\"Symbol\", \"Int64\", \"Int64\", \"Real\", \"Real\", \"Union{Nothing, Real, Vector{Float64}}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MultivariateStats.LinearRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MultivariateStats"
":package_license" = "MIT"
":load_path" = "MLJMultivariateStatsInterface.LinearRegressor"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaStats/MultivariateStats.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLinearRegressor\n```\n\nA model type for constructing a linear regressor, based on [MultivariateStats.jl](https://github.com/JuliaStats/MultivariateStats.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nLinearRegressor = @load LinearRegressor pkg=MultivariateStats\n```\n\nDo `model = LinearRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `LinearRegressor(bias=...)`.\n\n`LinearRegressor` assumes the target is a `Continuous` variable and trains a linear prediction function using the least squares algorithm. Options exist to specify a bias  term.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype    `Continuous`; check the column scitypes with `schema(X)`.\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is    `Continuous`; check the scitype with `scitype(y)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `bias=true`: Include the bias term if true, otherwise fit without bias term.\n\n# Operations\n\n  * `predict(mach, Xnew)`: Return predictions of the target given new features `Xnew`, which    should have the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `coefficients`: The linear coefficients determined by the model.\n  * `intercept`: The intercept determined by the model.\n\n# Examples\n\n```\nusing MLJ\n\nLinearRegressor = @load LinearRegressor pkg=MultivariateStats\nlinear_regressor = LinearRegressor()\n\nX, y = make_regression(100, 2) # a table and a vector (synthetic data)\nmach = machine(linear_regressor, X, y) |> fit!\n\nXnew, _ = make_regression(3, 2)\nyhat = predict(mach, Xnew) # new predictions\n```\n\nSee also [`MultitargetLinearRegressor`](@ref), [`RidgeRegressor`](@ref), [`MultitargetRidgeRegressor`](@ref)\n"
":name" = "LinearRegressor"
":human_name" = "linear regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:bias,)`"
":hyperparameter_types" = "`(\"Bool\",)`"
":hyperparameter_ranges" = "`(nothing,)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MultivariateStats.ICA]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "MultivariateStats"
":package_license" = "MIT"
":load_path" = "MLJMultivariateStatsInterface.ICA"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaStats/MultivariateStats.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nICA\n```\n\nA model type for constructing a independent component analysis model, based on [MultivariateStats.jl](https://github.com/JuliaStats/MultivariateStats.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nICA = @load ICA pkg=MultivariateStats\n```\n\nDo `model = ICA()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `ICA(outdim=...)`.\n\nIndependent component analysis is a computational technique for separating a multivariate signal into additive subcomponents, with the assumption that the subcomponents are non-Gaussian and independent from each other.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `outdim::Int=0`: The number of independent components to recover, set automatically if `0`.\n  * `alg::Symbol=:fastica`: The algorithm to use (only `:fastica` is supported at the moment).\n  * `fun::Symbol=:tanh`: The approximate neg-entropy function, one of `:tanh`, `:gaus`.\n  * `do_whiten::Bool=true`: Whether or not to perform pre-whitening.\n  * `maxiter::Int=100`: The maximum number of iterations.\n  * `tol::Real=1e-6`: The convergence tolerance for change in the unmixing matrix W.\n  * `mean::Union{Nothing, Real, Vector{Float64}}=nothing`: mean to use, if nothing (default) centering is computed and applied, if zero, no centering; otherwise a vector of means can be passed.\n  * `winit::Union{Nothing,Matrix{<:Real}}=nothing`: Initial guess for the unmixing matrix `W`: either an empty matrix (for random initialization of `W`), a matrix of size `m × k` (if `do_whiten` is true), or a matrix of size `m × k`. Here `m` is the number of components (columns) of the input.\n\n# Operations\n\n  * `transform(mach, Xnew)`: Return the component-separated version of input `Xnew`, which should have the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `projection`: The estimated component matrix.\n  * `mean`: The estimated mean vector.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `indim`: Dimension (number of columns) of the training data and new data to be transformed.\n  * `outdim`: Dimension of transformed data.\n  * `mean`: The mean of the untransformed training data, of length `indim`.\n\n# Examples\n\n```\nusing MLJ\n\nICA = @load ICA pkg=MultivariateStats\n\ntimes = range(0, 8, length=2000)\n\nsine_wave = sin.(2*times)\nsquare_wave = sign.(sin.(3*times))\nsawtooth_wave = map(t -> mod(2t, 2) - 1, times)\nsignals = hcat(sine_wave, square_wave, sawtooth_wave)\nnoisy_signals = signals + 0.2*randn(size(signals))\n\nmixing_matrix = [ 1 1 1; 0.5 2 1; 1.5 1 2]\nX = MLJ.table(noisy_signals*mixing_matrix)\n\nmodel = ICA(outdim = 3, tol=0.1)\nmach = machine(model, X) |> fit!\n\nX_unmixed = transform(mach, X)\n\nusing Plots\n\nplot(X.x2)\nplot(X.x2)\nplot(X.x3)\n\nplot(X_unmixed.x1)\nplot(X_unmixed.x2)\nplot(X_unmixed.x3)\n\n```\n\nSee also [`PCA`](@ref), [`KernelPCA`](@ref), [`FactorAnalysis`](@ref), [`PPCA`](@ref)\n"
":name" = "ICA"
":human_name" = "independent component analysis model"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":inverse_transform", ":transform"]
":hyperparameters" = "`(:outdim, :alg, :fun, :do_whiten, :maxiter, :tol, :winit, :mean)`"
":hyperparameter_types" = "`(\"Int64\", \"Symbol\", \"Symbol\", \"Bool\", \"Int64\", \"Real\", \"Union{Nothing, Matrix{<:Real}}\", \"Union{Nothing, Real, Vector{Float64}}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MultivariateStats.PPCA]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "MultivariateStats"
":package_license" = "MIT"
":load_path" = "MLJMultivariateStatsInterface.PPCA"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaStats/MultivariateStats.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nPPCA\n```\n\nA model type for constructing a probabilistic PCA model, based on [MultivariateStats.jl](https://github.com/JuliaStats/MultivariateStats.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nPPCA = @load PPCA pkg=MultivariateStats\n```\n\nDo `model = PPCA()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `PPCA(maxoutdim=...)`.\n\nProbabilistic principal component analysis is a dimension-reduction algorithm which represents a constrained form of the Gaussian distribution in which the number of free parameters can be restricted while still allowing the model to capture the dominant correlations in a data set. It is expressed as the maximum likelihood solution of a probabilistic latent variable model. For details, see Bishop (2006): C. M. Pattern Recognition and Machine Learning.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `maxoutdim=0`: Controls the the dimension (number of columns) of the output, `outdim`. Specifically, `outdim = min(n, indim, maxoutdim)`, where `n` is the number of observations and `indim` the input dimension.\n  * `method::Symbol=:ml`: The method to use to solve the problem, one of `:ml`, `:em`, `:bayes`.\n  * `maxiter::Int=1000`: The maximum number of iterations.\n  * `tol::Real=1e-6`: The convergence tolerance.\n  * `mean::Union{Nothing, Real, Vector{Float64}}=nothing`: If `nothing`, centering will be computed and applied; if set to `0` no centering is applied (data is assumed pre-centered); if a vector, the centering is done with that vector.\n\n# Operations\n\n  * `transform(mach, Xnew)`: Return a lower dimensional projection of the input `Xnew`, which should have the same scitype as `X` above.\n  * `inverse_transform(mach, Xsmall)`: For a dimension-reduced table `Xsmall`, such as returned by `transform`, reconstruct a table, having same the number of columns as the original training data `X`, that transforms to `Xsmall`. Mathematically, `inverse_transform` is a right-inverse for the PCA projection map, whose image is orthogonal to the kernel of that map. In particular, if `Xsmall = transform(mach, Xnew)`, then `inverse_transform(Xsmall)` is only an approximation to `Xnew`.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `projection`: Returns the projection matrix, which has size `(indim, outdim)`, where `indim` and `outdim` are the number of features of the input and ouput respectively. Each column of the projection matrix corresponds to a principal component.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `indim`: Dimension (number of columns) of the training data and new data to be transformed.\n  * `outdim`: Dimension of transformed data.\n  * `tvat`: The variance of the components.\n  * `loadings`: The model's loadings matrix. A matrix of size (`indim`, `outdim`) where `indim` and `outdim` as as defined above.\n\n# Examples\n\n```\nusing MLJ\n\nPPCA = @load PPCA pkg=MultivariateStats\n\nX, y = @load_iris # a table and a vector\n\nmodel = PPCA(maxoutdim=2)\nmach = machine(model, X) |> fit!\n\nXproj = transform(mach, X)\n```\n\nSee also [`KernelPCA`](@ref), [`ICA`](@ref), [`FactorAnalysis`](@ref), [`PCA`](@ref)\n"
":name" = "PPCA"
":human_name" = "probabilistic PCA model"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":inverse_transform", ":transform"]
":hyperparameters" = "`(:maxoutdim, :method, :maxiter, :tol, :mean)`"
":hyperparameter_types" = "`(\"Int64\", \"Symbol\", \"Int64\", \"Real\", \"Union{Nothing, Real, Vector{Float64}}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MultivariateStats.RidgeRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MultivariateStats"
":package_license" = "MIT"
":load_path" = "MLJMultivariateStatsInterface.RidgeRegressor"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaStats/MultivariateStats.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nRidgeRegressor\n```\n\nA model type for constructing a ridge regressor, based on [MultivariateStats.jl](https://github.com/JuliaStats/MultivariateStats.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nRidgeRegressor = @load RidgeRegressor pkg=MultivariateStats\n```\n\nDo `model = RidgeRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `RidgeRegressor(lambda=...)`.\n\n`RidgeRegressor` adds a quadratic penalty term to least squares regression, for regularization. Ridge regression is particularly useful in the case of multicollinearity. Options exist to specify a bias term, and to adjust the strength of the penalty term.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype    `Continuous`; check column scitypes with `schema(X)`.\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is    `Continuous`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `lambda=1.0`: Is the non-negative parameter for the regularization strength. If lambda    is 0, ridge regression is equivalent to linear least squares regression, and as lambda    approaches infinity, all the linear coefficients approach 0.\n  * `bias=true`: Include the bias term if true, otherwise fit without bias term.\n\n# Operations\n\n  * `predict(mach, Xnew)`: Return predictions of the target given new features `Xnew`, which    should have the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `coefficients`: The linear coefficients determined by the model.\n  * `intercept`: The intercept determined by the model.\n\n# Examples\n\n```\nusing MLJ\n\nRidgeRegressor = @load RidgeRegressor pkg=MultivariateStats\npipe = Standardizer() |> RidgeRegressor(lambda=10)\n\nX, y = @load_boston\n\nmach = machine(pipe, X, y) |> fit!\nyhat = predict(mach, X)\ntraining_error = l1(yhat, y) |> mean\n```\n\nSee also [`LinearRegressor`](@ref), [`MultitargetLinearRegressor`](@ref), [`MultitargetRidgeRegressor`](@ref)\n"
":name" = "RidgeRegressor"
":human_name" = "ridge regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:lambda, :bias)`"
":hyperparameter_types" = "`(\"Union{Real, AbstractVecOrMat}\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MultivariateStats.KernelPCA]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "MultivariateStats"
":package_license" = "MIT"
":load_path" = "MLJMultivariateStatsInterface.KernelPCA"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaStats/MultivariateStats.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nKernelPCA\n```\n\nA model type for constructing a kernel prinicipal component analysis model, based on [MultivariateStats.jl](https://github.com/JuliaStats/MultivariateStats.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nKernelPCA = @load KernelPCA pkg=MultivariateStats\n```\n\nDo `model = KernelPCA()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `KernelPCA(maxoutdim=...)`.\n\nIn kernel PCA the linear operations of ordinary principal component analysis are performed in a [reproducing Hilbert space](https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space).\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `maxoutdim=0`: Controls the the dimension (number of columns) of the output, `outdim`. Specifically, `outdim = min(n, indim, maxoutdim)`, where `n` is the number of observations and `indim` the input dimension.\n  * `kernel::Function=(x,y)->x'y`: The kernel function, takes in 2 vector arguments x and y, returns a scalar value. Defaults to the dot product of `x` and `y`.\n  * `solver::Symbol=:eig`: solver to use for the eigenvalues, one of `:eig`(default, uses `LinearAlgebra.eigen`), `:eigs`(uses `Arpack.eigs`).\n  * `inverse::Bool=true`: perform calculations needed for inverse transform\n  * `beta::Real=1.0`: strength of the ridge regression that learns the inverse transform when inverse is true.\n  * `tol::Real=0.0`: Convergence tolerance for eigenvalue solver.\n  * `maxiter::Int=300`: maximum number of iterations for eigenvalue solver.\n\n# Operations\n\n  * `transform(mach, Xnew)`: Return a lower dimensional projection of the input `Xnew`, which   should have the same scitype as `X` above.\n  * `inverse_transform(mach, Xsmall)`: For a dimension-reduced table `Xsmall`, such as returned by `transform`, reconstruct a table, having same the number of columns as the original training data `X`, that transforms to `Xsmall`.  Mathematically, `inverse_transform` is a right-inverse for the PCA projection map, whose image is orthogonal to the kernel of that map. In particular, if `Xsmall = transform(mach, Xnew)`, then `inverse_transform(Xsmall)` is only an approximation to `Xnew`.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `projection`: Returns the projection matrix, which has size `(indim, outdim)`, where `indim` and `outdim` are the number of features of the input and ouput respectively.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `indim`: Dimension (number of columns) of the training data and new data to be transformed.\n  * `outdim`: Dimension of transformed data.\n  * `principalvars`: The variance of the principal components.\n\n# Examples\n\n```\nusing MLJ\nusing LinearAlgebra\n\nKernelPCA = @load KernelPCA pkg=MultivariateStats\n\nX, y = @load_iris # a table and a vector\n\nfunction rbf_kernel(length_scale)\n    return (x,y) -> norm(x-y)^2 / ((2 * length_scale)^2)\nend\n\nmodel = KernelPCA(maxoutdim=2, kernel=rbf_kernel(1))\nmach = machine(model, X) |> fit!\n\nXproj = transform(mach, X)\n```\n\nSee also [`PCA`](@ref), [`ICA`](@ref), [`FactorAnalysis`](@ref), [`PPCA`](@ref)\n"
":name" = "KernelPCA"
":human_name" = "kernel prinicipal component analysis model"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":inverse_transform", ":transform"]
":hyperparameters" = "`(:maxoutdim, :kernel, :solver, :inverse, :beta, :tol, :maxiter)`"
":hyperparameter_types" = "`(\"Int64\", \"Union{Nothing, Function}\", \"Symbol\", \"Bool\", \"Real\", \"Real\", \"Int64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MultivariateStats.MultitargetRidgeRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MultivariateStats"
":package_license" = "MIT"
":load_path" = "MLJMultivariateStatsInterface.MultitargetRidgeRegressor"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaStats/MultivariateStats.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nMultitargetRidgeRegressor\n```\n\nA model type for constructing a multitarget ridge regressor, based on [MultivariateStats.jl](https://github.com/JuliaStats/MultivariateStats.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nMultitargetRidgeRegressor = @load MultitargetRidgeRegressor pkg=MultivariateStats\n```\n\nDo `model = MultitargetRidgeRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `MultitargetRidgeRegressor(lambda=...)`.\n\nMulti-target ridge regression adds a quadratic penalty term to multi-target least squares regression, for regularization. Ridge regression is particularly useful in the case of multicollinearity. In this case, the output represents a response vector. Options exist to specify a bias term, and to adjust the strength of the penalty term.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype    `Continuous`; check column scitypes with `schema(X)`.\n  * `y` is the target, which can be any table of responses whose element scitype is    `Continuous`; check the scitype with `scitype(y)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `lambda=1.0`: Is the non-negative parameter for the regularization strength. If lambda    is 0, ridge regression is equivalent to linear least squares regression, and as lambda    approaches infinity, all the linear coefficients approach 0.\n  * `bias=true`: Include the bias term if true, otherwise fit without bias term.\n\n# Operations\n\n  * `predict(mach, Xnew)`: Return predictions of the target given new features `Xnew`, which    should have the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `coefficients`: The linear coefficients determined by the model.\n  * `intercept`: The intercept determined by the model.\n\n# Examples\n\n```\nusing MLJ\nusing DataFrames\n\nRidgeRegressor = @load MultitargetRidgeRegressor pkg=MultivariateStats\n\nX, y = make_regression(100, 6; n_targets = 2)  # a table and a table (synthetic data)\n\nridge_regressor = RidgeRegressor(lambda=1.5)\nmach = machine(ridge_regressor, X, y) |> fit!\n\nXnew, _ = make_regression(3, 6)\nyhat = predict(mach, Xnew) # new predictions\n```\n\nSee also [`LinearRegressor`](@ref), [`MultitargetLinearRegressor`](@ref), [`RidgeRegressor`](@ref)\n"
":name" = "MultitargetRidgeRegressor"
":human_name" = "multitarget ridge regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:lambda, :bias)`"
":hyperparameter_types" = "`(\"Union{Real, AbstractVecOrMat}\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MultivariateStats.SubspaceLDA]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MultivariateStats"
":package_license" = "MIT"
":load_path" = "MLJMultivariateStatsInterface.SubspaceLDA"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaStats/MultivariateStats.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nSubspaceLDA\n```\n\nA model type for constructing a subpace LDA model, based on [MultivariateStats.jl](https://github.com/JuliaStats/MultivariateStats.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nSubspaceLDA = @load SubspaceLDA pkg=MultivariateStats\n```\n\nDo `model = SubspaceLDA()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `SubspaceLDA(normalize=...)`.\n\nMulticlass subspace linear discriminant analysis (LDA) is a variation on ordinary [`LDA`](@ref) suitable for high dimensional data, as it avoids storing scatter matrices. For details, refer the [MultivariateStats.jl documentation](https://juliastats.org/MultivariateStats.jl/stable/).\n\nIn addition to dimension reduction (using `transform`) probabilistic classification is provided (using `predict`).  In the case of classification, the class probability for a new observation reflects the proximity of that observation to training observations associated with that class, and how far away the observation is from observations associated with other classes. Specifically, the distances, in the transformed (projected) space, of a new observation, from the centroid of each target class, is computed; the resulting vector of distances, multiplied by minus one, is passed to a softmax function to obtain a class probability prediction. Here \"distance\" is computed using a user-specified distance function.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `OrderedFactor` or `Multiclass`; check the scitype with `scitype(y)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `normalize=true`: Option to normalize the between class variance for the number of observations in each class, one of `true` or `false`.\n  * `outdim`: the ouput dimension, automatically set to `min(indim, nclasses-1)` if equal to `0`. If a non-zero `outdim` is passed, then the actual output dimension used is `min(rank, outdim)` where `rank` is the rank of the within-class covariance matrix.\n  * `dist=Distances.SqEuclidean()`: The distance metric to use when performing classification (to compare the distance between a new point and centroids in the transformed space); must be a subtype of `Distances.SemiMetric` from Distances.jl, e.g., `Distances.CosineDist`.\n\n# Operations\n\n  * `transform(mach, Xnew)`: Return a lower dimensional projection of the input `Xnew`, which should have the same scitype as `X` above.\n  * `predict(mach, Xnew)`: Return predictions of the target given features `Xnew`, which should have same scitype as `X` above. Predictions are probabilistic but uncalibrated.\n  * `predict_mode(mach, Xnew)`: Return the modes of the probabilistic predictions returned above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `classes`: The classes seen during model fitting.\n  * `projection_matrix`: The learned projection matrix, of size `(indim, outdim)`, where `indim` and `outdim` are the input and output dimensions respectively (See Report section below).\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `indim`: The dimension of the input space i.e the number of training features.\n  * `outdim`: The dimension of the transformed space the model is projected to.\n  * `mean`: The mean of the untransformed training data. A vector of length `indim`.\n  * `nclasses`: The number of classes directly observed in the training data (which can be less than the total number of classes in the class pool)\n\n`class_means`: The class-specific means of the training data. A matrix of size   `(indim, nclasses)` with the ith column being the class-mean of the ith class in   `classes` (See fitted params section above).\n\n  * `class_weights`: The weights (class counts) of each class. A vector of length `nclasses` with the ith element being the class weight of the ith class in `classes`. (See fitted params section above.)\n  * `explained_variance_ratio`: The ratio of explained variance to total variance. Each dimension corresponds to an eigenvalue.\n\n# Examples\n\n```\nusing MLJ\n\nSubspaceLDA = @load SubspaceLDA pkg=MultivariateStats\n\nX, y = @load_iris # a table and a vector\n\nmodel = SubspaceLDA()\nmach = machine(model, X, y) |> fit!\n\nXproj = transform(mach, X)\ny_hat = predict(mach, X)\nlabels = predict_mode(mach, X)\n```\n\nSee also [`LDA`](@ref), [`BayesianLDA`](@ref), [`BayesianSubspaceLDA`](@ref)\n"
":name" = "SubspaceLDA"
":human_name" = "subpace LDA model"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":transform"]
":hyperparameters" = "`(:normalize, :outdim, :dist)`"
":hyperparameter_types" = "`(\"Bool\", \"Int64\", \"Distances.SemiMetric\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MultivariateStats.BayesianLDA]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MultivariateStats"
":package_license" = "MIT"
":load_path" = "MLJMultivariateStatsInterface.BayesianLDA"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaStats/MultivariateStats.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nBayesianLDA\n```\n\nA model type for constructing a Bayesian LDA model, based on [MultivariateStats.jl](https://github.com/JuliaStats/MultivariateStats.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nBayesianLDA = @load BayesianLDA pkg=MultivariateStats\n```\n\nDo `model = BayesianLDA()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `BayesianLDA(method=...)`.\n\nThe Bayesian multiclass LDA algorithm learns a projection matrix as described in ordinary [`LDA`](@ref).  Predicted class posterior probability distributions are derived by applying Bayes' rule with a multivariate Gaussian class-conditional distribution. A prior class distribution can be specified by the user or inferred from training data class frequency.\n\nSee also the [package documentation](https://multivariatestatsjl.readthedocs.io/en/latest/lda.html).  For more information about the algorithm, see [Li, Zhu and Ogihara (2006): Using Discriminant Analysis for Multi-class Classification: An Experimental Investigation](https://doi.org/10.1007/s10115-006-0013-y).\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `OrderedFactor` or `Multiclass`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `method::Symbol=:gevd`: choice of solver, one of `:gevd` or `:whiten` methods.\n  * `cov_w::StatsBase.SimpleCovariance()`: An estimator for the within-class covariance (used in computing the within-class scatter matrix, `Sw`). Any robust estimator from `CovarianceEstimation.jl` can be used.\n  * `cov_b::StatsBase.SimpleCovariance()`: The same as `cov_w` but for the between-class covariance (used in computing the between-class scatter matrix, `Sb`).\n  * `outdim::Int=0`: The output dimension, i.e., dimension of the transformed space, automatically set to `min(indim, nclasses-1)` if equal to 0.\n  * `regcoef::Float64=1e-6`: The regularization coefficient. A positive value `regcoef*eigmax(Sw)` where `Sw` is the within-class scatter matrix, is added to the diagonal of `Sw` to improve numerical stability. This can be useful if using the standard covariance estimator.\n  * `priors::Union{Nothing, UnivariateFinite{<:Any, <:Any, <:Any, <:Real}, Dict{<:Any, <:Real}} = nothing`: For use in prediction with Bayes rule. If `priors = nothing` then `priors` are estimated from the class proportions in the training data. Otherwise it requires a `Dict` or `UnivariateFinite` object specifying the classes with non-zero probabilities in the training target.\n\n# Operations\n\n  * `transform(mach, Xnew)`: Return a lower dimensional projection of the input `Xnew`, which should have the same scitype as `X` above.\n  * `predict(mach, Xnew)`: Return predictions of the target given features `Xnew`, which should have the same scitype as `X` above. Predictions are probabilistic but uncalibrated.\n  * `predict_mode(mach, Xnew)`: Return the modes of the probabilistic predictions returned above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `classes`: The classes seen during model fitting.\n  * `projection_matrix`: The learned projection matrix, of size `(indim, outdim)`, where `indim` and `outdim` are the input and output dimensions respectively (See Report section below).\n  * `priors`: The class priors for classification. As inferred from training target `y`, if not user-specified. A `UnivariateFinite` object with levels consistent with `levels(y)`.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `indim`: The dimension of the input space i.e the number of training features.\n  * `outdim`: The dimension of the transformed space the model is projected to.\n  * `mean`: The mean of the untransformed training data. A vector of length `indim`.\n  * `nclasses`: The number of classes directly observed in the training data (which can be less than the total number of classes in the class pool).\n  * `class_means`: The class-specific means of the training data. A matrix of size `(indim, nclasses)` with the ith column being the class-mean of the ith class in `classes` (See fitted params section above).\n  * `class_weights`: The weights (class counts) of each class. A vector of length `nclasses` with the ith element being the class weight of the ith class in `classes`. (See fitted params section above.)\n  * `Sb`: The between class scatter matrix.\n  * `Sw`: The within class scatter matrix.\n\n# Examples\n\n```\nusing MLJ\n\nBayesianLDA = @load BayesianLDA pkg=MultivariateStats\n\nX, y = @load_iris # a table and a vector\n\nmodel = BayesianLDA()\nmach = machine(model, X, y) |> fit!\n\nXproj = transform(mach, X)\ny_hat = predict(mach, X)\nlabels = predict_mode(mach, X)\n```\n\nSee also [`LDA`](@ref), [`SubspaceLDA`](@ref), [`BayesianSubspaceLDA`](@ref)\n"
":name" = "BayesianLDA"
":human_name" = "Bayesian LDA model"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":transform"]
":hyperparameters" = "`(:method, :cov_w, :cov_b, :outdim, :regcoef, :priors)`"
":hyperparameter_types" = "`(\"Symbol\", \"StatsBase.CovarianceEstimator\", \"StatsBase.CovarianceEstimator\", \"Int64\", \"Float64\", \"Union{Nothing, Dict{<:Any, <:Real}, CategoricalDistributions.UnivariateFinite{<:Any, <:Any, <:Any, <:Real}}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MultivariateStats.PCA]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "MultivariateStats"
":package_license" = "MIT"
":load_path" = "MLJMultivariateStatsInterface.PCA"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":package_url" = "https://github.com/JuliaStats/MultivariateStats.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nPCA\n```\n\nA model type for constructing a pca, based on [MultivariateStats.jl](https://github.com/JuliaStats/MultivariateStats.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nPCA = @load PCA pkg=MultivariateStats\n```\n\nDo `model = PCA()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `PCA(maxoutdim=...)`.\n\nPrincipal component analysis learns a linear projection onto a lower dimensional space while preserving most of the initial variance seen in the training data.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `maxoutdim=0`: Together with `variance_ratio`, controls the output dimension `outdim` chosen by the model. Specifically, suppose that `k` is the smallest integer such that retaining the `k` most significant principal components accounts for `variance_ratio` of the total variance in the training data. Then `outdim = min(outdim, maxoutdim)`. If `maxoutdim=0` (default) then the effective `maxoutdim` is `min(n, indim - 1)` where `n` is the number of observations and `indim` the number of features in the training data.\n  * `variance_ratio::Float64=0.99`: The ratio of variance preserved after the transformation\n  * `method=:auto`: The method to use to solve the problem. Choices are\n\n      * `:svd`: Support Vector Decomposition of the matrix.\n      * `:cov`: Covariance matrix decomposition.\n      * `:auto`: Use `:cov` if the matrices first dimension is smaller than its second dimension and otherwise use `:svd`\n  * `mean=nothing`: if `nothing`, centering will be computed and applied, if set to `0` no centering (data is assumed pre-centered); if a vector is passed, the centering is done with that vector.\n\n# Operations\n\n  * `transform(mach, Xnew)`: Return a lower dimensional projection of the input `Xnew`, which should have the same scitype as `X` above.\n  * `inverse_transform(mach, Xsmall)`: For a dimension-reduced table `Xsmall`, such as returned by `transform`, reconstruct a table, having same the number of columns as the original training data `X`, that transforms to `Xsmall`. Mathematically, `inverse_transform` is a right-inverse for the PCA projection map, whose image is orthogonal to the kernel of that map. In particular, if `Xsmall = transform(mach, Xnew)`, then `inverse_transform(Xsmall)` is only an approximation to `Xnew`.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `projection`: Returns the projection matrix, which has size `(indim, outdim)`, where `indim` and `outdim` are the number of features of the input and output respectively.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `indim`: Dimension (number of columns) of the training data and new data to be transformed.\n  * `outdim = min(n, indim, maxoutdim)` is the output dimension; here `n` is the number of observations.\n  * `tprincipalvar`: Total variance of the principal components.\n  * `tresidualvar`: Total residual variance.\n  * `tvar`: Total observation variance (principal + residual variance).\n  * `mean`: The mean of the untransformed training data, of length `indim`.\n  * `principalvars`: The variance of the principal components. An AbstractVector of length `outdim`\n  * `loadings`: The models loadings, weights for each variable used when calculating principal components. A matrix of size (`indim`, `outdim`) where `indim` and `outdim` are as defined above.\n\n# Examples\n\n```\nusing MLJ\n\nPCA = @load PCA pkg=MultivariateStats\n\nX, y = @load_iris # a table and a vector\n\nmodel = PCA(maxoutdim=2)\nmach = machine(model, X) |> fit!\n\nXproj = transform(mach, X)\n```\n\nSee also [`KernelPCA`](@ref), [`ICA`](@ref), [`FactorAnalysis`](@ref), [`PPCA`](@ref)\n"
":name" = "PCA"
":human_name" = "pca"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":inverse_transform", ":transform"]
":hyperparameters" = "`(:maxoutdim, :method, :variance_ratio, :mean)`"
":hyperparameter_types" = "`(\"Int64\", \"Symbol\", \"Float64\", \"Union{Nothing, Real, Vector{Float64}}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[DecisionTree.AdaBoostStumpClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "DecisionTree"
":package_license" = "MIT"
":load_path" = "MLJDecisionTreeInterface.AdaBoostStumpClassifier"
":package_uuid" = "7806a523-6efd-50cb-b5f6-3fa6f1930dbb"
":package_url" = "https://github.com/bensadeghi/DecisionTree.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nAdaBoostStumpClassifier\n```\n\nA model type for constructing a Ada-boosted stump classifier, based on [DecisionTree.jl](https://github.com/bensadeghi/DecisionTree.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nAdaBoostStumpClassifier = @load AdaBoostStumpClassifier pkg=DecisionTree\n```\n\nDo `model = AdaBoostStumpClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `AdaBoostStumpClassifier(n_iter=...)`.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere:\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have one of the following element scitypes: `Continuous`, `Count`, or `<:OrderedFactor`; check column scitypes with `schema(X)`\n  * `y`: the target, which can be any `AbstractVector` whose element scitype is `<:OrderedFactor` or `<:Multiclass`; check the scitype with `scitype(y)`\n\nTrain the machine with `fit!(mach, rows=...)`.\n\n# Hyperparameters\n\n  * `n_iter=10`:   number of iterations of AdaBoost\n  * `feature_importance`: method to use for computing feature importances. One of `(:impurity, :split)`\n  * `rng=Random.GLOBAL_RNG`: random number generator or seed\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given features `Xnew` having the same scitype as `X` above. Predictions are probabilistic, but uncalibrated.\n  * `predict_mode(mach, Xnew)`: instead return the mode of each prediction above.\n\n# Fitted Parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `stumps`: the `Ensemble` object returned by the core DecisionTree.jl algorithm.\n  * `coefficients`: the stump coefficients (one per stump)\n\n# Report\n\n  * `features`: the names of the features encountered in training\n\n# Accessor functions\n\n  * `feature_importances(mach)` returns a vector of `(feature::Symbol => importance)` pairs; the type of importance is determined by the hyperparameter `feature_importance` (see above)\n\n# Examples\n\n```\nusing MLJ\nBooster = @load AdaBoostStumpClassifier pkg=DecisionTree\nbooster = Booster(n_iter=15)\n\nX, y = @load_iris\nmach = machine(booster, X, y) |> fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\nyhat = predict(mach, Xnew) # probabilistic predictions\npredict_mode(mach, Xnew)   # point predictions\npdf.(yhat, \"virginica\")    # probabilities for the \"verginica\" class\n\nfitted_params(mach).stumps # raw `Ensemble` object from DecisionTree.jl\nfitted_params(mach).coefs  # coefficient associated with each stump\nfeature_importances(mach)\n```\n\nSee also [DecisionTree.jl](https://github.com/bensadeghi/DecisionTree.jl) and the unwrapped model type [`MLJDecisionTreeInterface.DecisionTree.AdaBoostStumpClassifier`](@ref).\n"
":name" = "AdaBoostStumpClassifier"
":human_name" = "Ada-boosted stump classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":reformat", ":selectrows", ":feature_importances"]
":hyperparameters" = "`(:n_iter, :feature_importance, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Symbol\", \"Union{Integer, Random.AbstractRNG}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`true`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[DecisionTree.DecisionTreeRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "DecisionTree"
":package_license" = "MIT"
":load_path" = "MLJDecisionTreeInterface.DecisionTreeRegressor"
":package_uuid" = "7806a523-6efd-50cb-b5f6-3fa6f1930dbb"
":package_url" = "https://github.com/bensadeghi/DecisionTree.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nDecisionTreeRegressor\n```\n\nA model type for constructing a CART decision tree regressor, based on [DecisionTree.jl](https://github.com/bensadeghi/DecisionTree.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nDecisionTreeRegressor = @load DecisionTreeRegressor pkg=DecisionTree\n```\n\nDo `model = DecisionTreeRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `DecisionTreeRegressor(max_depth=...)`.\n\n`DecisionTreeRegressor` implements the [CART algorithm](https://en.wikipedia.org/wiki/Decision_tree_learning), originally published in Breiman, Leo; Friedman, J. H.; Olshen, R. A.; Stone, C. J. (1984): \"Classification and regression trees\". *Monterey, CA: Wadsworth & Brooks/Cole Advanced Books & Software.*.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have one of the following element scitypes: `Continuous`, `Count`, or `<:OrderedFactor`; check column scitypes with `schema(X)`\n  * `y`: the target, which can be any `AbstractVector` whose element scitype is `Continuous`; check the scitype with `scitype(y)`\n\nTrain the machine with `fit!(mach, rows=...)`.\n\n# Hyperparameters\n\n  * `max_depth=-1`:          max depth of the decision tree (-1=any)\n  * `min_samples_leaf=1`:    max number of samples each leaf needs to have\n  * `min_samples_split=2`:   min number of samples needed for a split\n  * `min_purity_increase=0`: min purity needed for a split\n  * `n_subfeatures=0`: number of features to select at random (0 for all)\n  * `post_prune=false`:      set to `true` for post-fit pruning\n  * `merge_purity_threshold=1.0`: (post-pruning) merge leaves having                          combined purity `>= merge_purity_threshold`\n  * `feature_importance`: method to use for computing feature importances. One of `(:impurity, :split)`\n  * `rng=Random.GLOBAL_RNG`: random number generator or seed\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given new features `Xnew` having the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `tree`: the tree or stump object returned by the core DecisionTree.jl algorithm\n  * `features`: the names of the features encountered in training\n\n# Report\n\n  * `features`: the names of the features encountered in training\n\n# Accessor functions\n\n  * `feature_importances(mach)` returns a vector of `(feature::Symbol => importance)` pairs; the type of importance is determined by the hyperparameter `feature_importance` (see above)\n\n# Examples\n\n```\nusing MLJ\nDecisionTreeRegressor = @load DecisionTreeRegressor pkg=DecisionTree\nmodel = DecisionTreeRegressor(max_depth=3, min_samples_split=3)\n\nX, y = make_regression(100, 4; rng=123) # synthetic data\nmach = machine(model, X, y) |> fit!\n\nXnew, _ = make_regression(3, 2; rng=123)\nyhat = predict(mach, Xnew) # new predictions\n\njulia> fitted_params(mach).tree\nx1 < 0.2758\n├─ x2 < 0.9137\n│  ├─ x1 < -0.9582\n│  │  ├─ 0.9189256882087312 (0/12)\n│  │  └─ -0.23180616021065256 (0/38)\n│  └─ -1.6461153800037722 (0/9)\n└─ x1 < 1.062\n   ├─ x2 < -0.4969\n   │  ├─ -0.9330755147107384 (0/5)\n   │  └─ -2.3287967825015548 (0/17)\n   └─ x2 < 0.4598\n      ├─ -2.931299926506291 (0/11)\n      └─ -4.726518740473489 (0/8)\n\nfeature_importances(mach) # get feature importances\n```\n\nSee also [DecisionTree.jl](https://github.com/bensadeghi/DecisionTree.jl) and the unwrapped model type [`MLJDecisionTreeInterface.DecisionTree.DecisionTreeRegressor`](@ref).\n"
":name" = "DecisionTreeRegressor"
":human_name" = "CART decision tree regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":reformat", ":selectrows", ":feature_importances"]
":hyperparameters" = "`(:max_depth, :min_samples_leaf, :min_samples_split, :min_purity_increase, :n_subfeatures, :post_prune, :merge_purity_threshold, :feature_importance, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Int64\", \"Int64\", \"Float64\", \"Int64\", \"Bool\", \"Float64\", \"Symbol\", \"Union{Integer, Random.AbstractRNG}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`true`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[DecisionTree.DecisionTreeClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "DecisionTree"
":package_license" = "MIT"
":load_path" = "MLJDecisionTreeInterface.DecisionTreeClassifier"
":package_uuid" = "7806a523-6efd-50cb-b5f6-3fa6f1930dbb"
":package_url" = "https://github.com/bensadeghi/DecisionTree.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nDecisionTreeClassifier\n```\n\nA model type for constructing a CART decision tree classifier, based on [DecisionTree.jl](https://github.com/bensadeghi/DecisionTree.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nDecisionTreeClassifier = @load DecisionTreeClassifier pkg=DecisionTree\n```\n\nDo `model = DecisionTreeClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `DecisionTreeClassifier(max_depth=...)`.\n\n`DecisionTreeClassifier` implements the [CART algorithm](https://en.wikipedia.org/wiki/Decision_tree_learning), originally published in Breiman, Leo; Friedman, J. H.; Olshen, R. A.; Stone, C. J. (1984): \"Classification and regression trees\". *Monterey, CA: Wadsworth & Brooks/Cole Advanced Books & Software.*.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have one of the following element scitypes: `Continuous`, `Count`, or `<:OrderedFactor`; check column scitypes with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `<:OrderedFactor` or `<:Multiclass`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyperparameters\n\n  * `max_depth=-1`:          max depth of the decision tree (-1=any)\n  * `min_samples_leaf=1`:    max number of samples each leaf needs to have\n  * `min_samples_split=2`:   min number of samples needed for a split\n  * `min_purity_increase=0`: min purity needed for a split\n  * `n_subfeatures=0`: number of features to select at random (0 for all)\n  * `post_prune=false`:      set to `true` for post-fit pruning\n  * `merge_purity_threshold=1.0`: (post-pruning) merge leaves having                          combined purity `>= merge_purity_threshold`\n  * `display_depth=5`:       max depth to show when displaying the tree\n  * `feature_importance`: method to use for computing feature importances. One of `(:impurity, :split)`\n  * `rng=Random.GLOBAL_RNG`: random number generator or seed\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given features `Xnew` having the same scitype as `X` above. Predictions are probabilistic, but uncalibrated.\n  * `predict_mode(mach, Xnew)`: instead return the mode of each prediction above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `raw_tree`: the raw `Node`, `Leaf` or `Root` object returned by the core DecisionTree.jl algorithm\n  * `tree`: a visualizable, wrapped version of `raw_tree` implementing the AbstractTrees.jl interface; see \"Examples\" below\n  * `encoding`: dictionary of target classes keyed on integers used internally by DecisionTree.jl\n  * `features`: the names of the features encountered in training, in an order consistent with the output of `print_tree` (see below)\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `classes_seen`: list of target classes actually observed in training\n  * `print_tree`: alternative method to print the fitted tree, with single argument the tree depth; interpretation requires internal integer-class encoding (see \"Fitted parameters\" above).\n  * `features`: the names of the features encountered in training, in an order consistent with the output of `print_tree` (see below)\n\n# Accessor functions\n\n  * `feature_importances(mach)` returns a vector of `(feature::Symbol => importance)` pairs; the type of importance is determined by the hyperparameter `feature_importance` (see above)\n\n# Examples\n\n```\nusing MLJ\nDecisionTreeClassifier = @load DecisionTreeClassifier pkg=DecisionTree\nmodel = DecisionTreeClassifier(max_depth=3, min_samples_split=3)\n\nX, y = @load_iris\nmach = machine(model, X, y) |> fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\nyhat = predict(mach, Xnew) # probabilistic predictions\npredict_mode(mach, Xnew)   # point predictions\npdf.(yhat, \"virginica\")    # probabilities for the \"verginica\" class\n\njulia> tree = fitted_params(mach).tree\npetal_length < 2.45\n├─ setosa (50/50)\n└─ petal_width < 1.75\n   ├─ petal_length < 4.95\n   │  ├─ versicolor (47/48)\n   │  └─ virginica (4/6)\n   └─ petal_length < 4.85\n      ├─ virginica (2/3)\n      └─ virginica (43/43)\n\nusing Plots, TreeRecipe\nplot(tree) # for a graphical representation of the tree\n\nfeature_importances(mach)\n```\n\nSee also [DecisionTree.jl](https://github.com/bensadeghi/DecisionTree.jl) and the unwrapped model type [`MLJDecisionTreeInterface.DecisionTree.DecisionTreeClassifier`](@ref).\n"
":name" = "DecisionTreeClassifier"
":human_name" = "CART decision tree classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":reformat", ":selectrows", ":feature_importances"]
":hyperparameters" = "`(:max_depth, :min_samples_leaf, :min_samples_split, :min_purity_increase, :n_subfeatures, :post_prune, :merge_purity_threshold, :display_depth, :feature_importance, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Int64\", \"Int64\", \"Float64\", \"Int64\", \"Bool\", \"Float64\", \"Int64\", \"Symbol\", \"Union{Integer, Random.AbstractRNG}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`true`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[DecisionTree.RandomForestRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "DecisionTree"
":package_license" = "MIT"
":load_path" = "MLJDecisionTreeInterface.RandomForestRegressor"
":package_uuid" = "7806a523-6efd-50cb-b5f6-3fa6f1930dbb"
":package_url" = "https://github.com/bensadeghi/DecisionTree.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nRandomForestRegressor\n```\n\nA model type for constructing a CART random forest regressor, based on [DecisionTree.jl](https://github.com/bensadeghi/DecisionTree.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nRandomForestRegressor = @load RandomForestRegressor pkg=DecisionTree\n```\n\nDo `model = RandomForestRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `RandomForestRegressor(max_depth=...)`.\n\n`DecisionTreeRegressor` implements the standard [Random Forest algorithm](https://en.wikipedia.org/wiki/Random_forest), originally published in Breiman, L. (2001): \"Random Forests.\", *Machine Learning*, vol. 45, pp. 5–32\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have one of the following element scitypes: `Continuous`, `Count`, or `<:OrderedFactor`; check column scitypes with `schema(X)`\n  * `y`: the target, which can be any `AbstractVector` whose element scitype is `Continuous`; check the scitype with `scitype(y)`\n\nTrain the machine with `fit!(mach, rows=...)`.\n\n# Hyperparameters\n\n  * `max_depth=-1`: max depth of the decision tree (-1=any)\n  * `min_samples_leaf=1`: min number of samples each leaf needs to have\n  * `min_samples_split=2`: min number of samples needed for a split\n  * `min_purity_increase=0`: min purity needed for a split\n  * `n_subfeatures=-1`: number of features to select at random (0 for all, -1 for square root of number of features)\n  * `n_trees=10`: number of trees to train\n  * `sampling_fraction=0.7`  fraction of samples to train each tree on\n  * `feature_importance`: method to use for computing feature importances. One of `(:impurity, :split)`\n  * `rng=Random.GLOBAL_RNG`: random number generator or seed\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given new features `Xnew` having the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `forest`: the `Ensemble` object returned by the core DecisionTree.jl algorithm\n\n# Report\n\n  * `features`: the names of the features encountered in training\n\n# Accessor functions\n\n  * `feature_importances(mach)` returns a vector of `(feature::Symbol => importance)` pairs; the type of importance is determined by the hyperparameter `feature_importance` (see above)\n\n# Examples\n\n```\nusing MLJ\nForest = @load RandomForestRegressor pkg=DecisionTree\nforest = Forest(max_depth=4, min_samples_split=3)\n\nX, y = make_regression(100, 2) # synthetic data\nmach = machine(forest, X, y) |> fit!\n\nXnew, _ = make_regression(3, 2)\nyhat = predict(mach, Xnew) # new predictions\n\nfitted_params(mach).forest # raw `Ensemble` object from DecisionTree.jl\nfeature_importances(mach)\n```\n\nSee also [DecisionTree.jl](https://github.com/bensadeghi/DecisionTree.jl) and the unwrapped model type [`MLJDecisionTreeInterface.DecisionTree.RandomForestRegressor`](@ref).\n"
":name" = "RandomForestRegressor"
":human_name" = "CART random forest regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":reformat", ":selectrows", ":update", ":feature_importances"]
":hyperparameters" = "`(:max_depth, :min_samples_leaf, :min_samples_split, :min_purity_increase, :n_subfeatures, :n_trees, :sampling_fraction, :feature_importance, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Int64\", \"Int64\", \"Float64\", \"Int64\", \"Int64\", \"Float64\", \"Symbol\", \"Union{Integer, Random.AbstractRNG}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = ":n_trees"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`true`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[DecisionTree.RandomForestClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "DecisionTree"
":package_license" = "MIT"
":load_path" = "MLJDecisionTreeInterface.RandomForestClassifier"
":package_uuid" = "7806a523-6efd-50cb-b5f6-3fa6f1930dbb"
":package_url" = "https://github.com/bensadeghi/DecisionTree.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nRandomForestClassifier\n```\n\nA model type for constructing a CART random forest classifier, based on [DecisionTree.jl](https://github.com/bensadeghi/DecisionTree.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nRandomForestClassifier = @load RandomForestClassifier pkg=DecisionTree\n```\n\nDo `model = RandomForestClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `RandomForestClassifier(max_depth=...)`.\n\n`RandomForestClassifier` implements the standard [Random Forest algorithm](https://en.wikipedia.org/wiki/Random_forest), originally published in Breiman, L. (2001): \"Random Forests.\", *Machine Learning*, vol. 45, pp. 5–32.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have one of the following element scitypes: `Continuous`, `Count`, or `<:OrderedFactor`; check column scitypes with `schema(X)`\n  * `y`: the target, which can be any `AbstractVector` whose element scitype is `<:OrderedFactor` or `<:Multiclass`; check the scitype with `scitype(y)`\n\nTrain the machine with `fit!(mach, rows=...)`.\n\n# Hyperparameters\n\n  * `max_depth=-1`:          max depth of the decision tree (-1=any)\n  * `min_samples_leaf=1`:    min number of samples each leaf needs to have\n  * `min_samples_split=2`:   min number of samples needed for a split\n  * `min_purity_increase=0`: min purity needed for a split\n  * `n_subfeatures=-1`: number of features to select at random (0 for all, -1 for square root of number of features)\n  * `n_trees=10`:            number of trees to train\n  * `sampling_fraction=0.7`  fraction of samples to train each tree on\n  * `feature_importance`: method to use for computing feature importances. One of `(:impurity, :split)`\n  * `rng=Random.GLOBAL_RNG`: random number generator or seed\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given features `Xnew` having the same scitype as `X` above. Predictions are probabilistic, but uncalibrated.\n  * `predict_mode(mach, Xnew)`: instead return the mode of each prediction above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `forest`: the `Ensemble` object returned by the core DecisionTree.jl algorithm\n\n# Report\n\n  * `features`: the names of the features encountered in training\n\n# Accessor functions\n\n  * `feature_importances(mach)` returns a vector of `(feature::Symbol => importance)` pairs; the type of importance is determined by the hyperparameter `feature_importance` (see above)\n\n# Examples\n\n```\nusing MLJ\nForest = @load RandomForestClassifier pkg=DecisionTree\nforest = Forest(min_samples_split=6, n_subfeatures=3)\n\nX, y = @load_iris\nmach = machine(forest, X, y) |> fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\nyhat = predict(mach, Xnew) # probabilistic predictions\npredict_mode(mach, Xnew)   # point predictions\npdf.(yhat, \"virginica\")    # probabilities for the \"verginica\" class\n\nfitted_params(mach).forest # raw `Ensemble` object from DecisionTrees.jl\n\nfeature_importances(mach)  # `:impurity` feature importances\nforest.feature_importance = :split\nfeature_importance(mach)   # `:split` feature importances\n\n```\n\nSee also [DecisionTree.jl](https://github.com/bensadeghi/DecisionTree.jl) and the unwrapped model type [`MLJDecisionTreeInterface.DecisionTree.RandomForestClassifier`](@ref).\n"
":name" = "RandomForestClassifier"
":human_name" = "CART random forest classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":reformat", ":selectrows", ":update", ":feature_importances"]
":hyperparameters" = "`(:max_depth, :min_samples_leaf, :min_samples_split, :min_purity_increase, :n_subfeatures, :n_trees, :sampling_fraction, :feature_importance, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Int64\", \"Int64\", \"Float64\", \"Int64\", \"Int64\", \"Float64\", \"Symbol\", \"Union{Integer, Random.AbstractRNG}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = ":n_trees"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`true`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[Clustering.HierarchicalClustering]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "Clustering"
":package_license" = "MIT"
":load_path" = "MLJClusteringInterface.HierarchicalClustering"
":package_uuid" = "aaaa29a8-35af-508c-8bc3-b662a17a0fe5"
":package_url" = "https://github.com/JuliaStats/Clustering.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nHierarchicalClustering\n```\n\nA model type for constructing a hierarchical clusterer, based on [Clustering.jl](https://github.com/JuliaStats/Clustering.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nHierarchicalClustering = @load HierarchicalClustering pkg=Clustering\n```\n\nDo `model = HierarchicalClustering()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `HierarchicalClustering(linkage=...)`.\n\n[Hierarchical Clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering) is a clustering algorithm that organizes the data in a dendrogram based on distances between groups of points and computes cluster assignments by cutting the dendrogram at a given height. More information is available at the [Clustering.jl documentation](https://juliastats.org/Clustering.jl/stable/index.html). Use `predict` to get cluster assignments. The dendrogram and the dendrogram cutter are accessed from the machine report (see below).\n\nThis is a static implementation, i.e., it does not generalize to new data instances, and there is no training data. For clusterers that do generalize, see [`KMeans`](@ref) or [`KMedoids`](@ref).\n\nIn MLJ or MLJBase, create a machine with\n\n```\nmach = machine(model)\n```\n\n# Hyper-parameters\n\n  * `linkage = :single`: linkage method (:single, :average, :complete, :ward, :ward_presquared)\n  * `metric = SqEuclidean`: metric (see `Distances.jl` for available metrics)\n  * `branchorder = :r`: branchorder (:r, :barjoseph, :optimal)\n  * `h = nothing`: height at which the dendrogram is cut\n  * `k = 3`: number of clusters.\n\nIf both `k` and `h` are specified, it is guaranteed that the number of clusters is not less than `k` and their height is not above `h`.\n\n# Operations\n\n  * `predict(mach, X)`: return cluster label assignments, as an unordered `CategoricalVector`. Here `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n\n# Report\n\nAfter calling `predict(mach)`, the fields of `report(mach)`  are:\n\n  * `dendrogram`: the dendrogram that was computed when calling `predict`.\n  * `cutter`: a dendrogram cutter that can be called with a height `h` or a number of clusters `k`, to obtain a new assignment of the data points to clusters (see example below).\n\n# Examples\n\n```\nusing MLJ\n\nX, labels  = make_moons(400, noise=0.09, rng=1) # synthetic data with 2 clusters; X\n\nHierarchicalClustering = @load HierarchicalClustering pkg=Clustering\nmodel = HierarchicalClustering(linkage = :complete)\nmach = machine(model)\n\n# compute and output cluster assignments for observations in `X`:\nyhat = predict(mach, X)\n\n# plot dendrogram:\nusing StatsPlots\nplot(report(mach).dendrogram)\n\n# make new predictions by cutting the dendrogram at another height\nreport(mach).cutter(h = 2.5)\n```\n"
":name" = "HierarchicalClustering"
":human_name" = "hierarchical clusterer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Static`"
":implemented_methods" = [":clean!", ":predict"]
":hyperparameters" = "`(:linkage, :metric, :branchorder, :h, :k)`"
":hyperparameter_types" = "`(\"Symbol\", \"Distances.SemiMetric\", \"Symbol\", \"Union{Nothing, Float64}\", \"Int64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`(:predict,)`"

[Clustering.DBSCAN]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "Clustering"
":package_license" = "MIT"
":load_path" = "MLJClusteringInterface.DBSCAN"
":package_uuid" = "aaaa29a8-35af-508c-8bc3-b662a17a0fe5"
":package_url" = "https://github.com/JuliaStats/Clustering.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nDBSCAN\n```\n\nA model type for constructing a DBSCAN clusterer (density-based spatial clustering of applications with noise), based on [Clustering.jl](https://github.com/JuliaStats/Clustering.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nDBSCAN = @load DBSCAN pkg=Clustering\n```\n\nDo `model = DBSCAN()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `DBSCAN(radius=...)`.\n\n[DBSCAN](https://en.wikipedia.org/wiki/DBSCAN) is a clustering algorithm that groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away). More information is available at the [Clustering.jl documentation](https://juliastats.org/Clustering.jl/stable/index.html). Use `predict` to get cluster assignments. Point types - core, boundary or noise - are accessed from the machine report (see below).\n\nThis is a static implementation, i.e., it does not generalize to new data instances, and there is no training data. For clusterers that do generalize, see [`KMeans`](@ref) or [`KMedoids`](@ref).\n\nIn MLJ or MLJBase, create a machine with\n\n```\nmach = machine(model)\n```\n\n# Hyper-parameters\n\n  * `radius=1.0`: query radius.\n  * `leafsize=20`: number of points binned in each leaf node of the nearest neighbor k-d tree.\n  * `min_neighbors=1`: minimum number of a core point neighbors.\n  * `min_cluster_size=1`: minimum number of points in a valid cluster.\n\n# Operations\n\n  * `predict(mach, X)`: return cluster label assignments, as an unordered `CategoricalVector`. Here `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`. Note that points of type `noise` will always get a label of `0`.\n\n# Report\n\nAfter calling `predict(mach)`, the fields of `report(mach)`  are:\n\n  * `point_types`: A `CategoricalVector` with the DBSCAN point type classification, one element per row of `X`. Elements are either `'C'` (core), `'B'` (boundary), or `'N'` (noise).\n  * `nclusters`: The number of clusters (excluding the noise \"cluster\")\n  * `cluster_labels`: The unique list of cluster labels\n  * `clusters`: A vector of `Clustering.DbscanCluster` objects from Clustering.jl, which have these fields:\n\n      * `size`: number of points in a cluster (core + boundary)\n      * `core_indices`: indices of points in the cluster core\n      * `boundary_indices`: indices of points on the cluster boundary\n\n# Examples\n\n```\nusing MLJ\n\nX, labels  = make_moons(400, noise=0.09, rng=1) # synthetic data with 2 clusters; X\ny = map(labels) do label\n    label == 0 ? \"cookie\" : \"monster\"\nend;\ny = coerce(y, Multiclass);\n\nDBSCAN = @load DBSCAN pkg=Clustering\nmodel = DBSCAN(radius=0.13, min_cluster_size=5)\nmach = machine(model)\n\n# compute and output cluster assignments for observations in `X`:\nyhat = predict(mach, X)\n\n# get DBSCAN point types:\nreport(mach).point_types\nreport(mach).nclusters\n\n# compare cluster labels with actual labels:\ncompare = zip(yhat, y) |> collect;\ncompare[1:10] # clusters align with classes\n\n# visualize clusters, noise in red:\npoints = zip(X.x1, X.x2) |> collect\ncolors = map(yhat) do i\n   i == 0 ? :red :\n   i == 1 ? :blue :\n   i == 2 ? :green :\n   i == 3 ? :yellow :\n   :black\nend\nusing Plots\nscatter(points, color=colors)\n```\n"
":name" = "DBSCAN"
":human_name" = "DBSCAN clusterer (density-based spatial clustering of applications with noise)"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Static`"
":implemented_methods" = [":clean!", ":predict"]
":hyperparameters" = "`(:radius, :leafsize, :min_neighbors, :min_cluster_size)`"
":hyperparameter_types" = "`(\"Real\", \"Int64\", \"Int64\", \"Int64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`(:predict,)`"

[Clustering.KMeans]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "Clustering"
":package_license" = "MIT"
":load_path" = "MLJClusteringInterface.KMeans"
":package_uuid" = "aaaa29a8-35af-508c-8bc3-b662a17a0fe5"
":package_url" = "https://github.com/JuliaStats/Clustering.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nKMeans\n```\n\nA model type for constructing a K-means clusterer, based on [Clustering.jl](https://github.com/JuliaStats/Clustering.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nKMeans = @load KMeans pkg=Clustering\n```\n\nDo `model = KMeans()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `KMeans(k=...)`.\n\n[K-means](http://en.wikipedia.org/wiki/K_means) is a classical method for clustering or vector quantization. It produces a fixed number of clusters, each associated with a *center* (also known as a *prototype*), and each data point is assigned to a cluster with the nearest center.\n\nFrom a mathematical standpoint, K-means is a coordinate descent algorithm that solves the following optimization problem:\n\n$$\n\\text{minimize} \\ \\sum_{i=1}^n \\| \\mathbf{x}_i - \\boldsymbol{\\mu}_{z_i} \\|^2 \\ \\text{w.r.t.} \\ (\\boldsymbol{\\mu}, z)\n$$\n\nHere, $\\boldsymbol{\\mu}_k$ is the center of the $k$-th cluster, and $z_i$ is an index of the cluster for $i$-th point $\\mathbf{x}_i$.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column  scitypes with `schema(X)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `k=3`: The number of centroids to use in clustering.\n  * `metric::SemiMetric=Distances.SqEuclidean`: The metric used to calculate the clustering. Must have type `PreMetric` from Distances.jl.\n  * `init = :kmpp`: One of the following options to indicate how cluster seeds should be initialized:\n\n      * `:kmpp`: KMeans++\n      * `:kmenc`: K-medoids initialization based on centrality\n      * `:rand`: random\n      * an instance of `Clustering.SeedingAlgorithm` from Clustering.jl\n      * an integer vector of length `k` that provides the indices of points to use as initial cluster centers.\n\n    See [documentation of Clustering.jl](https://juliastats.org/Clustering.jl/stable/kmeans.html#Clustering.kmeans).\n\n# Operations\n\n  * `predict(mach, Xnew)`: return cluster label assignments, given new  features `Xnew` having the same Scitype as `X` above.\n  * `transform(mach, Xnew)`: instead return the mean pairwise distances from  new samples to the cluster centers.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `centers`: The coordinates of the cluster centers.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `assignments`: The cluster assignments of each point in the training data.\n  * `cluster_labels`: The labels assigned to each cluster.\n\n# Examples\n\n```\nusing MLJ\nKMeans = @load KMeans pkg=Clustering\n\ntable = load_iris()\ny, X = unpack(table, ==(:target), rng=123)\nmodel = KMeans(k=3)\nmach = machine(model, X) |> fit!\n\nyhat = predict(mach, X)\n@assert yhat == report(mach).assignments\n\ncompare = zip(yhat, y) |> collect;\ncompare[1:8] # clusters align with classes\n\ncenter_dists = transform(mach, fitted_params(mach).centers')\n\n@assert center_dists[1][1] == 0.0\n@assert center_dists[2][2] == 0.0\n@assert center_dists[3][3] == 0.0\n```\n\nSee also [`KMedoids`](@ref)\n"
":name" = "KMeans"
":human_name" = "K-means clusterer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":transform"]
":hyperparameters" = "`(:k, :metric, :init)`"
":hyperparameter_types" = "`(\"Int64\", \"Distances.SemiMetric\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[Clustering.KMedoids]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "Clustering"
":package_license" = "MIT"
":load_path" = "MLJClusteringInterface.KMedoids"
":package_uuid" = "aaaa29a8-35af-508c-8bc3-b662a17a0fe5"
":package_url" = "https://github.com/JuliaStats/Clustering.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nKMedoids\n```\n\nA model type for constructing a K-medoids clusterer, based on [Clustering.jl](https://github.com/JuliaStats/Clustering.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nKMedoids = @load KMedoids pkg=Clustering\n```\n\nDo `model = KMedoids()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `KMedoids(k=...)`.\n\n[K-medoids](http://en.wikipedia.org/wiki/K-medoids) is a clustering algorithm that works by finding $k$ data points (called *medoids*) such that the total distance between each data point and the closest *medoid* is minimal.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `k=3`: The number of centroids to use in clustering.\n  * `metric::SemiMetric=Distances.SqEuclidean`: The metric used to calculate the clustering. Must have type `PreMetric` from Distances.jl.\n  * `init` (defaults to `:kmpp`): how medoids should be initialized, could  be one of the following:\n\n      * `:kmpp`: KMeans++\n      * `:kmenc`: K-medoids initialization based on centrality\n      * `:rand`: random\n      * an instance of `Clustering.SeedingAlgorithm` from Clustering.jl\n      * an integer vector of length `k` that provides the indices of points to use as initial medoids.\n\n    See [documentation of Clustering.jl](https://juliastats.org/Clustering.jl/stable/kmedoids.html#Clustering.kmedoids).\n\n# Operations\n\n  * `predict(mach, Xnew)`: return cluster label assignments, given new  features `Xnew` having the same Scitype as `X` above.\n  * `transform(mach, Xnew)`: instead return the mean pairwise distances from  new samples to the cluster centers.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `medoids`: The coordinates of the cluster medoids.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `assignments`: The cluster assignments of each point in the training data.\n  * `cluster_labels`: The labels assigned to each cluster.\n\n# Examples\n\n```\nusing MLJ\nKMedoids = @load KMedoids pkg=Clustering\n\ntable = load_iris()\ny, X = unpack(table, ==(:target), rng=123)\nmodel = KMedoids(k=3)\nmach = machine(model, X) |> fit!\n\nyhat = predict(mach, X)\n@assert yhat == report(mach).assignments\n\ncompare = zip(yhat, y) |> collect;\ncompare[1:8] # clusters align with classes\n\ncenter_dists = transform(mach, fitted_params(mach).medoids')\n\n@assert center_dists[1][1] == 0.0\n@assert center_dists[2][2] == 0.0\n@assert center_dists[3][3] == 0.0\n```\n\nSee also [`KMeans`](@ref)\n"
":name" = "KMedoids"
":human_name" = "K-medoids clusterer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":transform"]
":hyperparameters" = "`(:k, :metric, :init)`"
":hyperparameter_types" = "`(\"Int64\", \"Distances.SemiMetric\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[EvoLinear.EvoSplineRegressor]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "EvoLinear"
":package_license" = "MIT"
":load_path" = "EvoLinear.EvoSplineRegressor"
":package_uuid" = "ab853011-1780-437f-b4b5-5de6f4777246"
":package_url" = "https://github.com/jeremiedb/EvoLinear.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nEvoSplineRegressor(; kwargs...)\n```\n\nA model type for constructing a EvoSplineRegressor, based on [EvoLinear.jl](https://github.com/jeremiedb/EvoLinear.jl), and implementing both an internal API and the MLJ model interface.\n\n# Keyword arguments\n\n  * `loss=:mse`: loss function to be minimised.    Can be one of:\n\n      * `:mse`\n      * `:logistic`\n      * `:poisson`\n      * `:gamma`\n      * `:tweedie`\n  * `nrounds=10`: maximum number of training rounds.\n  * `eta=1`: Learning rate. Typically in the range `[1e-2, 1]`.\n  * `L1=0`: Regularization penalty applied by shrinking to 0 weight update if update is < L1. No penalty if update > L1. Results in sparse feature selection. Typically in the `[0, 1]` range on normalized features.\n  * `L2=0`: Regularization penalty applied to the squared of the weight update value. Restricts large parameter values. Typically in the `[0, 1]` range on normalized features.\n  * `rng=123`: random seed. Not used at the moment.\n  * `updater=:all`: training method. Only `:all` is supported at the moment. Gradients for each feature are computed simultaneously, then bias is updated based on all features update.\n  * `device=:cpu`: Only `:cpu` is supported at the moment.\n\n# Internal API\n\nDo `config = EvoSplineRegressor()` to construct an hyper-parameter struct with default hyper-parameters. Provide keyword arguments as listed above to override defaults, for example:\n\n```julia\nEvoSplineRegressor(loss=:logistic, L1=1e-3, L2=1e-2, nrounds=100)\n```\n\n## Training model\n\nA model is built using [`fit`](@ref):\n\n```julia\nconfig = EvoSplineRegressor()\nm = fit(config; x, y, w)\n```\n\n## Inference\n\nFitted results is an `EvoLinearModel` which acts as a prediction function when passed a features matrix as argument.  \n\n```julia\npreds = m(x)\n```\n\n# MLJ Interface\n\nFrom MLJ, the type can be imported using:\n\n```julia\nEvoSplineRegressor = @load EvoSplineRegressor pkg=EvoLinear\n```\n\nDo `model = EvoLinearRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `EvoSplineRegressor(loss=...)`.\n\n## Training model\n\nIn MLJ or MLJBase, bind an instance `model` to data with `mach = machine(model, X, y)` where: \n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have one of the following element scitypes: `Continuous`, `Count`, or `<:OrderedFactor`; check column scitypes with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `<:Continuous`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n## Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given\n\nfeatures `Xnew` having the same scitype as `X` above. Predictions   are deterministic.\n\n## Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `:fitresult`: the `SplineModel` object returned by EvoSplineRegressor fitting algorithm.\n\n## Report\n\nThe fields of `report(mach)` are:\n\n  * `:coef`: Vector of coefficients (βs) associated to each of the features.\n  * `:bias`: Value of the bias.\n  * `:names`: Names of each of the features.\n"
":name" = "EvoSplineRegressor"
":human_name" = "evo spline regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":predict", ":update"]
":hyperparameters" = "`(:nrounds, :opt, :batchsize, :act, :eta, :L2, :knots, :rng, :device)`"
":hyperparameter_types" = "`(\"Int64\", \"Symbol\", \"Int64\", \"Symbol\", \"Any\", \"Any\", \"Union{Nothing, Dict}\", \"Any\", \"Symbol\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = ":nrounds"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[EvoLinear.EvoLinearRegressor]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "EvoLinear"
":package_license" = "MIT"
":load_path" = "EvoLinear.EvoLinearRegressor"
":package_uuid" = "ab853011-1780-437f-b4b5-5de6f4777246"
":package_url" = "https://github.com/jeremiedb/EvoLinear.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nEvoLinearRegressor(; kwargs...)\n```\n\nA model type for constructing a EvoLinearRegressor, based on [EvoLinear.jl](https://github.com/jeremiedb/EvoLinear.jl), and implementing both an internal API and the MLJ model interface.\n\n# Keyword arguments\n\n  * `loss=:mse`: loss function to be minimised.    Can be one of:\n\n      * `:mse`\n      * `:logistic`\n      * `:poisson`\n      * `:gamma`\n      * `:tweedie`\n  * `nrounds=10`: maximum number of training rounds.\n  * `eta=1`: Learning rate. Typically in the range `[1e-2, 1]`.\n  * `L1=0`: Regularization penalty applied by shrinking to 0 weight update if update is < L1. No penalty if update > L1. Results in sparse feature selection. Typically in the `[0, 1]` range on normalized features.\n  * `L2=0`: Regularization penalty applied to the squared of the weight update value. Restricts large parameter values. Typically in the `[0, 1]` range on normalized features.\n  * `rng=123`: random seed. Not used at the moment.\n  * `updater=:all`: training method. Only `:all` is supported at the moment. Gradients for each feature are computed simultaneously, then bias is updated based on all features update.\n  * `device=:cpu`: Only `:cpu` is supported at the moment.\n\n# Internal API\n\nDo `config = EvoLinearRegressor()` to construct an hyper-parameter struct with default hyper-parameters. Provide keyword arguments as listed above to override defaults, for example:\n\n```julia\nEvoLinearRegressor(loss=:logistic, L1=1e-3, L2=1e-2, nrounds=100)\n```\n\n## Training model\n\nA model is built using [`fit`](@ref):\n\n```julia\nconfig = EvoLinearRegressor()\nm = fit(config; x, y, w)\n```\n\n## Inference\n\nFitted results is an `EvoLinearModel` which acts as a prediction function when passed a features matrix as argument.  \n\n```julia\npreds = m(x)\n```\n\n# MLJ Interface\n\nFrom MLJ, the type can be imported using:\n\n```julia\nEvoLinearRegressor = @load EvoLinearRegressor pkg=EvoLinear\n```\n\nDo `model = EvoLinearRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `EvoLinearRegressor(loss=...)`.\n\n## Training model\n\nIn MLJ or MLJBase, bind an instance `model` to data with `mach = machine(model, X, y)` where: \n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have one of the following element scitypes: `Continuous`, `Count`, or `<:OrderedFactor`; check column scitypes with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `<:Continuous`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n## Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given\n\nfeatures `Xnew` having the same scitype as `X` above. Predictions   are deterministic.\n\n## Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `:fitresult`: the `EvoLinearModel` object returned by EvoLnear.jl fitting algorithm.\n\n## Report\n\nThe fields of `report(mach)` are:\n\n  * `:coef`: Vector of coefficients (βs) associated to each of the features.\n  * `:bias`: Value of the bias.\n  * `:names`: Names of each of the features.\n"
":name" = "EvoLinearRegressor"
":human_name" = "evo linear regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":predict", ":update"]
":hyperparameters" = "`(:updater, :nrounds, :eta, :L1, :L2, :rng, :device)`"
":hyperparameter_types" = "`(\"Symbol\", \"Int64\", \"Any\", \"Any\", \"Any\", \"Any\", \"Symbol\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = ":nrounds"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[XGBoost.XGBoostCount]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Count}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Count}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Count}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "XGBoost"
":package_license" = "unknown"
":load_path" = "MLJXGBoostInterface.XGBoostCount"
":package_uuid" = "009559a3-9522-5dbb-924b-0b6ed2b22bb9"
":package_url" = "https://github.com/dmlc/XGBoost.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nXGBoostCount\n```\n\nA model type for constructing a eXtreme Gradient Boosting Count Regressor, based on [XGBoost.jl](https://github.com/dmlc/XGBoost.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nXGBoostCount = @load XGBoostCount pkg=XGBoost\n```\n\nDo `model = XGBoostCount()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `XGBoostCount(test=...)`.\n\nUnivariate discrete regression using [xgboost](https://xgboost.readthedocs.io/en/stable/index.html).\n\n# Training data\n\nIn `MLJ` or `MLJBase`, bind an instance `model` to data with\n\n```julia\nm = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features, either an `AbstractMatrix` or Tables.jl-compatible table.\n  * `y`: is an `AbstractVector` continuous target.\n\nTrain using `fit!(m, rows=...)`.\n\n# Hyper-parameters\n\nSee https://xgboost.readthedocs.io/en/stable/parameter.html.\n"
":name" = "XGBoostCount"
":human_name" = "eXtreme Gradient Boosting Count Regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!"]
":hyperparameters" = "`(:test, :num_round, :booster, :disable_default_eval_metric, :eta, :num_parallel_tree, :gamma, :max_depth, :min_child_weight, :max_delta_step, :subsample, :colsample_bytree, :colsample_bylevel, :colsample_bynode, :lambda, :alpha, :tree_method, :sketch_eps, :scale_pos_weight, :updater, :refresh_leaf, :process_type, :grow_policy, :max_leaves, :max_bin, :predictor, :sample_type, :normalize_type, :rate_drop, :one_drop, :skip_drop, :feature_selector, :top_k, :tweedie_variance_power, :objective, :base_score, :watchlist, :nthread, :importance_type, :seed, :validate_parameters)`"
":hyperparameter_types" = "`(\"Int64\", \"Int64\", \"String\", \"Union{Bool, Int64}\", \"Float64\", \"Int64\", \"Float64\", \"Int64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"String\", \"Float64\", \"Float64\", \"Union{Nothing, String}\", \"Union{Bool, Int64}\", \"String\", \"String\", \"Int64\", \"Int64\", \"String\", \"String\", \"String\", \"Float64\", \"Union{Bool, Int64}\", \"Float64\", \"String\", \"Int64\", \"Float64\", \"Any\", \"Float64\", \"Any\", \"Int64\", \"String\", \"Union{Nothing, Int64}\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`true`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[XGBoost.XGBoostRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "XGBoost"
":package_license" = "unknown"
":load_path" = "MLJXGBoostInterface.XGBoostRegressor"
":package_uuid" = "009559a3-9522-5dbb-924b-0b6ed2b22bb9"
":package_url" = "https://github.com/dmlc/XGBoost.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nXGBoostRegressor\n```\n\nA model type for constructing a eXtreme Gradient Boosting Regressor, based on [XGBoost.jl](https://github.com/dmlc/XGBoost.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nXGBoostRegressor = @load XGBoostRegressor pkg=XGBoost\n```\n\nDo `model = XGBoostRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `XGBoostRegressor(test=...)`.\n\nUnivariate continuous regression using [xgboost](https://xgboost.readthedocs.io/en/stable/index.html).\n\n# Training data\n\nIn `MLJ` or `MLJBase`, bind an instance `model` to data with\n\n```julia\nm = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features whose columns have `Continuous` element scitype; check  column scitypes with `schema(X)`.\n  * `y`: is an `AbstractVector` target with `Continuous` elements; check the scitype with `scitype(y)`.\n\nTrain using `fit!(m, rows=...)`.\n\n# Hyper-parameters\n\nSee https://xgboost.readthedocs.io/en/stable/parameter.html.\n"
":name" = "XGBoostRegressor"
":human_name" = "eXtreme Gradient Boosting Regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!"]
":hyperparameters" = "`(:test, :num_round, :booster, :disable_default_eval_metric, :eta, :num_parallel_tree, :gamma, :max_depth, :min_child_weight, :max_delta_step, :subsample, :colsample_bytree, :colsample_bylevel, :colsample_bynode, :lambda, :alpha, :tree_method, :sketch_eps, :scale_pos_weight, :updater, :refresh_leaf, :process_type, :grow_policy, :max_leaves, :max_bin, :predictor, :sample_type, :normalize_type, :rate_drop, :one_drop, :skip_drop, :feature_selector, :top_k, :tweedie_variance_power, :objective, :base_score, :watchlist, :nthread, :importance_type, :seed, :validate_parameters)`"
":hyperparameter_types" = "`(\"Int64\", \"Int64\", \"String\", \"Union{Bool, Int64}\", \"Float64\", \"Int64\", \"Float64\", \"Int64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"String\", \"Float64\", \"Float64\", \"Union{Nothing, String}\", \"Union{Bool, Int64}\", \"String\", \"String\", \"Int64\", \"Int64\", \"String\", \"String\", \"String\", \"Float64\", \"Union{Bool, Int64}\", \"Float64\", \"String\", \"Int64\", \"Float64\", \"Any\", \"Float64\", \"Any\", \"Int64\", \"String\", \"Union{Nothing, Int64}\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`true`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[XGBoost.XGBoostClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "XGBoost"
":package_license" = "unknown"
":load_path" = "MLJXGBoostInterface.XGBoostClassifier"
":package_uuid" = "009559a3-9522-5dbb-924b-0b6ed2b22bb9"
":package_url" = "https://github.com/dmlc/XGBoost.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nXGBoostClassifier\n```\n\nA model type for constructing a eXtreme Gradient Boosting Classifier, based on [XGBoost.jl](https://github.com/dmlc/XGBoost.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nXGBoostClassifier = @load XGBoostClassifier pkg=XGBoost\n```\n\nDo `model = XGBoostClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `XGBoostClassifier(test=...)`.\n\nUnivariate classification using [xgboost](https://xgboost.readthedocs.io/en/stable/index.html).\n\n# Training data\n\nIn `MLJ` or `MLJBase`, bind an instance `model` to data with\n\n```julia\nm = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features, either an `AbstractMatrix` or Tables.jl-compatible table.\n  * `y`: is an `AbstractVector` `Finite` target.\n\nTrain using `fit!(m, rows=...)`.\n\n# Hyper-parameters\n\nSee https://xgboost.readthedocs.io/en/stable/parameter.html.\n"
":name" = "XGBoostClassifier"
":human_name" = "eXtreme Gradient Boosting Classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":predict"]
":hyperparameters" = "`(:test, :num_round, :booster, :disable_default_eval_metric, :eta, :num_parallel_tree, :gamma, :max_depth, :min_child_weight, :max_delta_step, :subsample, :colsample_bytree, :colsample_bylevel, :colsample_bynode, :lambda, :alpha, :tree_method, :sketch_eps, :scale_pos_weight, :updater, :refresh_leaf, :process_type, :grow_policy, :max_leaves, :max_bin, :predictor, :sample_type, :normalize_type, :rate_drop, :one_drop, :skip_drop, :feature_selector, :top_k, :tweedie_variance_power, :objective, :base_score, :watchlist, :nthread, :importance_type, :seed, :validate_parameters)`"
":hyperparameter_types" = "`(\"Int64\", \"Int64\", \"String\", \"Union{Bool, Int64}\", \"Float64\", \"Int64\", \"Float64\", \"Int64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"String\", \"Float64\", \"Float64\", \"Union{Nothing, String}\", \"Union{Bool, Int64}\", \"String\", \"String\", \"Int64\", \"Int64\", \"String\", \"String\", \"String\", \"Float64\", \"Union{Bool, Int64}\", \"Float64\", \"String\", \"Int64\", \"Float64\", \"Any\", \"Float64\", \"Any\", \"Int64\", \"String\", \"Union{Nothing, Int64}\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`true`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[LightGBM.LGBMClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "LightGBM"
":package_license" = "MIT Expat"
":load_path" = "LightGBM.MLJInterface.LGBMClassifier"
":package_uuid" = "7acf609c-83a4-11e9-1ffb-b912bcd3b04a"
":package_url" = "https://github.com/IQVIA-ML/LightGBM.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Microsoft LightGBM FFI wrapper: Classifier"
":name" = "LGBMClassifier"
":human_name" = "lgbm classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":predict", ":update"]
":hyperparameters" = "`(:boosting, :num_iterations, :learning_rate, :num_leaves, :max_depth, :tree_learner, :histogram_pool_size, :min_data_in_leaf, :min_sum_hessian_in_leaf, :max_delta_step, :lambda_l1, :lambda_l2, :min_gain_to_split, :feature_fraction, :feature_fraction_bynode, :feature_fraction_seed, :bagging_fraction, :pos_bagging_fraction, :neg_bagging_fraction, :bagging_freq, :bagging_seed, :early_stopping_round, :extra_trees, :extra_seed, :max_bin, :bin_construct_sample_cnt, :init_score, :drop_rate, :max_drop, :skip_drop, :xgboost_dart_mode, :uniform_drop, :drop_seed, :top_rate, :other_rate, :min_data_per_group, :max_cat_threshold, :cat_l2, :cat_smooth, :objective, :categorical_feature, :data_random_seed, :is_sparse, :is_unbalance, :boost_from_average, :scale_pos_weight, :use_missing, :feature_pre_filter, :metric, :metric_freq, :is_training_metric, :ndcg_at, :num_machines, :num_threads, :local_listen_port, :time_out, :machine_list_file, :save_binary, :device_type, :force_col_wise, :force_row_wise, :truncate_booster)`"
":hyperparameter_types" = "`(\"String\", \"Int64\", \"Float64\", \"Int64\", \"Int64\", \"String\", \"Float64\", \"Int64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Int64\", \"Float64\", \"Float64\", \"Float64\", \"Int64\", \"Int64\", \"Int64\", \"Bool\", \"Int64\", \"Int64\", \"Any\", \"String\", \"Float64\", \"Int64\", \"Float64\", \"Bool\", \"Bool\", \"Int64\", \"Float64\", \"Float64\", \"Int64\", \"Int64\", \"Float64\", \"Float64\", \"String\", \"Vector{Int64}\", \"Int64\", \"Bool\", \"Bool\", \"Bool\", \"Any\", \"Bool\", \"Bool\", \"Vector{String}\", \"Int64\", \"Bool\", \"Vector{Int64}\", \"Int64\", \"Int64\", \"Int64\", \"Int64\", \"String\", \"Bool\", \"String\", \"Bool\", \"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[LightGBM.LGBMRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "LightGBM"
":package_license" = "MIT Expat"
":load_path" = "LightGBM.MLJInterface.LGBMRegressor"
":package_uuid" = "7acf609c-83a4-11e9-1ffb-b912bcd3b04a"
":package_url" = "https://github.com/IQVIA-ML/LightGBM.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Microsoft LightGBM FFI wrapper: Regressor"
":name" = "LGBMRegressor"
":human_name" = "lgbm regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":predict", ":update"]
":hyperparameters" = "`(:boosting, :num_iterations, :learning_rate, :num_leaves, :max_depth, :tree_learner, :histogram_pool_size, :min_data_in_leaf, :min_sum_hessian_in_leaf, :max_delta_step, :lambda_l1, :lambda_l2, :min_gain_to_split, :feature_fraction, :feature_fraction_bynode, :feature_fraction_seed, :bagging_fraction, :bagging_freq, :bagging_seed, :early_stopping_round, :extra_trees, :extra_seed, :max_bin, :bin_construct_sample_cnt, :init_score, :drop_rate, :max_drop, :skip_drop, :xgboost_dart_mode, :uniform_drop, :drop_seed, :top_rate, :other_rate, :min_data_per_group, :max_cat_threshold, :cat_l2, :cat_smooth, :objective, :categorical_feature, :data_random_seed, :is_sparse, :is_unbalance, :boost_from_average, :use_missing, :feature_pre_filter, :alpha, :metric, :metric_freq, :is_training_metric, :ndcg_at, :num_machines, :num_threads, :local_listen_port, :time_out, :machine_list_file, :save_binary, :device_type, :force_col_wise, :force_row_wise, :truncate_booster)`"
":hyperparameter_types" = "`(\"String\", \"Int64\", \"Float64\", \"Int64\", \"Int64\", \"String\", \"Float64\", \"Int64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Int64\", \"Float64\", \"Int64\", \"Int64\", \"Int64\", \"Bool\", \"Int64\", \"Int64\", \"Any\", \"String\", \"Float64\", \"Int64\", \"Float64\", \"Bool\", \"Bool\", \"Int64\", \"Float64\", \"Float64\", \"Int64\", \"Int64\", \"Float64\", \"Float64\", \"String\", \"Vector{Int64}\", \"Int64\", \"Bool\", \"Bool\", \"Bool\", \"Bool\", \"Bool\", \"Float64\", \"Vector{String}\", \"Int64\", \"Bool\", \"Vector{Int64}\", \"Int64\", \"Int64\", \"Int64\", \"Int64\", \"String\", \"Bool\", \"String\", \"Bool\", \"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJText.TfidfTransformer]
":input_scitype" = "`Union{AbstractVector{<:AbstractVector{ScientificTypesBase.Textual}}, AbstractVector{<:ScientificTypesBase.Multiset{<:Tuple{Vararg{ScientificTypesBase.Textual, var\"_s1\"}} where var\"_s1\"}}, AbstractVector{<:ScientificTypesBase.Multiset{ScientificTypesBase.Textual}}}`"
":output_scitype" = "`AbstractMatrix{ScientificTypesBase.Continuous}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{Union{AbstractVector{<:AbstractVector{ScientificTypesBase.Textual}}, AbstractVector{<:ScientificTypesBase.Multiset{<:Tuple{Vararg{ScientificTypesBase.Textual, var\"_s1\"}} where var\"_s1\"}}, AbstractVector{<:ScientificTypesBase.Multiset{ScientificTypesBase.Textual}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractMatrix{ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`Union{AbstractVector{<:AbstractVector{ScientificTypesBase.Textual}}, AbstractVector{<:ScientificTypesBase.Multiset{<:Tuple{Vararg{ScientificTypesBase.Textual, var\"_s1\"}} where var\"_s1\"}}, AbstractVector{<:ScientificTypesBase.Multiset{ScientificTypesBase.Textual}}}`"
":is_pure_julia" = "`true`"
":package_name" = "MLJText"
":package_license" = "MIT"
":load_path" = "MLJText.TfidfTransformer"
":package_uuid" = "7876af07-990d-54b4-ab0e-23690620f79a"
":package_url" = "https://github.com/JuliaAI/MLJText.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nTfidfTransformer\n```\n\nA model type for constructing a TF-IFD transformer, based on [MLJText.jl](https://github.com/JuliaAI/MLJText.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nTfidfTransformer = @load TfidfTransformer pkg=MLJText\n```\n\nDo `model = TfidfTransformer()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `TfidfTransformer(max_doc_freq=...)`.\n\nThe transformer converts a collection of documents, tokenized or pre-parsed as bags of words/ngrams, to a matrix of [TF-IDF scores](https://en.wikipedia.org/wiki/Tf–idf#Inverse_document_frequency_2). Here \"TF\" means term-frequency while \"IDF\" means inverse document frequency (defined below). The TF-IDF score is the product of the two. This is a common term weighting scheme in information retrieval, that has also found good use in document classification. The goal of using TF-IDF instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.\n\nIn textbooks and implementations there is variation in the definition of IDF. Here two IDF definitions are available. The default, smoothed option provides the IDF for a term `t` as `log((1 + n)/(1 + df(t))) + 1`, where `n` is the total number of documents and `df(t)` the number of documents in which `t` appears. Setting `smooth_df = false` provides an IDF of `log(n/df(t)) + 1`.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nHere:\n\n  * `X` is any vector whose elements are either tokenized documents or bags of words/ngrams. Specifically, each element is one of the following:\n\n      * A vector of abstract strings (tokens), e.g., `[\"I\", \"like\", \"Sam\", \".\", \"Sam\", \"is\", \"nice\", \".\"]` (scitype `AbstractVector{Textual}`)\n      * A dictionary of counts, indexed on abstract strings, e.g., `Dict(\"I\"=>1, \"Sam\"=>2, \"Sam is\"=>1)` (scitype `Multiset{Textual}}`)\n      * A dictionary of counts, indexed on plain ngrams, e.g., `Dict((\"I\",)=>1, (\"Sam\",)=>2, (\"I\", \"Sam\")=>1)` (scitype `Multiset{<:NTuple{N,Textual} where N}`); here a *plain ngram* is a tuple of abstract strings.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `max_doc_freq=1.0`: Restricts the vocabulary that the transformer will consider.  Terms that occur in `> max_doc_freq` documents will not be considered by the transformer. For example, if `max_doc_freq` is set to 0.9, terms that are in more than 90% of the documents will be removed.\n  * `min_doc_freq=0.0`: Restricts the vocabulary that the transformer will consider.  Terms that occur in `< max_doc_freq` documents will not be considered by the transformer. A value of 0.01 means that only terms that are at least in 1% of the documents will be included.\n  * `smooth_idf=true`: Control which definition of IDF to use (see above).\n\n# Operations\n\n  * `transform(mach, Xnew)`: Based on the vocabulary and IDF learned in training, return the matrix of TF-IDF scores for `Xnew`, a vector of the same form as `X` above. The matrix has size `(n, p)`, where `n = length(Xnew)` and `p` the size of the vocabulary. Tokens/ngrams not appearing in the learned vocabulary are scored zero.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `vocab`: A vector containing the strings used in the transformer's vocabulary.\n  * `idf_vector`: The transformer's calculated IDF vector.\n\n# Examples\n\n`TfidfTransformer` accepts a variety of inputs. The example below transforms tokenized documents:\n\n```julia\nusing MLJ\nimport TextAnalysis\n\nTfidfTransformer = @load TfidfTransformer pkg=MLJText\n\ndocs = [\"Hi my name is Sam.\", \"How are you today?\"]\ntfidf_transformer = TfidfTransformer()\n\njulia> tokenized_docs = TextAnalysis.tokenize.(docs)\n2-element Vector{Vector{String}}:\n [\"Hi\", \"my\", \"name\", \"is\", \"Sam\", \".\"]\n [\"How\", \"are\", \"you\", \"today\", \"?\"]\n\nmach = machine(tfidf_transformer, tokenized_docs)\nfit!(mach)\n\nfitted_params(mach)\n\ntfidf_mat = transform(mach, tokenized_docs)\n```\n\nAlternatively, one can provide documents pre-parsed as ngrams counts:\n\n```julia\nusing MLJ\nimport TextAnalysis\n\ndocs = [\"Hi my name is Sam.\", \"How are you today?\"]\ncorpus = TextAnalysis.Corpus(TextAnalysis.NGramDocument.(docs, 1, 2))\nngram_docs = TextAnalysis.ngrams.(corpus)\n\njulia> ngram_docs[1]\nDict{AbstractString, Int64} with 11 entries:\n  \"is\"      => 1\n  \"my\"      => 1\n  \"name\"    => 1\n  \".\"       => 1\n  \"Hi\"      => 1\n  \"Sam\"     => 1\n  \"my name\" => 1\n  \"Hi my\"   => 1\n  \"name is\" => 1\n  \"Sam .\"   => 1\n  \"is Sam\"  => 1\n\ntfidf_transformer = TfidfTransformer()\nmach = machine(tfidf_transformer, ngram_docs)\nMLJ.fit!(mach)\nfitted_params(mach)\n\ntfidf_mat = transform(mach, ngram_docs)\n```\n\nSee also [`CountTransformer`](@ref), [`BM25Transformer`](@ref)\n"
":name" = "TfidfTransformer"
":human_name" = "TF-IFD transformer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fitted_params"]
":hyperparameters" = "`(:max_doc_freq, :min_doc_freq, :smooth_idf)`"
":hyperparameter_types" = "`(\"Float64\", \"Float64\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJText.CountTransformer]
":input_scitype" = "`Union{AbstractVector{<:AbstractVector{ScientificTypesBase.Textual}}, AbstractVector{<:ScientificTypesBase.Multiset{<:Tuple{Vararg{ScientificTypesBase.Textual, var\"_s1\"}} where var\"_s1\"}}, AbstractVector{<:ScientificTypesBase.Multiset{ScientificTypesBase.Textual}}}`"
":output_scitype" = "`AbstractMatrix{ScientificTypesBase.Continuous}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{Union{AbstractVector{<:AbstractVector{ScientificTypesBase.Textual}}, AbstractVector{<:ScientificTypesBase.Multiset{<:Tuple{Vararg{ScientificTypesBase.Textual, var\"_s1\"}} where var\"_s1\"}}, AbstractVector{<:ScientificTypesBase.Multiset{ScientificTypesBase.Textual}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractMatrix{ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`Union{AbstractVector{<:AbstractVector{ScientificTypesBase.Textual}}, AbstractVector{<:ScientificTypesBase.Multiset{<:Tuple{Vararg{ScientificTypesBase.Textual, var\"_s1\"}} where var\"_s1\"}}, AbstractVector{<:ScientificTypesBase.Multiset{ScientificTypesBase.Textual}}}`"
":is_pure_julia" = "`true`"
":package_name" = "MLJText"
":package_license" = "MIT"
":load_path" = "MLJText.CountTransformer"
":package_uuid" = "7876af07-990d-54b4-ab0e-23690620f79a"
":package_url" = "https://github.com/JuliaAI/MLJText.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nCountTransformer\n```\n\nA model type for constructing a count transformer, based on [MLJText.jl](https://github.com/JuliaAI/MLJText.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nCountTransformer = @load CountTransformer pkg=MLJText\n```\n\nDo `model = CountTransformer()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `CountTransformer(max_doc_freq=...)`.\n\nThe transformer converts a collection of documents, tokenized or pre-parsed as bags of words/ngrams, to a matrix of term counts.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nHere:\n\n  * `X` is any vector whose elements are either tokenized documents or bags of words/ngrams. Specifically, each element is one of the following:\n\n      * A vector of abstract strings (tokens), e.g., `[\"I\", \"like\", \"Sam\", \".\", \"Sam\", \"is\", \"nice\", \".\"]` (scitype `AbstractVector{Textual}`)\n      * A dictionary of counts, indexed on abstract strings, e.g., `Dict(\"I\"=>1, \"Sam\"=>2, \"Sam is\"=>1)` (scitype `Multiset{Textual}}`)\n      * A dictionary of counts, indexed on plain ngrams, e.g., `Dict((\"I\",)=>1, (\"Sam\",)=>2, (\"I\", \"Sam\")=>1)` (scitype `Multiset{<:NTuple{N,Textual} where N}`); here a *plain ngram* is a tuple of abstract strings.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `max_doc_freq=1.0`: Restricts the vocabulary that the transformer will consider. Terms that occur in `> max_doc_freq` documents will not be considered by the transformer. For example, if `max_doc_freq` is set to 0.9, terms that are in more than 90% of the documents will be removed.\n  * `min_doc_freq=0.0`: Restricts the vocabulary that the transformer will consider. Terms that occur in `< max_doc_freq` documents will not be considered by the transformer. A value of 0.01 means that only terms that are at least in 1% of the documents will be included.\n\n# Operations\n\n  * `transform(mach, Xnew)`: Based on the vocabulary learned in training, return the matrix of counts for `Xnew`, a vector of the same form as `X` above. The matrix has size `(n, p)`, where `n = length(Xnew)` and `p` the size of the vocabulary. Tokens/ngrams not appearing in the learned vocabulary are scored zero.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `vocab`: A vector containing the string used in the transformer's vocabulary.\n\n# Examples\n\n`CountTransformer` accepts a variety of inputs. The example below transforms tokenized documents:\n\n```julia\nusing MLJ\nimport TextAnalysis\n\nCountTransformer = @load CountTransformer pkg=MLJText\n\ndocs = [\"Hi my name is Sam.\", \"How are you today?\"]\ncount_transformer = CountTransformer()\n\njulia> tokenized_docs = TextAnalysis.tokenize.(docs)\n2-element Vector{Vector{String}}:\n [\"Hi\", \"my\", \"name\", \"is\", \"Sam\", \".\"]\n [\"How\", \"are\", \"you\", \"today\", \"?\"]\n\nmach = machine(count_transformer, tokenized_docs)\nfit!(mach)\n\nfitted_params(mach)\n\ntfidf_mat = transform(mach, tokenized_docs)\n```\n\nAlternatively, one can provide documents pre-parsed as ngrams counts:\n\n```julia\nusing MLJ\nimport TextAnalysis\n\ndocs = [\"Hi my name is Sam.\", \"How are you today?\"]\ncorpus = TextAnalysis.Corpus(TextAnalysis.NGramDocument.(docs, 1, 2))\nngram_docs = TextAnalysis.ngrams.(corpus)\n\njulia> ngram_docs[1]\nDict{AbstractString, Int64} with 11 entries:\n  \"is\"      => 1\n  \"my\"      => 1\n  \"name\"    => 1\n  \".\"       => 1\n  \"Hi\"      => 1\n  \"Sam\"     => 1\n  \"my name\" => 1\n  \"Hi my\"   => 1\n  \"name is\" => 1\n  \"Sam .\"   => 1\n  \"is Sam\"  => 1\n\ncount_transformer = CountTransformer()\nmach = machine(count_transformer, ngram_docs)\nMLJ.fit!(mach)\nfitted_params(mach)\n\ntfidf_mat = transform(mach, ngram_docs)\n```\n\nSee also [`TfidfTransformer`](@ref), [`BM25Transformer`](@ref)\n"
":name" = "CountTransformer"
":human_name" = "count transformer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fitted_params"]
":hyperparameters" = "`(:max_doc_freq, :min_doc_freq)`"
":hyperparameter_types" = "`(\"Float64\", \"Float64\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJText.BM25Transformer]
":input_scitype" = "`Union{AbstractVector{<:AbstractVector{ScientificTypesBase.Textual}}, AbstractVector{<:ScientificTypesBase.Multiset{<:Tuple{Vararg{ScientificTypesBase.Textual, var\"_s1\"}} where var\"_s1\"}}, AbstractVector{<:ScientificTypesBase.Multiset{ScientificTypesBase.Textual}}}`"
":output_scitype" = "`AbstractMatrix{ScientificTypesBase.Continuous}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{Union{AbstractVector{<:AbstractVector{ScientificTypesBase.Textual}}, AbstractVector{<:ScientificTypesBase.Multiset{<:Tuple{Vararg{ScientificTypesBase.Textual, var\"_s1\"}} where var\"_s1\"}}, AbstractVector{<:ScientificTypesBase.Multiset{ScientificTypesBase.Textual}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractMatrix{ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`Union{AbstractVector{<:AbstractVector{ScientificTypesBase.Textual}}, AbstractVector{<:ScientificTypesBase.Multiset{<:Tuple{Vararg{ScientificTypesBase.Textual, var\"_s1\"}} where var\"_s1\"}}, AbstractVector{<:ScientificTypesBase.Multiset{ScientificTypesBase.Textual}}}`"
":is_pure_julia" = "`true`"
":package_name" = "MLJText"
":package_license" = "MIT"
":load_path" = "MLJText.BM25Transformer"
":package_uuid" = "7876af07-990d-54b4-ab0e-23690620f79a"
":package_url" = "https://github.com/JuliaAI/MLJText.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nBM25Transformer\n```\n\nA model type for constructing a b m25 transformer, based on [MLJText.jl](https://github.com/JuliaAI/MLJText.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nBM25Transformer = @load BM25Transformer pkg=MLJText\n```\n\nDo `model = BM25Transformer()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `BM25Transformer(max_doc_freq=...)`.\n\nThe transformer converts a collection of documents, tokenized or pre-parsed as bags of words/ngrams, to a matrix of [Okapi BM25 document-word statistics](https://en.wikipedia.org/wiki/Okapi_BM25). The BM25 scoring function uses both term frequency (TF) and inverse document frequency (IDF, defined below), as in [`TfidfTransformer`](ref), but additionally adjusts for the probability that a user will consider a search result relevant based, on the terms in the search query and those in each document.\n\nIn textbooks and implementations there is variation in the definition of IDF. Here two IDF definitions are available. The default, smoothed option provides the IDF for a term `t` as `log((1 + n)/(1 + df(t))) + 1`, where `n` is the total number of documents and `df(t)` the number of documents in which `t` appears. Setting `smooth_df = false` provides an IDF of `log(n/df(t)) + 1`.\n\nReferences:\n\n  * http://ethen8181.github.io/machine-learning/search/bm25_intro.html\n  * https://en.wikipedia.org/wiki/Okapi_BM25\n  * https://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nIn textbooks and implementations there is variation in the definition of IDF. Here two IDF definitions are available. The default, smoothed option provides the IDF for a term `t` as `log((1 + n)/(1 + df(t))) + 1`, where `n` is the total number of documents and `df(t)` the number of documents in which `t` appears. Setting `smooth_df = false` provides an IDF of `log(n/df(t)) + 1`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `max_doc_freq=1.0`: Restricts the vocabulary that the transformer will consider. Terms that occur in `> max_doc_freq` documents will not be considered by the transformer. For example, if `max_doc_freq` is set to 0.9, terms that are in more than 90% of the documents will be removed.\n  * `min_doc_freq=0.0`: Restricts the vocabulary that the transformer will consider. Terms that occur in `< max_doc_freq` documents will not be considered by the transformer. A value of 0.01 means that only terms that are at least in 1% of the documents will be included.\n  * `κ=2`: The term frequency saturation characteristic. Higher values represent slower saturation. What we mean by saturation is the degree to which a term occurring extra times adds to the overall score.\n  * `β=0.075`: Amplifies the particular document length compared to the average length. The bigger β is, the more document length is amplified in terms of the overall score. The default value is 0.75, and the bounds are restricted between 0 and 1.\n  * `smooth_idf=true`: Control which definition of IDF to use (see above).\n\n# Operations\n\n  * `transform(mach, Xnew)`: Based on the vocabulary, IDF, and mean word counts learned in training, return the matrix of BM25 scores for `Xnew`, a vector of the same form as `X` above. The matrix has size `(n, p)`, where `n = length(Xnew)` and `p` the size of the vocabulary. Tokens/ngrams not appearing in the learned vocabulary are scored zero.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `vocab`: A vector containing the string used in the transformer's vocabulary.\n  * `idf_vector`: The transformer's calculated IDF vector.\n  * `mean_words_in_docs`: The mean number of words in each document.\n\n# Examples\n\n`BM25Transformer` accepts a variety of inputs. The example below transforms tokenized documents:\n\n```julia\nusing MLJ\nimport TextAnalysis\n\nBM25Transformer = @load BM25Transformer pkg=MLJText\n\ndocs = [\"Hi my name is Sam.\", \"How are you today?\"]\nbm25_transformer = BM25Transformer()\n\njulia> tokenized_docs = TextAnalysis.tokenize.(docs)\n2-element Vector{Vector{String}}:\n [\"Hi\", \"my\", \"name\", \"is\", \"Sam\", \".\"]\n [\"How\", \"are\", \"you\", \"today\", \"?\"]\n\nmach = machine(bm25_transformer, tokenized_docs)\nfit!(mach)\n\nfitted_params(mach)\n\ntfidf_mat = transform(mach, tokenized_docs)\n```\n\nAlternatively, one can provide documents pre-parsed as ngrams counts:\n\n```julia\nusing MLJ\nimport TextAnalysis\n\ndocs = [\"Hi my name is Sam.\", \"How are you today?\"]\ncorpus = TextAnalysis.Corpus(TextAnalysis.NGramDocument.(docs, 1, 2))\nngram_docs = TextAnalysis.ngrams.(corpus)\n\njulia> ngram_docs[1]\nDict{AbstractString, Int64} with 11 entries:\n  \"is\"      => 1\n  \"my\"      => 1\n  \"name\"    => 1\n  \".\"       => 1\n  \"Hi\"      => 1\n  \"Sam\"     => 1\n  \"my name\" => 1\n  \"Hi my\"   => 1\n  \"name is\" => 1\n  \"Sam .\"   => 1\n  \"is Sam\"  => 1\n\nbm25_transformer = BM25Transformer()\nmach = machine(bm25_transformer, ngram_docs)\nMLJ.fit!(mach)\nfitted_params(mach)\n\ntfidf_mat = transform(mach, ngram_docs)\n```\n\nSee also [`TfidfTransformer`](@ref), [`CountTransformer`](@ref)\n"
":name" = "BM25Transformer"
":human_name" = "b m25 transformer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fitted_params"]
":hyperparameters" = "`(:max_doc_freq, :min_doc_freq, :κ, :β, :smooth_idf)`"
":hyperparameter_types" = "`(\"Float64\", \"Float64\", \"Int64\", \"Float64\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[EvoTrees.EvoTreeClassifier]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}, Tuple{Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "EvoTrees"
":package_license" = "Apache"
":load_path" = "EvoTrees.EvoTreeClassifier"
":package_uuid" = "f6006082-12f8-11e9-0c9c-0d5d367ab1e5"
":package_url" = "https://github.com/Evovest/EvoTrees.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "EvoTreeClassifier(;kwargs...)\n\nA model type for constructing a EvoTreeClassifier, based on [EvoTrees.jl](https://github.com/Evovest/EvoTrees.jl), and implementing both an internal API and the MLJ model interface. EvoTreeClassifier is used to perform multi-class classification, using cross-entropy loss.\n\n# Hyper-parameters\n\n  * `nrounds=10`:                 Number of rounds. It corresponds to the number of trees that will be sequentially stacked.\n  * `lambda::T=0.0`:              L2 regularization term on weights. Must be >= 0. Higher lambda can result in a more robust model.\n  * `gamma::T=0.0`:               Minimum gain improvement needed to perform a node split. Higher gamma can result in a more robust model.\n  * `max_depth=5`:                Maximum depth of a tree. Must be >= 1. A tree of depth 1 is made of a single prediction leaf. A complete tree of depth N contains `2^(N - 1)` terminal leaves and `2^(N - 1) - 1` split nodes. Compute cost is proportional to `2^max_depth`. Typical optimal values are in the 3 to 9 range.\n  * `min_weight=0.0`:             Minimum weight needed in a node to perform a split. Matches the number of observations by default or the sum of weights as provided by the `weights` vector.\n  * `rowsample=1.0`:              Proportion of rows that are sampled at each iteration to build the tree. Should be in `]0, 1]`.\n  * `colsample=1.0`:              Proportion of columns / features that are sampled at each iteration to build the tree. Should be in `]0, 1]`.\n  * `nbins=32`:                   Number of bins into which each feature is quantized. Buckets are defined based on quantiles, hence resulting in equal weight bins.\n  * `rng=123`:                    Either an integer used as a seed to the random number generator or an actual random number generator (`::Random.AbstractRNG`).\n  * `device=\"cpu\"`:               Hardware device to use for computations. Can be either `\"cpu\"` or `\"gpu\"`.\n\n# Internal API\n\nDo `config = EvoTreeClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeClassifier(max_depth=...).\n\n## Training model\n\nA model is built using [`fit_evotree`](@ref):\n\n```julia\nmodel = fit_evotree(config; x_train, y_train, kwargs...)\n```\n\n## Inference\n\nPredictions are obtained using [`predict`](@ref) which returns a `Matrix` of size `[nobs, K]` where `K` is the number of classes:\n\n```julia\nEvoTrees.predict(model, X)\n```\n\nAlternatively, models act as a functor, returning predictions when called as a function with features as argument:\n\n```julia\nmodel(X)\n```\n\n# MLJ\n\nFrom MLJ, the type can be imported using:\n\n```julia\nEvoTreeClassifier = @load EvoTreeClassifier pkg=EvoTrees\n```\n\nDo `model = EvoTreeClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `EvoTreeClassifier(loss=...)`.\n\n## Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have one of the following element scitypes: `Continuous`, `Count`, or `<:OrderedFactor`; check column scitypes with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `<:Multiclas` or `<:OrderedFactor`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n## Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given features `Xnew` having the same scitype as `X` above. Predictions are probabilistic.\n  * `predict_mode(mach, Xnew)`: returns the mode of each of the prediction above.\n\n## Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `:fitresult`: The `GBTree` object returned by EvoTrees.jl fitting algorithm.\n\n## Report\n\nThe fields of `report(mach)` are:\n\n  * `:features`: The names of the features encountered in training.\n\n# Examples\n\n```\n# Internal API\nusing EvoTrees\nconfig = EvoTreeClassifier(max_depth=5, nbins=32, nrounds=100)\nnobs, nfeats = 1_000, 5\nx_train, y_train = randn(nobs, nfeats), rand(1:3, nobs)\nmodel = fit_evotree(config; x_train, y_train)\npreds = EvoTrees.predict(model, x_train)\n```\n\n```\n# MLJ Interface\nusing MLJ\nEvoTreeClassifier = @load EvoTreeClassifier pkg=EvoTrees\nmodel = EvoTreeClassifier(max_depth=5, nbins=32, nrounds=100)\nX, y = @load_iris\nmach = machine(model, X, y) |> fit!\npreds = predict(mach, X)\npreds = predict_mode(mach, X)\n```\n\nSee also [EvoTrees.jl](https://github.com/Evovest/EvoTrees.jl).\n"
":name" = "EvoTreeClassifier"
":human_name" = "evo tree classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":predict"]
":hyperparameters" = "`(:nrounds, :lambda, :gamma, :eta, :max_depth, :min_weight, :rowsample, :colsample, :nbins, :alpha, :rng, :device)`"
":hyperparameter_types" = "`(\"Int64\", \"Any\", \"Any\", \"Any\", \"Int64\", \"Any\", \"Any\", \"Any\", \"Int64\", \"Any\", \"Any\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = ":nrounds"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`true`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[EvoTrees.EvoTreeGaussian]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Continuous}}, Tuple{Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "EvoTrees"
":package_license" = "Apache"
":load_path" = "EvoTrees.EvoTreeGaussian"
":package_uuid" = "f6006082-12f8-11e9-0c9c-0d5d367ab1e5"
":package_url" = "https://github.com/Evovest/EvoTrees.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "EvoTreeGaussian(;kwargs...)\n\nA model type for constructing a EvoTreeGaussian, based on [EvoTrees.jl](https://github.com/Evovest/EvoTrees.jl), and implementing both an internal API the MLJ model interface. EvoTreeGaussian is used to perform Gaussian probabilistic regression, fitting μ and σ parameters to maximize likelihood.\n\n# Hyper-parameters\n\n  * `nrounds=10`:                 Number of rounds. It corresponds to the number of trees that will be sequentially stacked.\n  * `lambda::T=0.0`:              L2 regularization term on weights. Must be >= 0. Higher lambda can result in a more robust model.\n  * `gamma::T=0.0`:               Minimum gain imprvement needed to perform a node split. Higher gamma can result in a more robust model.\n  * `max_depth=5`:                Maximum depth of a tree. Must be >= 1. A tree of depth 1 is made of a single prediction leaf. A complete tree of depth N contains `2^(N - 1)` terminal leaves and `2^(N - 1) - 1` split nodes. Compute cost is proportional to 2^max_depth. Typical optimal values are in the 3 to 9 range.\n  * `min_weight=0.0`:             Minimum weight needed in a node to perform a split. Matches the number of observations by default or the sum of weights as provided by the `weights` vector.\n  * `rowsample=1.0`:              Proportion of rows that are sampled at each iteration to build the tree. Should be in `]0, 1]`.\n  * `colsample=1.0`:              Proportion of columns / features that are sampled at each iteration to build the tree. Should be in `]0, 1]`.\n  * `nbins=32`:                   Number of bins into which each feature is quantized. Buckets are defined based on quantiles, hence resulting in equal weight bins.\n  * `monotone_constraints=Dict{Int, Int}()`: Specify monotonic constraints using a dict where the key is the feature index and the value the applicable constraint (-1=decreasing, 0=none, 1=increasing).  !Experimental feature: note that for Gaussian regression, constraints may not be enforce systematically.\n  * `rng=123`:                    Either an integer used as a seed to the random number generator or an actual random number generator (`::Random.AbstractRNG`).\n  * `device=\"cpu\"`:               Hardware device to use for computations. Can be either `\"cpu\"` or `\"gpu\"`.\n\n# Internal API\n\nDo `config = EvoTreeGaussian()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeGaussian(max_depth=...).\n\n## Training model\n\nA model is built using [`fit_evotree`](@ref):\n\n```julia\nmodel = fit_evotree(config; x_train, y_train, kwargs...)\n```\n\n## Inference\n\nPredictions are obtained using [`predict`](@ref) which returns a `Matrix` of size `[nobs, 2]` where the second dimensions refer to `μ` and `σ` respectively:\n\n```julia\nEvoTrees.predict(model, X)\n```\n\nAlternatively, models act as a functor, returning predictions when called as a function with features as argument:\n\n```julia\nmodel(X)\n```\n\n# MLJ\n\nFrom MLJ, the type can be imported using:\n\n```julia\nEvoTreeGaussian = @load EvoTreeGaussian pkg=EvoTrees\n```\n\nDo `model = EvoTreeGaussian()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `EvoTreeGaussian(loss=...)`.\n\n## Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have one of the following element scitypes: `Continuous`, `Count`, or `<:OrderedFactor`; check column scitypes with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `<:Continuous`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n## Operations\n\n  * `predict(mach, Xnew)`: returns a vector of Gaussian distributions given features `Xnew` having the same scitype as `X` above.\n\nPredictions are probabilistic.\n\nSpecific metrics can also be predicted using:\n\n  * `predict_mean(mach, Xnew)`\n  * `predict_mode(mach, Xnew)`\n  * `predict_median(mach, Xnew)`\n\n## Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `:fitresult`: The `GBTree` object returned by EvoTrees.jl fitting algorithm.\n\n## Report\n\nThe fields of `report(mach)` are:\n\n  * `:features`: The names of the features encountered in training.\n\n# Examples\n\n```\n# Internal API\nusing EvoTrees\nparams = EvoTreeGaussian(max_depth=5, nbins=32, nrounds=100)\nnobs, nfeats = 1_000, 5\nx_train, y_train = randn(nobs, nfeats), rand(nobs)\nmodel = fit_evotree(params; x_train, y_train)\npreds = EvoTrees.predict(model, x_train)\n```\n\n```\n# MLJ Interface\nusing MLJ\nEvoTreeGaussian = @load EvoTreeGaussian pkg=EvoTrees\nmodel = EvoTreeGaussian(max_depth=5, nbins=32, nrounds=100)\nX, y = @load_boston\nmach = machine(model, X, y) |> fit!\npreds = predict(mach, X)\npreds = predict_mean(mach, X)\npreds = predict_mode(mach, X)\npreds = predict_median(mach, X)\n```\n"
":name" = "EvoTreeGaussian"
":human_name" = "evo tree gaussian"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":predict"]
":hyperparameters" = "`(:nrounds, :lambda, :gamma, :eta, :max_depth, :min_weight, :rowsample, :colsample, :nbins, :alpha, :monotone_constraints, :rng, :device)`"
":hyperparameter_types" = "`(\"Int64\", \"Any\", \"Any\", \"Any\", \"Int64\", \"Any\", \"Any\", \"Any\", \"Int64\", \"Any\", \"Any\", \"Any\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = ":nrounds"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`true`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[EvoTrees.EvoTreeMLE]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Continuous}}, Tuple{Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "EvoTrees"
":package_license" = "Apache"
":load_path" = "EvoTrees.EvoTreeMLE"
":package_uuid" = "f6006082-12f8-11e9-0c9c-0d5d367ab1e5"
":package_url" = "https://github.com/Evovest/EvoTrees.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "EvoTreeMLE(;kwargs...)\n\nA model type for constructing a EvoTreeMLE, based on [EvoTrees.jl](https://github.com/Evovest/EvoTrees.jl), and implementing both an internal API the MLJ model interface. EvoTreeMLE performs maximum likelihood estimation. Assumed distribution is specified through `loss` kwargs. Both Gaussian and Logistic distributions are supported.\n\n# Hyper-parameters\n\n`loss=:gaussian`:         Loss to be be minimized during training. One of:\n\n  * `:gaussian` / `:gaussian_mle`\n  * `:logistic` / `:logistic_mle`\n  * `nrounds=10`:                 Number of rounds. It corresponds to the number of trees that will be sequentially stacked.\n  * `lambda::T=0.0`:              L2 regularization term on weights. Must be >= 0. Higher lambda can result in a more robust model.\n  * `gamma::T=0.0`:               Minimum gain imprvement needed to perform a node split. Higher gamma can result in a more robust model.\n  * `max_depth=5`:                Maximum depth of a tree. Must be >= 1. A tree of depth 1 is made of a single prediction leaf.\n\nA complete tree of depth N contains `2^(N - 1)` terminal leaves and `2^(N - 1) - 1` split nodes.   Compute cost is proportional to 2^max_depth. Typical optimal values are in the 3 to 9 range.\n\n  * `min_weight=0.0`:             Minimum weight needed in a node to perform a split. Matches the number of observations by default or the sum of weights as provided by the `weights` vector.\n  * `rowsample=1.0`:              Proportion of rows that are sampled at each iteration to build the tree. Should be in `]0, 1]`.\n  * `colsample=1.0`:              Proportion of columns / features that are sampled at each iteration to build the tree. Should be in `]0, 1]`.\n  * `nbins=32`:                   Number of bins into which each feature is quantized. Buckets are defined based on quantiles, hence resulting in equal weight bins.\n  * `monotone_constraints=Dict{Int, Int}()`: Specify monotonic constraints using a dict where the key is the feature index and the value the applicable constraint (-1=decreasing, 0=none, 1=increasing).  !Experimental feature: note that for MLE regression, constraints may not be enforced systematically.\n  * `rng=123`:                    Either an integer used as a seed to the random number generator or an actual random number generator (`::Random.AbstractRNG`).\n  * `device=\"cpu\"`:               Hardware device to use for computations. Can be either `\"cpu\"` or `\"gpu\"`.\n\n# Internal API\n\nDo `config = EvoTreeMLE()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeMLE(max_depth=...).\n\n## Training model\n\nA model is built using [`fit_evotree`](@ref):\n\n```julia\nmodel = fit_evotree(config; x_train, y_train, kwargs...)\n```\n\n## Inference\n\nPredictions are obtained using [`predict`](@ref) which returns a `Matrix` of size `[nobs, nparams]` where the second dimensions refer to `μ` & `σ` for Normal/Gaussian and `μ` & `s` for Logistic.\n\n```julia\nEvoTrees.predict(model, X)\n```\n\nAlternatively, models act as a functor, returning predictions when called as a function with features as argument:\n\n```julia\nmodel(X)\n```\n\n# MLJ\n\nFrom MLJ, the type can be imported using:\n\n```julia\nEvoTreeMLE = @load EvoTreeMLE pkg=EvoTrees\n```\n\nDo `model = EvoTreeMLE()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `EvoTreeMLE(loss=...)`.\n\n## Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have one of the following element scitypes: `Continuous`, `Count`, or `<:OrderedFactor`; check column scitypes with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `<:Continuous`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n## Operations\n\n  * `predict(mach, Xnew)`: returns a vector of Gaussian or Logistic distributions (according to provided `loss`) given features `Xnew` having the same scitype as `X` above.\n\nPredictions are probabilistic.\n\nSpecific metrics can also be predicted using:\n\n  * `predict_mean(mach, Xnew)`\n  * `predict_mode(mach, Xnew)`\n  * `predict_median(mach, Xnew)`\n\n## Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `:fitresult`: The `GBTree` object returned by EvoTrees.jl fitting algorithm.\n\n## Report\n\nThe fields of `report(mach)` are:\n\n  * `:features`: The names of the features encountered in training.\n\n# Examples\n\n```\n# Internal API\nusing EvoTrees\nconfig = EvoTreeMLE(max_depth=5, nbins=32, nrounds=100)\nnobs, nfeats = 1_000, 5\nx_train, y_train = randn(nobs, nfeats), rand(nobs)\nmodel = fit_evotree(config; x_train, y_train)\npreds = EvoTrees.predict(model, x_train)\n```\n\n```\n# MLJ Interface\nusing MLJ\nEvoTreeMLE = @load EvoTreeMLE pkg=EvoTrees\nmodel = EvoTreeMLE(max_depth=5, nbins=32, nrounds=100)\nX, y = @load_boston\nmach = machine(model, X, y) |> fit!\npreds = predict(mach, X)\npreds = predict_mean(mach, X)\npreds = predict_mode(mach, X)\npreds = predict_median(mach, X)\n```\n"
":name" = "EvoTreeMLE"
":human_name" = "evo tree mle"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = []
":hyperparameters" = "`(:nrounds, :lambda, :gamma, :eta, :max_depth, :min_weight, :rowsample, :colsample, :nbins, :alpha, :monotone_constraints, :rng, :device)`"
":hyperparameter_types" = "`(\"Int64\", \"Any\", \"Any\", \"Any\", \"Int64\", \"Any\", \"Any\", \"Any\", \"Int64\", \"Any\", \"Any\", \"Any\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = ":nrounds"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`true`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[EvoTrees.EvoTreeRegressor]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Continuous}}, Tuple{Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "EvoTrees"
":package_license" = "Apache"
":load_path" = "EvoTrees.EvoTreeRegressor"
":package_uuid" = "f6006082-12f8-11e9-0c9c-0d5d367ab1e5"
":package_url" = "https://github.com/Evovest/EvoTrees.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "EvoTreeRegressor(;kwargs...)\n\nA model type for constructing a EvoTreeRegressor, based on [EvoTrees.jl](https://github.com/Evovest/EvoTrees.jl), and implementing both an internal API and the MLJ model interface.\n\n# Hyper-parameters\n\n  * `loss=:linear`:         Loss to be be minimized during training. One of:\n\n      * `:linear`\n      * `:logistic`\n      * `:gamma`\n      * `:tweedie`\n      * `:quantile`\n      * `:L1`\n  * `nrounds=10`:           Number of rounds. It corresponds to the number of trees that will be sequentially stacked.\n  * `lambda::T=0.0`:        L2 regularization term on weights. Must be >= 0. Higher lambda can result in a more robust model.\n  * `gamma::T=0.0`:         Minimum gain improvement needed to perform a node split. Higher gamma can result in a more robust model.\n  * `alpha::T=0.5`:         Loss specific parameter in the [0, 1] range:                           - `:quantile`: target quantile for the regression.                           - `:L1`: weighting parameters to positive vs negative residuals.                                 - Positive residual weights = `alpha`                                 - Negative residual weights = `(1 - alpha)`\n  * `max_depth=5`:          Maximum depth of a tree. Must be >= 1. A tree of depth 1 is made of a single prediction leaf. A complete tree of depth N contains `2^(N - 1)` terminal leaves and `2^(N - 1) - 1` split nodes. Compute cost is proportional to `2^max_depth`. Typical optimal values are in the 3 to 9 range.\n  * `min_weight=0.0`:       Minimum weight needed in a node to perform a split. Matches the number of observations by default or the sum of weights as provided by the `weights` vector.\n  * `rowsample=1.0`:        Proportion of rows that are sampled at each iteration to build the tree. Should be in `]0, 1]`.\n  * `colsample=1.0`:        Proportion of columns / features that are sampled at each iteration to build the tree. Should be in `]0, 1]`.\n  * `nbins=32`:             Number of bins into which each feature is quantized. Buckets are defined based on quantiles, hence resulting in equal weight bins.\n  * `monotone_constraints=Dict{Int, Int}()`: Specify monotonic constraints using a dict where the key is the feature index and the value the applicable constraint (-1=decreasing, 0=none, 1=increasing).  Only `:linear`, `:logistic`, `:gamma` and `tweedie` losses are supported at the moment.\n  * `rng=123`:              Either an integer used as a seed to the random number generator or an actual random number generator (`::Random.AbstractRNG`).\n  * `device=\"cpu\"`:         Hardware device to use for computations. Can be either `\"cpu\"` or `\"gpu\"`. Only `:linear`, `:logistic`, `:gamma` and `tweedie` losses are supported on GPU.\n\n# Internal API\n\nDo `config = EvoTreeRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeRegressor(loss=...).\n\n## Training model\n\nA model is built using [`fit_evotree`](@ref):\n\n```julia\nmodel = fit_evotree(config; x_train, y_train, kwargs...)\n```\n\n## Inference\n\nPredictions are obtained using [`predict`](@ref) which returns a `Matrix` of size `[nobs, 1]`:\n\n```julia\nEvoTrees.predict(model, X)\n```\n\nAlternatively, models act as a functor, returning predictions when called as a function with features as argument:\n\n```julia\nmodel(X)\n```\n\n# MLJ Interface\n\nFrom MLJ, the type can be imported using:\n\n```julia\nEvoTreeRegressor = @load EvoTreeRegressor pkg=EvoTrees\n```\n\nDo `model = EvoTreeRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `EvoTreeRegressor(loss=...)`.\n\n## Training model\n\nIn MLJ or MLJBase, bind an instance `model` to data with     `mach = machine(model, X, y)` where\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have one of the following element scitypes: `Continuous`, `Count`, or `<:OrderedFactor`; check column scitypes with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `<:Continuous`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n## Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given features `Xnew` having the same scitype as `X` above. Predictions are deterministic.\n\n## Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `:fitresult`: The `GBTree` object returned by EvoTrees.jl fitting algorithm.\n\n## Report\n\nThe fields of `report(mach)` are:\n\n  * `:features`: The names of the features encountered in training.\n\n# Examples\n\n```\n# Internal API\nusing EvoTrees\nconfig = EvoTreeRegressor(max_depth=5, nbins=32, nrounds=100)\nnobs, nfeats = 1_000, 5\nx_train, y_train = randn(nobs, nfeats), rand(nobs)\nmodel = fit_evotree(config; x_train, y_train)\npreds = EvoTrees.predict(model, x_train)\n```\n\n```\n# MLJ Interface\nusing MLJ\nEvoTreeRegressor = @load EvoTreeRegressor pkg=EvoTrees\nmodel = EvoTreeRegressor(max_depth=5, nbins=32, nrounds=100)\nX, y = @load_boston\nmach = machine(model, X, y) |> fit!\npreds = predict(mach, X)\n```\n"
":name" = "EvoTreeRegressor"
":human_name" = "evo tree regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":predict"]
":hyperparameters" = "`(:nrounds, :lambda, :gamma, :eta, :max_depth, :min_weight, :rowsample, :colsample, :nbins, :alpha, :monotone_constraints, :rng, :device)`"
":hyperparameter_types" = "`(\"Int64\", \"Any\", \"Any\", \"Any\", \"Int64\", \"Any\", \"Any\", \"Any\", \"Int64\", \"Any\", \"Any\", \"Any\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = ":nrounds"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`true`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[EvoTrees.EvoTreeCount]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Count}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Count}}, Tuple{Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}}}, AbstractMatrix{ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Count}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "EvoTrees"
":package_license" = "Apache"
":load_path" = "EvoTrees.EvoTreeCount"
":package_uuid" = "f6006082-12f8-11e9-0c9c-0d5d367ab1e5"
":package_url" = "https://github.com/Evovest/EvoTrees.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "EvoTreeCount(;kwargs...)\n\nA model type for constructing a EvoTreeCount, based on [EvoTrees.jl](https://github.com/Evovest/EvoTrees.jl), and implementing both an internal API the MLJ model interface. EvoTreeCount is used to perform Poisson probabilistic regression on count target.\n\n# Hyper-parameters\n\n  * `nrounds=10`:                 Number of rounds. It corresponds to the number of trees that will be sequentially stacked.\n  * `lambda::T=0.0`:              L2 regularization term on weights. Must be >= 0. Higher lambda can result in a more robust model.\n  * `gamma::T=0.0`:               Minimum gain imprvement needed to perform a node split. Higher gamma can result in a more robust model.\n  * `max_depth=5`:                Maximum depth of a tree. Must be >= 1. A tree of depth 1 is made of a single prediction leaf. A complete tree of depth N contains `2^(N - 1)` terminal leaves and `2^(N - 1) - 1` split nodes. Compute cost is proportional to 2^max_depth. Typical optimal values are in the 3 to 9 range.\n  * `min_weight=0.0`:             Minimum weight needed in a node to perform a split. Matches the number of observations by default or the sum of weights as provided by the `weights` vector.\n  * `rowsample=1.0`:              Proportion of rows that are sampled at each iteration to build the tree. Should be `]0, 1]`.\n  * `colsample=1.0`:              Proportion of columns / features that are sampled at each iteration to build the tree. Should be `]0, 1]`.\n  * `nbins=32`:                   Number of bins into which each feature is quantized. Buckets are defined based on quantiles, hence resulting in equal weight bins.\n  * `monotone_constraints=Dict{Int, Int}()`: Specify monotonic constraints using a dict where the key is the feature index and the value the applicable constraint (-1=decreasing, 0=none, 1=increasing).\n  * `rng=123`:                    Either an integer used as a seed to the random number generator or an actual random number generator (`::Random.AbstractRNG`).\n  * `device=\"cpu\"`:               Hardware device to use for computations. Can be either `\"cpu\"` or `\"gpu\"`.\n\n# Internal API\n\nDo `config = EvoTreeCount()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in EvoTreeCount(max_depth=...).\n\n## Training model\n\nA model is built using [`fit_evotree`](@ref):\n\n```julia\nmodel = fit_evotree(config; x_train, y_train, kwargs...)\n```\n\n## Inference\n\nPredictions are obtained using [`predict`](@ref) which returns a `Matrix` of size `[nobs, 1]`:\n\n```julia\nEvoTrees.predict(model, X)\n```\n\nAlternatively, models act as a functor, returning predictions when called as a function with features as argument:\n\n```julia\nmodel(X)\n```\n\n# MLJ\n\nFrom MLJ, the type can be imported using:\n\n```julia\nEvoTreeCount = @load EvoTreeCount pkg=EvoTrees\n```\n\nDo `model = EvoTreeCount()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `EvoTreeCount(loss=...)`.\n\n## Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with     mach = machine(model, X, y) where\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have one of the following element scitypes: `Continuous`, `Count`, or `<:OrderedFactor`; check column scitypes with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `<:Count`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Operations\n\n  * `predict(mach, Xnew)`: returns a vector of Poisson distributions given features `Xnew` having the same scitype as `X` above. Predictions are probabilistic.\n\nSpecific metrics can also be predicted using:\n\n  * `predict_mean(mach, Xnew)`\n  * `predict_mode(mach, Xnew)`\n  * `predict_median(mach, Xnew)`\n\n## Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `:fitresult`: The `GBTree` object returned by EvoTrees.jl fitting algorithm.\n\n## Report\n\nThe fields of `report(mach)` are:\n\n  * `:features`: The names of the features encountered in training.\n\n# Examples\n\n```\n# Internal API\nusing EvoTrees\nconfig = EvoTreeCount(max_depth=5, nbins=32, nrounds=100)\nnobs, nfeats = 1_000, 5\nx_train, y_train = randn(nobs, nfeats), rand(0:2, nobs)\nmodel = fit_evotree(config; x_train, y_train)\npreds = EvoTrees.predict(model, x_train)\n```\n\n```\nusing MLJ\nEvoTreeCount = @load EvoTreeCount pkg=EvoTrees\nmodel = EvoTreeCount(max_depth=5, nbins=32, nrounds=100)\nnobs, nfeats = 1_000, 5\nX, y = randn(nobs, nfeats), rand(0:2, nobs)\nmach = machine(model, X, y) |> fit!\npreds = predict(mach, X)\npreds = predict_mean(mach, X)\npreds = predict_mode(mach, X)\npreds = predict_median(mach, X)\n\n```\n\nSee also [EvoTrees.jl](https://github.com/Evovest/EvoTrees.jl).\n"
":name" = "EvoTreeCount"
":human_name" = "evo tree count"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":predict"]
":hyperparameters" = "`(:nrounds, :lambda, :gamma, :eta, :max_depth, :min_weight, :rowsample, :colsample, :nbins, :alpha, :monotone_constraints, :rng, :device)`"
":hyperparameter_types" = "`(\"Int64\", \"Any\", \"Any\", \"Any\", \"Int64\", \"Any\", \"Any\", \"Any\", \"Int64\", \"Any\", \"Any\", \"Any\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = ":nrounds"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`true`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.ConstantClassifier]
":input_scitype" = "`ScientificTypesBase.Table`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table, AbstractVector{<:ScientificTypesBase.Finite}}, Tuple{ScientificTypesBase.Table, AbstractVector{<:ScientificTypesBase.Finite}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.ConstantClassifier"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Constant classifier (Probabilistic)."
":name" = "ConstantClassifier"
":human_name" = "constant classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`()`"
":hyperparameter_types" = "`()`"
":hyperparameter_ranges" = "`()`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.Standardizer]
":input_scitype" = "`Union{ScientificTypesBase.Table, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`Union{ScientificTypesBase.Table, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table, AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`Union{ScientificTypesBase.Table, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`Union{ScientificTypesBase.Table, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.Standardizer"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nStandardizer\n```\n\nA model type for constructing a standardizer, based on [MLJModels.jl](https://github.com/alan-turing-institute/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nStandardizer = @load Standardizer pkg=MLJModels\n```\n\nDo `model = Standardizer()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `Standardizer(features=...)`.\n\nUse this model to standardize (whiten) a `Continuous` vector, or relevant columns of a table. The rescalings applied by this transformer to new data are always those learned during the training phase, which are generally different from what would actually standardize the new data.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nwhere\n\n  * `X`: any Tables.jl compatible table or any abstract vector with `Continuous` element scitype (any abstract float vector). Only features in a table with `Continuous` scitype can be standardized; check column scitypes with `schema(X)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `features`: one of the following, with the behavior indicated below:\n\n      * `[]` (empty, the default): standardize all features (columns) having `Continuous` element scitype\n      * non-empty vector of feature names (symbols): standardize only the `Continuous` features in the vector (if `ignore=false`) or `Continuous` features *not* named in the vector (`ignore=true`).\n      * function or other callable: standardize a feature if the callable returns `true` on its name. For example, `Standardizer(features = name -> name in [:x1, :x3], ignore = true, count=true)` has the same effect as `Standardizer(features = [:x1, :x3], ignore = true, count=true)`, namely to standardize all `Continuous` and `Count` features, with the exception of `:x1` and `:x3`.\n\n    Note this behavior is further modified if the `ordered_factor` or `count` flags are set to `true`; see below\n  * `ignore=false`: whether to ignore or standardize specified `features`, as explained above\n  * `ordered_factor=false`: if `true`, standardize any `OrderedFactor` feature wherever a `Continuous` feature would be standardized, as described above\n  * `count=false`: if `true`, standardize any `Count` feature wherever a `Continuous` feature would be standardized, as described above\n\n# Operations\n\n  * `transform(mach, Xnew)`: return `Xnew` with relevant features standardized according to the rescalings learned during fitting of `mach`.\n  * `inverse_transform(mach, Z)`: apply the inverse transformation to `Z`, so that `inverse_transform(mach, transform(mach, Xnew))` is approximately the same as `Xnew`; unavailable if `ordered_factor` or `count` flags were set to `true`.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `features_fit` - the names of features that will be standardized\n  * `means` - the corresponding untransformed mean values\n  * `stds` - the corresponding untransformed standard deviations\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `features_fit`: the names of features that will be standardized\n\n# Examples\n\n```\nusing MLJ\n\nX = (ordinal1 = [1, 2, 3],\n     ordinal2 = coerce([:x, :y, :x], OrderedFactor),\n     ordinal3 = [10.0, 20.0, 30.0],\n     ordinal4 = [-20.0, -30.0, -40.0],\n     nominal = coerce([\"Your father\", \"he\", \"is\"], Multiclass));\n\njulia> schema(X)\n┌──────────┬──────────────────┐\n│ names    │ scitypes         │\n├──────────┼──────────────────┤\n│ ordinal1 │ Count            │\n│ ordinal2 │ OrderedFactor{2} │\n│ ordinal3 │ Continuous       │\n│ ordinal4 │ Continuous       │\n│ nominal  │ Multiclass{3}    │\n└──────────┴──────────────────┘\n\nstand1 = Standardizer();\n\njulia> transform(fit!(machine(stand1, X)), X)\n(ordinal1 = [1, 2, 3],\n ordinal2 = CategoricalValue{Symbol,UInt32}[:x, :y, :x],\n ordinal3 = [-1.0, 0.0, 1.0],\n ordinal4 = [1.0, 0.0, -1.0],\n nominal = CategoricalValue{String,UInt32}[\"Your father\", \"he\", \"is\"],)\n\nstand2 = Standardizer(features=[:ordinal3, ], ignore=true, count=true);\n\njulia> transform(fit!(machine(stand2, X)), X)\n(ordinal1 = [-1.0, 0.0, 1.0],\n ordinal2 = CategoricalValue{Symbol,UInt32}[:x, :y, :x],\n ordinal3 = [10.0, 20.0, 30.0],\n ordinal4 = [1.0, 0.0, -1.0],\n nominal = CategoricalValue{String,UInt32}[\"Your father\", \"he\", \"is\"],)\n```\n\nSee also [`OneHotEncoder`](@ref), [`ContinuousEncoder`](@ref).\n"
":name" = "Standardizer"
":human_name" = "standardizer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":inverse_transform", ":transform"]
":hyperparameters" = "`(:features, :ignore, :ordered_factor, :count)`"
":hyperparameter_types" = "`(\"Union{Function, AbstractVector{Symbol}}\", \"Bool\", \"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.DeterministicConstantClassifier]
":input_scitype" = "`ScientificTypesBase.Table`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.DeterministicConstantClassifier"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Constant classifier (Deterministic)."
":name" = "DeterministicConstantClassifier"
":human_name" = "deterministic constant classifier"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":predict"]
":hyperparameters" = "`()`"
":hyperparameter_types" = "`()`"
":hyperparameter_ranges" = "`()`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.UnivariateTimeTypeToContinuous]
":input_scitype" = "`AbstractVector{<:ScientificTypesBase.ScientificTimeType}`"
":output_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{AbstractVector{<:ScientificTypesBase.ScientificTimeType}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`AbstractVector{<:ScientificTypesBase.ScientificTimeType}`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.UnivariateTimeTypeToContinuous"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nUnivariateTimeTypeToContinuous\n```\n\nA model type for constructing a single variable transformer that creates continuous representations of temporally typed data, based on [MLJModels.jl](https://github.com/alan-turing-institute/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nUnivariateTimeTypeToContinuous = @load UnivariateTimeTypeToContinuous pkg=MLJModels\n```\n\nDo `model = UnivariateTimeTypeToContinuous()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `UnivariateTimeTypeToContinuous(zero_time=...)`.\n\nUse this model to convert vectors with a `TimeType` element type to vectors of `Float64` type (`Continuous` element scitype).\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, x)\n```\n\nwhere\n\n  * `x`: any abstract vector whose element type is a subtype of `Dates.TimeType`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `zero_time`: the time that is to correspond to 0.0 under transformations, with the type coinciding with the training data element type. If unspecified, the earliest time encountered in training is used.\n  * `step::Period=Hour(24)`: time interval to correspond to one unit under transformation\n\n# Operations\n\n  * `transform(mach, xnew)`: apply the encoding inferred when `mach` was fit\n\n# Fitted parameters\n\n`fitted_params(mach).fitresult` is the tuple `(zero_time, step)` actually used in transformations, which may differ from the user-specified hyper-parameters.\n\n# Example\n\n```\nusing MLJ\nusing Dates\n\nx = [Date(2001, 1, 1) + Day(i) for i in 0:4]\n\nencoder = UnivariateTimeTypeToContinuous(zero_time=Date(2000, 1, 1),\n                                         step=Week(1))\n\nmach = machine(encoder, x)\nfit!(mach)\njulia> transform(mach, x)\n5-element Vector{Float64}:\n 52.285714285714285\n 52.42857142857143\n 52.57142857142857\n 52.714285714285715\n 52.857142\n```\n"
":name" = "UnivariateTimeTypeToContinuous"
":human_name" = "single variable transformer that creates continuous representations of temporally typed data"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":transform"]
":hyperparameters" = "`(:zero_time, :step)`"
":hyperparameter_types" = "`(\"Union{Nothing, Dates.TimeType}\", \"Dates.Period\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.OneHotEncoder]
":input_scitype" = "`ScientificTypesBase.Table`"
":output_scitype" = "`ScientificTypesBase.Table`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.OneHotEncoder"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nOneHotEncoder\n```\n\nA model type for constructing a one-hot encoder, based on [MLJModels.jl](https://github.com/alan-turing-institute/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nOneHotEncoder = @load OneHotEncoder pkg=MLJModels\n```\n\nDo `model = OneHotEncoder()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `OneHotEncoder(features=...)`.\n\nUse this model to one-hot encode the `Multiclass` and `OrderedFactor` features (columns) of some table, leaving other columns unchanged.\n\nNew data to be transformed may lack features present in the fit data, but no *new* features can be present.\n\n**Warning:** This transformer assumes that `levels(col)` for any `Multiclass` or `OrderedFactor` column, `col`, is the same for training data and new data to be transformed.\n\nTo ensure *all* features are transformed into `Continuous` features, or dropped, use [`ContinuousEncoder`](@ref) instead.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nwhere\n\n  * `X`: any Tables.jl compatible table. Columns can be of mixed type but only those with element scitype `Multiclass` or `OrderedFactor` can be encoded. Check column scitypes with `schema(X)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `features`: a vector of symbols (column names). If empty (default) then all `Multiclass` and `OrderedFactor` features are encoded. Otherwise, encoding is further restricted to the specified features (`ignore=false`) or the unspecified features (`ignore=true`). This default behavior can be modified by the `ordered_factor` flag.\n  * `ordered_factor=false`: when `true`, `OrderedFactor` features are universally excluded\n  * `drop_last=true`: whether to drop the column corresponding to the final class of encoded features. For example, a three-class feature is spawned into three new features if `drop_last=false`, but just two features otherwise.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `all_features`: names of all features encountered in training\n  * `fitted_levels_given_feature`: dictionary of the levels associated with each feature encoded, keyed on the feature name\n  * `ref_name_pairs_given_feature`: dictionary of pairs `r => ftr` (such as `0x00000001 => :grad__A`) where `r` is a CategoricalArrays.jl reference integer representing a level, and `ftr` the corresponding new feature name; the dictionary is keyed on the names of features that are encoded\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `features_to_be_encoded`: names of input features to be encoded\n  * `new_features`: names of all output features\n\n# Example\n\n```\nusing MLJ\n\nX = (name=categorical([\"Danesh\", \"Lee\", \"Mary\", \"John\"]),\n     grade=categorical([\"A\", \"B\", \"A\", \"C\"], ordered=true),\n     height=[1.85, 1.67, 1.5, 1.67],\n     n_devices=[3, 2, 4, 3])\n\njulia> schema(X)\n┌───────────┬──────────────────┐\n│ names     │ scitypes         │\n├───────────┼──────────────────┤\n│ name      │ Multiclass{4}    │\n│ grade     │ OrderedFactor{3} │\n│ height    │ Continuous       │\n│ n_devices │ Count            │\n└───────────┴──────────────────┘\n\nhot = OneHotEncoder(drop_last=true)\nmach = fit!(machine(hot, X))\nW = transform(mach, X)\n\njulia> schema(W)\n┌──────────────┬────────────┐\n│ names        │ scitypes   │\n├──────────────┼────────────┤\n│ name__Danesh │ Continuous │\n│ name__John   │ Continuous │\n│ name__Lee    │ Continuous │\n│ grade__A     │ Continuous │\n│ grade__B     │ Continuous │\n│ height       │ Continuous │\n│ n_devices    │ Count      │\n└──────────────┴────────────┘\n```\n\nSee also [`ContinuousEncoder`](@ref).\n"
":name" = "OneHotEncoder"
":human_name" = "one-hot encoder"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":fitted_params", ":transform", ":OneHotEncoder"]
":hyperparameters" = "`(:features, :drop_last, :ordered_factor, :ignore)`"
":hyperparameter_types" = "`(\"Vector{Symbol}\", \"Bool\", \"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.ContinuousEncoder]
":input_scitype" = "`ScientificTypesBase.Table`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.ContinuousEncoder"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nContinuousEncoder\n```\n\nA model type for constructing a continuous encoder, based on [MLJModels.jl](https://github.com/alan-turing-institute/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nContinuousEncoder = @load ContinuousEncoder pkg=MLJModels\n```\n\nDo `model = ContinuousEncoder()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `ContinuousEncoder(drop_last=...)`.\n\nUse this model to arrange all features (columns) of a table to have `Continuous` element scitype, by applying the following protocol to each feature `ftr`:\n\n  * If `ftr` is already `Continuous` retain it.\n  * If `ftr` is `Multiclass`, one-hot encode it.\n  * If `ftr` is `OrderedFactor`, replace it with `coerce(ftr, Continuous)` (vector of floating point integers), unless `ordered_factors=false` is specified, in which case one-hot encode it.\n  * If `ftr` is `Count`, replace it with `coerce(ftr, Continuous)`.\n  * If `ftr` has some other element scitype, or was not observed in fitting the encoder, drop it from the table.\n\n**Warning:** This transformer assumes that `levels(col)` for any `Multiclass` or `OrderedFactor` column, `col`, is the same for training data and new data to be transformed.\n\nTo selectively one-hot-encode categorical features (without dropping columns) use [`OneHotEncoder`](@ref) instead.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nwhere\n\n  * `X`: any Tables.jl compatible table. Columns can be of mixed type but only those with element scitype `Multiclass` or `OrderedFactor` can be encoded. Check column scitypes with `schema(X)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `drop_last=true`: whether to drop the column corresponding to the final class of one-hot encoded features. For example, a three-class feature is spawned into three new features if `drop_last=false`, but two just features otherwise.\n  * `one_hot_ordered_factors=false`: whether to one-hot any feature with `OrderedFactor` element scitype, or to instead coerce it directly to a (single) `Continuous` feature using the order\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `features_to_keep`: names of features that will not be dropped from the table\n  * `one_hot_encoder`: the `OneHotEncoder` model instance for handling the one-hot encoding\n  * `one_hot_encoder_fitresult`: the fitted parameters of the `OneHotEncoder` model\n\n# Report\n\n  * `features_to_keep`: names of input features that will not be dropped from the table\n  * `new_features`: names of all output features\n\n# Example\n\n```julia\nX = (name=categorical([\"Danesh\", \"Lee\", \"Mary\", \"John\"]),\n     grade=categorical([\"A\", \"B\", \"A\", \"C\"], ordered=true),\n     height=[1.85, 1.67, 1.5, 1.67],\n     n_devices=[3, 2, 4, 3],\n     comments=[\"the force\", \"be\", \"with you\", \"too\"])\n\njulia> schema(X)\n┌───────────┬──────────────────┐\n│ names     │ scitypes         │\n├───────────┼──────────────────┤\n│ name      │ Multiclass{4}    │\n│ grade     │ OrderedFactor{3} │\n│ height    │ Continuous       │\n│ n_devices │ Count            │\n│ comments  │ Textual          │\n└───────────┴──────────────────┘\n\nencoder = ContinuousEncoder(drop_last=true)\nmach = fit!(machine(encoder, X))\nW = transform(mach, X)\n\njulia> schema(W)\n┌──────────────┬────────────┐\n│ names        │ scitypes   │\n├──────────────┼────────────┤\n│ name__Danesh │ Continuous │\n│ name__John   │ Continuous │\n│ name__Lee    │ Continuous │\n│ grade        │ Continuous │\n│ height       │ Continuous │\n│ n_devices    │ Continuous │\n└──────────────┴────────────┘\n\njulia> setdiff(schema(X).names, report(mach).features_to_keep) # dropped features\n1-element Vector{Symbol}:\n :comments\n\n```\n\nSee also [`OneHotEncoder`](@ref)\n"
":name" = "ContinuousEncoder"
":human_name" = "continuous encoder"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":fitted_params", ":transform", ":ContinuousEncoder"]
":hyperparameters" = "`(:drop_last, :one_hot_ordered_factors)`"
":hyperparameter_types" = "`(\"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.UnivariateBoxCoxTransformer]
":input_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":output_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.UnivariateBoxCoxTransformer"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nUnivariateBoxCoxTransformer\n```\n\nA model type for constructing a single variable Box-Cox transformer, based on [MLJModels.jl](https://github.com/alan-turing-institute/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nUnivariateBoxCoxTransformer = @load UnivariateBoxCoxTransformer pkg=MLJModels\n```\n\nDo `model = UnivariateBoxCoxTransformer()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `UnivariateBoxCoxTransformer(n=...)`.\n\nBox-Cox transformations attempt to make data look more normally distributed. This can improve performance and assist in the interpretation of models which suppose that data is generated by a normal distribution.\n\nA Box-Cox transformation (with shift) is of the form\n\n```\nx -> ((x + c)^λ - 1)/λ\n```\n\nfor some constant `c` and real `λ`, unless `λ = 0`, in which case the above is replaced with\n\n```\nx -> log(x + c)\n```\n\nGiven user-specified hyper-parameters `n::Integer` and `shift::Bool`, the present implementation learns the parameters `c` and `λ` from the training data as follows: If `shift=true` and zeros are encountered in the data, then `c` is set to `0.2` times the data mean.  If there are no zeros, then no shift is applied. Finally, `n` different values of `λ` between `-0.4` and `3` are considered, with `λ` fixed to the value maximizing normality of the transformed data.\n\n*Reference:* [Wikipedia entry for power  transform](https://en.wikipedia.org/wiki/Power_transform).\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, x)\n```\n\nwhere\n\n  * `x`: any abstract vector with element scitype `Continuous`; check the scitype with `scitype(x)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `n=171`: number of values of the exponent `λ` to try\n  * `shift=false`: whether to include a preliminary constant translation in transformations, in the presence of zeros\n\n# Operations\n\n  * `transform(mach, xnew)`: apply the Box-Cox transformation learned when fitting `mach`\n  * `inverse_transform(mach, z)`: reconstruct the vector `z` whose transformation learned by `mach` is `z`\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `λ`: the learned Box-Cox exponent\n  * `c`: the learned shift\n\n# Examples\n\n```\nusing MLJ\nusing UnicodePlots\nusing Random\nRandom.seed!(123)\n\ntransf = UnivariateBoxCoxTransformer()\n\nx = randn(1000).^2\n\nmach = machine(transf, x)\nfit!(mach)\n\nz = transform(mach, x)\n\njulia> histogram(x)\n                ┌                                        ┐\n   [ 0.0,  2.0) ┤███████████████████████████████████  848\n   [ 2.0,  4.0) ┤████▌ 109\n   [ 4.0,  6.0) ┤█▍ 33\n   [ 6.0,  8.0) ┤▍ 7\n   [ 8.0, 10.0) ┤▏ 2\n   [10.0, 12.0) ┤  0\n   [12.0, 14.0) ┤▏ 1\n                └                                        ┘\n                                 Frequency\n\njulia> histogram(z)\n                ┌                                        ┐\n   [-5.0, -4.0) ┤█▎ 8\n   [-4.0, -3.0) ┤████████▊ 64\n   [-3.0, -2.0) ┤█████████████████████▊ 159\n   [-2.0, -1.0) ┤█████████████████████████████▊ 216\n   [-1.0,  0.0) ┤███████████████████████████████████  254\n   [ 0.0,  1.0) ┤█████████████████████████▊ 188\n   [ 1.0,  2.0) ┤████████████▍ 90\n   [ 2.0,  3.0) ┤██▊ 20\n   [ 3.0,  4.0) ┤▎ 1\n                └                                        ┘\n                                 Frequency\n\n```\n"
":name" = "UnivariateBoxCoxTransformer"
":human_name" = "single variable Box-Cox transformer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":fitted_params", ":inverse_transform", ":transform", ":UnivariateBoxCoxTransformer"]
":hyperparameters" = "`(:n, :shift)`"
":hyperparameter_types" = "`(\"Int64\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.InteractionTransformer]
":input_scitype" = "`ScientificTypesBase.Table`"
":output_scitype" = "`ScientificTypesBase.Table`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.InteractionTransformer"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nInteractionTransformer\n```\n\nA model type for constructing a interaction transformer, based on [MLJModels.jl](https://github.com/alan-turing-institute/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nInteractionTransformer = @load InteractionTransformer pkg=MLJModels\n```\n\nDo `model = InteractionTransformer()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `InteractionTransformer(order=...)`.\n\nGenerates all polynomial interaction terms up to the given order for the subset of chosen columns.  Any column that contains elements with scitype `<:Infinite` is a valid basis to generate interactions.  If `features` is not specified, all such columns with scitype `<:Infinite` in the table are used as a basis.\n\nIn MLJ or MLJBase, you can transform features `X` with the single call\n\n```\ntransform(machine(model), X)\n```\n\nSee also the example below.\n\n# Hyper-parameters\n\n  * `order`: Maximum order of interactions to be generated.\n  * `features`: Restricts interations generation to those columns\n\n# Operations\n\n  * `transform(machine(model), X)`: Generates polynomial interaction terms out of table `X` using the hyper-parameters specified in `model`.\n\n# Example\n\n```\nusing MLJ\n\nX = (\n    A = [1, 2, 3],\n    B = [4, 5, 6],\n    C = [7, 8, 9],\n    D = [\"x₁\", \"x₂\", \"x₃\"]\n)\nit = InteractionTransformer(order=3)\nmach = machine(it)\n\njulia> transform(mach, X)\n(A = [1, 2, 3],\n B = [4, 5, 6],\n C = [7, 8, 9],\n D = [\"x₁\", \"x₂\", \"x₃\"],\n A_B = [4, 10, 18],\n A_C = [7, 16, 27],\n B_C = [28, 40, 54],\n A_B_C = [28, 80, 162],)\n\nit = InteractionTransformer(order=2, features=[:A, :B])\nmach = machine(it)\n\njulia> transform(mach, X)\n(A = [1, 2, 3],\n B = [4, 5, 6],\n C = [7, 8, 9],\n D = [\"x₁\", \"x₂\", \"x₃\"],\n A_B = [4, 10, 18],)\n\n```\n"
":name" = "InteractionTransformer"
":human_name" = "interaction transformer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Static`"
":implemented_methods" = [":clean!", ":transform"]
":hyperparameters" = "`(:order, :features)`"
":hyperparameter_types" = "`(\"Int64\", \"Union{Nothing, Vector{Symbol}}\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.ConstantRegressor]
":input_scitype" = "`ScientificTypesBase.Table`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.ConstantRegressor"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Constant regressor (Probabilistic)."
":name" = "ConstantRegressor"
":human_name" = "constant regressor"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":fitted_params", ":predict"]
":hyperparameters" = "`(:distribution_type,)`"
":hyperparameter_types" = "`(\"Type{D} where D<:Distributions.Sampleable\",)`"
":hyperparameter_ranges" = "`(nothing,)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.FeatureSelector]
":input_scitype" = "`ScientificTypesBase.Table`"
":output_scitype" = "`ScientificTypesBase.Table`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.FeatureSelector"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nFeatureSelector\n```\n\nA model type for constructing a feature selector, based on [MLJModels.jl](https://github.com/alan-turing-institute/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nFeatureSelector = @load FeatureSelector pkg=MLJModels\n```\n\nDo `model = FeatureSelector()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `FeatureSelector(features=...)`.\n\nUse this model to select features (columns) of a table, usually as part of a model `Pipeline`.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nwhere\n\n  * `X`: any table of input features, where \"table\" is in the sense of Tables.jl\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `features`: one of the following, with the behavior indicated:\n\n      * `[]` (empty, the default): filter out all features (columns) which were not encountered in training\n      * non-empty vector of feature names (symbols): keep only the specified features (`ignore=false`) or keep only unspecified features (`ignore=true`)\n      * function or other callable: keep a feature if the callable returns `true` on its name. For example, specifying `FeatureSelector(features = name -> name in [:x1, :x3], ignore = true)` has the same effect as `FeatureSelector(features = [:x1, :x3], ignore = true)`, namely to select all features, with the exception of `:x1` and `:x3`.\n  * `ignore`: whether to ignore or keep specified `features`, as explained above\n\n# Operations\n\n  * `transform(mach, Xnew)`: select features from the table `Xnew` as specified by the model, taking features seen during training into account, if relevant\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `features_to_keep`: the features that will be selected\n\n# Example\n\n```\nusing MLJ\n\nX = (ordinal1 = [1, 2, 3],\n     ordinal2 = coerce([\"x\", \"y\", \"x\"], OrderedFactor),\n     ordinal3 = [10.0, 20.0, 30.0],\n     ordinal4 = [-20.0, -30.0, -40.0],\n     nominal = coerce([\"Your father\", \"he\", \"is\"], Multiclass));\n\nselector = FeatureSelector(features=[:ordinal3, ], ignore=true);\n\njulia> transform(fit!(machine(selector, X)), X)\n(ordinal1 = [1, 2, 3],\n ordinal2 = CategoricalValue{Symbol,UInt32}[\"x\", \"y\", \"x\"],\n ordinal4 = [-20.0, -30.0, -40.0],\n nominal = CategoricalValue{String,UInt32}[\"Your father\", \"he\", \"is\"],)\n\n```\n"
":name" = "FeatureSelector"
":human_name" = "feature selector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":transform"]
":hyperparameters" = "`(:features, :ignore)`"
":hyperparameter_types" = "`(\"Union{Function, Vector{Symbol}}\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.UnivariateDiscretizer]
":input_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{AbstractVector{<:ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor}`"
":inverse_transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.UnivariateDiscretizer"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nUnivariateDiscretizer\n```\n\nA model type for constructing a single variable discretizer, based on [MLJModels.jl](https://github.com/alan-turing-institute/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nUnivariateDiscretizer = @load UnivariateDiscretizer pkg=MLJModels\n```\n\nDo `model = UnivariateDiscretizer()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `UnivariateDiscretizer(n_classes=...)`.\n\nDiscretization converts a `Continuous` vector into an `OrderedFactor` vector. In particular, the output is a `CategoricalVector` (whose reference type is optimized).\n\nThe transformation is chosen so that the vector on which the transformer is fit has, in transformed form, an approximately uniform distribution of values. Specifically, if `n_classes` is the level of discretization, then `2*n_classes - 1` ordered quantiles are computed, the odd quantiles being used for transforming (discretization) and the even quantiles for inverse transforming.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, x)\n```\n\nwhere\n\n  * `x`: any abstract vector with `Continuous` element scitype; check scitype with `scitype(x)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `n_classes`: number of discrete classes in the output\n\n# Operations\n\n  * `transform(mach, xnew)`: discretize `xnew` according to the discretization learned when fitting `mach`\n  * `inverse_transform(mach, z)`: attempt to reconstruct from `z` a vector that transforms to give `z`\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach).fitesult` include:\n\n  * `odd_quantiles`: quantiles used for transforming (length is `n_classes - 1`)\n  * `even_quantiles`: quantiles used for inverse transforming (length is `n_classes`)\n\n# Example\n\n```\nusing MLJ\nusing Random\nRandom.seed!(123)\n\ndiscretizer = UnivariateDiscretizer(n_classes=100)\nmach = machine(discretizer, randn(1000))\nfit!(mach)\n\njulia> x = rand(5)\n5-element Vector{Float64}:\n 0.8585244609846809\n 0.37541692370451396\n 0.6767070590395461\n 0.9208844241267105\n 0.7064611415680901\n\njulia> z = transform(mach, x)\n5-element CategoricalArrays.CategoricalArray{UInt8,1,UInt8}:\n 0x52\n 0x42\n 0x4d\n 0x54\n 0x4e\n\nx_approx = inverse_transform(mach, z)\njulia> x - x_approx\n5-element Vector{Float64}:\n 0.008224506144777322\n 0.012731354778359405\n 0.0056265330571125816\n 0.005738175684445124\n 0.006835652575801987\n```\n"
":name" = "UnivariateDiscretizer"
":human_name" = "single variable discretizer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":fitted_params", ":inverse_transform", ":transform", ":UnivariateDiscretizer"]
":hyperparameters" = "`(:n_classes,)`"
":hyperparameter_types" = "`(\"Int64\",)`"
":hyperparameter_ranges" = "`(nothing,)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.FillImputer]
":input_scitype" = "`ScientificTypesBase.Table`"
":output_scitype" = "`ScientificTypesBase.Table`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.FillImputer"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nFillImputer\n```\n\nA model type for constructing a fill imputer, based on [MLJModels.jl](https://github.com/alan-turing-institute/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nFillImputer = @load FillImputer pkg=MLJModels\n```\n\nDo `model = FillImputer()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `FillImputer(features=...)`.\n\nUse this model to impute `missing` values in tabular data. A fixed \"filler\" value is learned from the training data, one for each column of the table.\n\nFor imputing missing values in a vector, use [`UnivariateFillImputer`](@ref) instead.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have element scitypes `Union{Missing, T}`, where `T` is a subtype of `Continuous`, `Multiclass`, `OrderedFactor` or `Count`. Check scitypes with `schema(X)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `features`: a vector of names of features (symbols) for which imputation is to be attempted; default is empty, which is interpreted as \"impute all\".\n  * `continuous_fill`: function or other callable to determine value to be imputed in the case of `Continuous` (abstract float) data; default is to apply `median` after skipping `missing` values\n  * `count_fill`: function or other callable to determine value to be imputed in the case of `Count` (integer) data; default is to apply rounded `median` after skipping `missing` values\n  * `finite_fill`: function or other callable to determine value to be imputed in the case of `Multiclass` or `OrderedFactor` data (categorical vectors); default is to apply `mode` after skipping `missing` values\n\n# Operations\n\n  * `transform(mach, Xnew)`: return `Xnew` with missing values imputed with the fill values learned when fitting `mach`\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `features_seen_in_fit`: the names of features (columns) encountered during training\n  * `univariate_transformer`: the univariate model applied to determine   the fillers (it's fields contain the functions defining the filler computations)\n  * `filler_given_feature`: dictionary of filler values, keyed on feature (column) names\n\n# Examples\n\n```\nusing MLJ\nimputer = FillImputer()\n\nX = (a = [1.0, 2.0, missing, 3.0, missing],\n     b = coerce([\"y\", \"n\", \"y\", missing, \"y\"], Multiclass),\n     c = [1, 1, 2, missing, 3])\n\nschema(X)\njulia> schema(X)\n┌───────┬───────────────────────────────┐\n│ names │ scitypes                      │\n├───────┼───────────────────────────────┤\n│ a     │ Union{Missing, Continuous}    │\n│ b     │ Union{Missing, Multiclass{2}} │\n│ c     │ Union{Missing, Count}         │\n└───────┴───────────────────────────────┘\n\nmach = machine(imputer, X)\nfit!(mach)\n\njulia> fitted_params(mach).filler_given_feature\n(filler = 2.0,)\n\njulia> fitted_params(mach).filler_given_feature\nDict{Symbol, Any} with 3 entries:\n  :a => 2.0\n  :b => \"y\"\n  :c => 2\n\njulia> transform(mach, X)\n(a = [1.0, 2.0, 2.0, 3.0, 2.0],\n b = CategoricalValue{String, UInt32}[\"y\", \"n\", \"y\", \"y\", \"y\"],\n c = [1, 1, 2, 2, 3],)\n```\n\nSee also [`UnivariateFillImputer`](@ref).\n"
":name" = "FillImputer"
":human_name" = "fill imputer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":fitted_params", ":transform", ":FillImputer"]
":hyperparameters" = "`(:features, :continuous_fill, :count_fill, :finite_fill)`"
":hyperparameter_types" = "`(\"Vector{Symbol}\", \"Function\", \"Function\", \"Function\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.DeterministicConstantRegressor]
":input_scitype" = "`ScientificTypesBase.Table`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.DeterministicConstantRegressor"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Constant regressor (Deterministic)."
":name" = "DeterministicConstantRegressor"
":human_name" = "deterministic constant regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":predict"]
":hyperparameters" = "`()`"
":hyperparameter_types" = "`()`"
":hyperparameter_ranges" = "`()`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.UnivariateStandardizer]
":input_scitype" = "`AbstractVector{<:ScientificTypesBase.Infinite}`"
":output_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{AbstractVector{<:ScientificTypesBase.Infinite}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Infinite}`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.UnivariateStandardizer"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nUnivariateStandardizer()\n```\n\nTransformer type for standardizing (whitening) single variable data.\n\nThis model may be deprecated in the future. Consider using [`Standardizer`](@ref), which handles both tabular *and* univariate data.\n"
":name" = "UnivariateStandardizer"
":human_name" = "single variable discretizer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":fitted_params", ":inverse_transform", ":transform"]
":hyperparameters" = "`()`"
":hyperparameter_types" = "`()`"
":hyperparameter_ranges" = "`()`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJModels.UnivariateFillImputer]
":input_scitype" = "`Union{AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.Count}}, AbstractVector{<:Union{Missing, ScientificTypesBase.Finite}}}`"
":output_scitype" = "`Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{Union{AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.Count}}, AbstractVector{<:Union{Missing, ScientificTypesBase.Finite}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":inverse_transform_scitype" = "`Union{AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.Count}}, AbstractVector{<:Union{Missing, ScientificTypesBase.Finite}}}`"
":is_pure_julia" = "`true`"
":package_name" = "MLJModels"
":package_license" = "MIT"
":load_path" = "MLJModels.UnivariateFillImputer"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":package_url" = "https://github.com/alan-turing-institute/MLJModels.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nUnivariateFillImputer\n```\n\nA model type for constructing a single variable fill imputer, based on [MLJModels.jl](https://github.com/alan-turing-institute/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nUnivariateFillImputer = @load UnivariateFillImputer pkg=MLJModels\n```\n\nDo `model = UnivariateFillImputer()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `UnivariateFillImputer(continuous_fill=...)`.\n\nUse this model to imputing `missing` values in a vector with a fixed value learned from the non-missing values of training vector.\n\nFor imputing missing values in tabular data, use [`FillImputer`](@ref) instead.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, x)\n```\n\nwhere\n\n  * `x`: any abstract vector with element scitype `Union{Missing, T}` where `T` is a subtype of `Continuous`, `Multiclass`, `OrderedFactor` or `Count`; check scitype using `scitype(x)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `continuous_fill`: function or other callable to determine value to be imputed in the case of `Continuous` (abstract float) data; default is to apply `median` after skipping `missing` values\n  * `count_fill`: function or other callable to determine value to be imputed in the case of `Count` (integer) data; default is to apply rounded `median` after skipping `missing` values\n  * `finite_fill`: function or other callable to determine value to be imputed in the case of `Multiclass` or `OrderedFactor` data (categorical vectors); default is to apply `mode` after skipping `missing` values\n\n# Operations\n\n  * `transform(mach, xnew)`: return `xnew` with missing values imputed with the fill values learned when fitting `mach`\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `filler`: the fill value to be imputed in all new data\n\n# Examples\n\n```\nusing MLJ\nimputer = UnivariateFillImputer()\n\nx_continuous = [1.0, 2.0, missing, 3.0]\nx_multiclass = coerce([\"y\", \"n\", \"y\", missing, \"y\"], Multiclass)\nx_count = [1, 1, 1, 2, missing, 3, 3]\n\nmach = machine(imputer, x_continuous)\nfit!(mach)\n\njulia> fitted_params(mach)\n(filler = 2.0,)\n\njulia> transform(mach, [missing, missing, 101.0])\n3-element Vector{Float64}:\n 2.0\n 2.0\n 101.0\n\nmach2 = machine(imputer, x_multiclass) |> fit!\n\njulia> transform(mach2, x_multiclass)\n5-element CategoricalArray{String,1,UInt32}:\n \"y\"\n \"n\"\n \"y\"\n \"y\"\n \"y\"\n\nmach3 = machine(imputer, x_count) |> fit!\n\njulia> transform(mach3, [missing, missing, 5])\n3-element Vector{Int64}:\n 2\n 2\n 5\n```\n\nFor imputing tabular data, use [`FillImputer`](@ref).\n"
":name" = "UnivariateFillImputer"
":human_name" = "single variable fill imputer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":fitted_params", ":transform", ":UnivariateFillImputer"]
":hyperparameters" = "`(:continuous_fill, :count_fill, :finite_fill)`"
":hyperparameter_types" = "`(\"Function\", \"Function\", \"Function\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.MCDDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.MCDDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nMCDDetector(store_precision = true,\n               assume_centered = false,\n               support_fraction = nothing,\n               random_state = nothing)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.mcd](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.mcd)\n"
":name" = "MCDDetector"
":human_name" = "mcd detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:store_precision, :assume_centered, :support_fraction, :random_state)`"
":hyperparameter_types" = "`(\"Bool\", \"Bool\", \"Union{Nothing, Real}\", \"Union{Nothing, Integer}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.COPODDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.COPODDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nCOPODDetector(n_jobs = 1)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.copod](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.copod)\n"
":name" = "COPODDetector"
":human_name" = "copod detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:n_jobs,)`"
":hyperparameter_types" = "`(\"Integer\",)`"
":hyperparameter_ranges" = "`(nothing,)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.HBOSDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.HBOSDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nHBOSDetector(n_bins = 10,\n                alpha = 0.1,\n                tol = 0.5)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.hbos](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.hbos)\n"
":name" = "HBOSDetector"
":human_name" = "hbos detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:n_bins, :alpha, :tol)`"
":hyperparameter_types" = "`(\"Integer\", \"Real\", \"Real\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.IForestDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.IForestDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nIForestDetector(n_estimators = 100,\n                   max_samples = \"auto\",\n                   max_features = 1.0\n                   bootstrap = false,\n                   random_state = nothing,\n                   verbose = 0,\n                   n_jobs = 1)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.iforest](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.iforest)\n"
":name" = "IForestDetector"
":human_name" = "i forest detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:n_estimators, :max_samples, :max_features, :bootstrap, :random_state, :verbose, :n_jobs)`"
":hyperparameter_types" = "`(\"Integer\", \"Union{Real, String}\", \"Real\", \"Bool\", \"Union{Nothing, Integer}\", \"Integer\", \"Integer\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.SOSDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.SOSDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nSOSDetector(perplexity = 4.5,\n               metric = \"minkowski\",\n               eps = 1e-5)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.sos](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.sos)\n"
":name" = "SOSDetector"
":human_name" = "sos detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:perplexity, :metric, :eps)`"
":hyperparameter_types" = "`(\"Real\", \"String\", \"Real\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.ABODDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.ABODDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nABODDetector(n_neighbors = 5,\n                method = \"fast\")\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.abod](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.abod)\n"
":name" = "ABODDetector"
":human_name" = "abod detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:n_neighbors, :method)`"
":hyperparameter_types" = "`(\"Integer\", \"String\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.LOFDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.LOFDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLOFDetector(n_neighbors = 5,\n               algorithm = \"auto\",\n               leaf_size = 30,\n               metric = \"minkowski\",\n               p = 2,\n               metric_params = nothing,\n               n_jobs = 1,\n               novelty = true)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lof](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lof)\n"
":name" = "LOFDetector"
":human_name" = "lof detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:n_neighbors, :algorithm, :leaf_size, :metric, :p, :metric_params, :n_jobs, :novelty)`"
":hyperparameter_types" = "`(\"Integer\", \"String\", \"Integer\", \"String\", \"Union{Nothing, Integer}\", \"Any\", \"Integer\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.PCADetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.PCADetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nPCADetector(n_components = nothing,\n               n_selected_components = nothing,\n               copy = true,\n               whiten = false,\n               svd_solver = \"auto\",\n               tol = 0.0\n               iterated_power = \"auto\",\n               standardization = true,\n               weighted = true,\n               random_state = nothing)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.pca](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.pca)\n"
":name" = "PCADetector"
":human_name" = "pca detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:n_components, :n_selected_components, :copy, :whiten, :svd_solver, :tol, :iterated_power, :standardization, :weighted, :random_state)`"
":hyperparameter_types" = "`(\"Union{Nothing, Real}\", \"Union{Nothing, Integer}\", \"Bool\", \"Bool\", \"String\", \"Real\", \"Union{Integer, String}\", \"Bool\", \"Bool\", \"Union{Nothing, Integer}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.INNEDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.INNEDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nINNEDetector(n_estimators=200,\n                max_samples=\"auto\",\n                random_state=None)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.inne](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.inne)\n"
":name" = "INNEDetector"
":human_name" = "inne detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:n_estimators, :max_samples, :random_state)`"
":hyperparameter_types" = "`(\"Integer\", \"Union{Real, String}\", \"Union{Nothing, Integer}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.OCSVMDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.OCSVMDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nOCSVMDetector(kernel = \"rbf\",\n                 degree = 3,\n                 gamma = \"auto\",\n                 coef0 = 0.0,\n                 tol = 0.001,\n                 nu = 0.5,\n                 shrinking = true,\n                 cache_size = 200,\n                 verbose = false,\n                 max_iter = -1)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.ocsvm](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.ocsvm)\n"
":name" = "OCSVMDetector"
":human_name" = "ocsvm detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:kernel, :degree, :gamma, :coef0, :tol, :nu, :shrinking, :cache_size, :verbose, :max_iter)`"
":hyperparameter_types" = "`(\"String\", \"Integer\", \"Union{Real, String}\", \"Real\", \"Real\", \"Real\", \"Bool\", \"Integer\", \"Bool\", \"Integer\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.ECODDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.ECODDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nECODDetector(n_jobs = 1)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.ecod](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.ecod)\n"
":name" = "ECODDetector"
":human_name" = "ecod detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:n_jobs,)`"
":hyperparameter_types" = "`(\"Any\",)`"
":hyperparameter_ranges" = "`(nothing,)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.SODDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.SODDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nSODDetector(n_neighbors = 5,\n               ref_set = 10,\n               alpha = 0.8)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.sod](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.sod)\n"
":name" = "SODDetector"
":human_name" = "sod detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:n_neighbors, :ref_set, :alpha)`"
":hyperparameter_types" = "`(\"Integer\", \"Integer\", \"Real\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.LODADetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.LODADetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLODADetector(n_bins = 10,\n                n_random_cuts = 100)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.loda](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.loda)\n"
":name" = "LODADetector"
":human_name" = "loda detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:n_bins, :n_random_cuts)`"
":hyperparameter_types" = "`(\"Integer\", \"Integer\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.KDEDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.KDEDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nKDEDetector(bandwidth=1.0,\n               algorithm=\"auto\",\n               leaf_size=30,\n               metric=\"minkowski\",\n               metric_params=None)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.kde](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.kde)\n"
":name" = "KDEDetector"
":human_name" = "kde detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:bandwidth, :algorithm, :leaf_size, :metric, :metric_params)`"
":hyperparameter_types" = "`(\"Real\", \"String\", \"Integer\", \"String\", \"Any\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.CDDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.CDDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nCDDetector(whitening = true,\n              rule_of_thumb = false)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cd](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cd)\n"
":name" = "CDDetector"
":human_name" = "cd detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:whitening, :rule_of_thumb)`"
":hyperparameter_types" = "`(\"Bool\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.KNNDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.KNNDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nKNNDetector(n_neighbors = 5,\n               method = \"largest\",\n               radius = 1.0,\n               algorithm = \"auto\",\n               leaf_size = 30,\n               metric = \"minkowski\",\n               p = 2,\n               metric_params = nothing,\n               n_jobs = 1)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.knn](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.knn)\n"
":name" = "KNNDetector"
":human_name" = "knn detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:n_neighbors, :method, :radius, :algorithm, :leaf_size, :metric, :p, :metric_params, :n_jobs)`"
":hyperparameter_types" = "`(\"Integer\", \"String\", \"Real\", \"String\", \"Integer\", \"String\", \"Union{Nothing, Integer}\", \"Any\", \"Integer\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.GMMDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.GMMDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nGMMDetector(n_components=1,\n               covariance_type=\"full\",\n               tol=0.001,\n               reg_covar=1e-06,\n               max_iter=100,\n               n_init=1,\n               init_params=\"kmeans\",\n               weights_init=None,\n               means_init=None,\n               precisions_init=None,\n               random_state=None,\n               warm_start=False)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.gmm](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.gmm)\n"
":name" = "GMMDetector"
":human_name" = "gmm detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:n_components, :covariance_type, :tol, :reg_covar, :max_iter, :n_init, :init_params, :random_state, :warm_start)`"
":hyperparameter_types" = "`(\"Integer\", \"String\", \"Real\", \"Real\", \"Integer\", \"Integer\", \"String\", \"Union{Nothing, Integer}\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.COFDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.COFDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nCOFDetector(n_neighbors = 5,\n               method=\"fast\")\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cof](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cof)\n"
":name" = "COFDetector"
":human_name" = "cof detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:n_neighbors, :method)`"
":hyperparameter_types" = "`(\"Integer\", \"String\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.CBLOFDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.CBLOFDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nCBLOFDetector(n_clusters = 8,\n                 alpha = 0.9,\n                 beta = 5,\n                 use_weights = false,\n                 random_state = nothing,\n                 n_jobs = 1)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cblof](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cblof)\n"
":name" = "CBLOFDetector"
":human_name" = "cblof detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:n_clusters, :alpha, :beta, :use_weights, :random_state, :n_jobs)`"
":hyperparameter_types" = "`(\"Integer\", \"Real\", \"Real\", \"Bool\", \"Union{Nothing, Integer}\", \"Integer\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.LOCIDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.LOCIDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLOCIDetector(alpha = 0.5,\n                k = 3)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.loci](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.loci)\n"
":name" = "LOCIDetector"
":human_name" = "loci detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:alpha, :k)`"
":hyperparameter_types" = "`(\"Real\", \"Real\")`"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.LMDDDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.LMDDDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLMDDDetector(n_iter = 50,\n                dis_measure = \"aad\",\n                random_state = nothing)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lmdd](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lmdd)\n"
":name" = "LMDDDetector"
":human_name" = "lmdd detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:n_iter, :dis_measure, :random_state)`"
":hyperparameter_types" = "`(\"Integer\", \"String\", \"Union{Nothing, Integer}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OutlierDetectionPython.RODDetector]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "OutlierDetectionPython"
":package_license" = "MIT"
":load_path" = "OutlierDetectionPython.RODDetector"
":package_uuid" = "2449c660-d36c-460e-a68b-92ab3c865b3e"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionPython.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nRODDetector(parallel_execution = false)\n```\n\n[https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.rod](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.rod)\n"
":name" = "RODDetector"
":human_name" = "rod detector"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":hyperparameters" = "`(:parallel_execution,)`"
":hyperparameter_types" = "`(\"Bool\",)`"
":hyperparameter_ranges" = "`(nothing,)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[OneRule.OneRuleClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Finite}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "OneRule"
":package_license" = "MIT"
":load_path" = "OneRule.OneRuleClassifier"
":package_uuid" = "90484964-6d6a-4979-af09-8657dbed84ff"
":package_url" = "https://github.com/roland-KA/OneRule.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nOneRuleClassifier\n```\n\nA model type for constructing a one rule classifier, based on [OneRule.jl](https://github.com/roland-KA/OneRule.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nOneRuleClassifier = @load OneRuleClassifier pkg=OneRule\n```\n\nDo `model = OneRuleClassifier()` to construct an instance with default hyper-parameters. \n\n`OneRuleClassifier` implements the OneRule method for classification by Robert Holte      (\"Very simple classification rules perform well on most commonly used datasets\"      in: Machine Learning 11.1 (1993), pp. 63-90). \n\n```\nFor more information see:\n\n- Witten, Ian H., Eibe Frank, and Mark A. Hall. \n  Data Mining Practical Machine Learning Tools and Techniques Third Edition. \n  Morgan Kaufmann, 2017, pp. 93-96.\n- [Machine Learning - (One|Simple) Rule](https://datacadamia.com/data_mining/one_rule)\n- [OneRClassifier - One Rule for Classification](http://rasbt.github.io/mlxtend/user_guide/classifier/OneRClassifier/)\n```\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with     mach = machine(model, X, y) where\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have one of the following element scitypes: `Multiclass`, `OrderedFactor`, or `<:Finite`; check column scitypes with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `OrderedFactor` or `Multiclass`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\nThis classifier has no hyper-parameters.\n\n# Operations\n\n  * `predict(mach, Xnew)`: return (deterministic) predictions of the target given features `Xnew` having the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `tree`: the tree (a `OneTree`) returned by the core OneTree.jl algorithm\n  * `all_classes`: all classes (i.e. levels) of the target (used also internally to transfer `levels`-information to `predict`)\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `tree`: The `OneTree` created based on the training data\n  * `nrules`: The number of rules `tree` contains\n  * `error_rate`: fraction of wrongly classified instances\n  * `error_count`: number of wrongly classified instances\n  * `classes_seen`: list of target classes actually observed in training\n  * `features`: the names of the features encountered in training\n\n# Examples\n\n```\nusing MLJ\n\nORClassifier = @load OneRuleClassifier pkg=OneRule\n\norc = ORClassifier()\n\noutlook = [\"sunny\", \"sunny\", \"overcast\", \"rainy\", \"rainy\", \"rainy\", \"overcast\", \"sunny\", \"sunny\", \"rainy\",  \"sunny\", \"overcast\", \"overcast\", \"rainy\"]\ntemperature = [\"hot\", \"hot\", \"hot\", \"mild\", \"cool\", \"cool\", \"cool\", \"mild\", \"cool\", \"mild\", \"mild\", \"mild\", \"hot\", \"mild\"]\nhumidity = [\"high\", \"high\", \"high\", \"high\", \"normal\", \"normal\", \"normal\", \"high\", \"normal\", \"normal\", \"normal\", \"high\", \"normal\", \"high\"]\nwindy = [\"false\", \"true\", \"false\", \"false\", \"false\", \"true\", \"true\", \"false\", \"false\", \"false\", \"true\", \"true\", \"false\", \"true\"]\n\nweather_data = (outlook = outlook, temperature = temperature, humidity = humidity, windy = windy)\nplay_data = [\"no\", \"no\", \"yes\", \"yes\", \"yes\", \"no\", \"yes\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\"]\n\nweather = coerce(weather_data, Textual => Multiclass)\nplay = coerce(play, Multiclass)\n\nmach = machine(orc, weather, play)\nfit!(mach)\n\nyhat = MLJ.predict(mach, weather)       # in a real context 'new' `weather` data would be used\none_tree = fitted_params(mach).tree\nreport(mach).error_rate\n```\n\nSee also [OneRule.jl](https://github.com/roland-KA/OneRule.jl).\n"
":name" = "OneRuleClassifier"
":human_name" = "one rule classifier"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`()`"
":hyperparameter_types" = "`()`"
":hyperparameter_ranges" = "`()`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[SelfOrganizingMaps.SelfOrganizingMap]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractMatrix{ScientificTypesBase.Continuous}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractMatrix{ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "SelfOrganizingMaps"
":package_license" = "MIT"
":load_path" = "SelfOrganizingMaps.SelfOrganizingMap"
":package_uuid" = "ba4b7379-301a-4be0-bee6-171e4e152787"
":package_url" = "https://github.com/john-waczak/SelfOrganizingMaps.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nSelfOrganizingMap\n```\n\nA model type for constructing a self organizing map, based on [SelfOrganizingMaps.jl](https://github.com/john-waczak/SelfOrganizingMaps.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nSelfOrganizingMap = @load SelfOrganizingMap pkg=SelfOrganizingMaps\n```\n\nDo `model = SelfOrganizingMap()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `SelfOrganizingMap(k=...)`.\n\nSelfOrganizingMaps implements [Kohonen's Self Organizing Map](https://ieeexplore.ieee.org/abstract/document/58325?casa_token=pGue0TD38nAAAAAA:kWFkvMJQKgYOTJjJx-_bRx8n_tnWEpau2QeoJ1gJt0IsywAuvkXYc0o5ezdc2mXfCzoEZUQXSQ), Proceedings of the IEEE; Kohonen, T.; (1990):\"The self-organizing map\"\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with     mach = machine(model, X) where\n\n  * `X`: an `AbstractMatrix` or `Table` of input features whose columns are of scitype `Continuous.`\n\nTrain the machine with `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `k=10`: Number of nodes along once side of SOM grid. There are `k²` total nodes.\n  * `η=0.5`: Learning rate. Scales adjust made to winning node and its neighbors during each round of training.\n  * `σ²=0.05`: The (squared) neighbor radius. Used to determine scale for neighbor node adjustments.\n  * `grid_type=:rectangular`  Node grid geometry. One of `(:rectangular, :hexagonal, :spherical)`.\n  * `η_decay=:exponential` Learning rate schedule function. One of `(:exponential, :asymptotic)`\n  * `σ_decay=:exponential` Neighbor radius schedule function. One of `(:exponential, :asymptotic, :none)`\n  * `neighbor_function=:gaussian` Kernel function used to make adjustment to neighbor weights. Scale is set by `σ²`. One of `(:gaussian, :mexican_hat)`.\n  * `matching_distance=euclidean` Distance function from `Distances.jl` used to determine winning node.\n  * `Nepochs=1` Number of times to repeat training on the shuffled dataset.\n\n# Operations\n\n  * `transform(mach, Xnew)`: returns the coordinates of the winning SOM node for each instance of `Xnew`. For SOM of grid*type `:rectangular` and `:hexagonal`, these are cartesian coordinates. For grid*type `:spherical`, these are the latitude and longitude in radians.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `coords`: The coordinates of each of the SOM nodes (points in the domain of the map) with shape (k², 2)\n  * `weights`: Array of weight vectors for the SOM nodes (corresponding points in the map's range) of shape (k², input dimension)\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `classes`: the index of the winning node for each instance of the training data X interpreted as a class label\n\n# Examples\n\n```\nusing MLJ\nsom = @load SelfOrganizingMap pkg=SelfOrganizingMaps\nmodel = som()\nX, y = make_regression(50, 3) # synthetic data\nmach = machine(model, X) |> fit!\nX̃ = transform(mach, X)\n\nrpt = report(mach)\nclasses = rpt.classes\n```\n"
":name" = "SelfOrganizingMap"
":human_name" = "self organizing map"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":transform"]
":hyperparameters" = "`(:k, :η, :σ², :grid_type, :η_decay, :σ_decay, :neighbor_function, :matching_distance, :Nepochs)`"
":hyperparameter_types" = "`(\"Int64\", \"Float64\", \"Float64\", \"Symbol\", \"Symbol\", \"Symbol\", \"Symbol\", \"Distances.PreMetric\", \"Int64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[LIBSVM.ProbabilisticNuSVC]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "LIBSVM"
":package_license" = "unknown"
":load_path" = "MLJLIBSVMInterface.ProbabilisticNuSVC"
":package_uuid" = "b1bec4e5-fd48-53fe-b0cb-9723c09d164b"
":package_url" = "https://github.com/mpastell/LIBSVM.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nProbabilisticNuSVC\n```\n\nA model type for constructing a probabilistic ν-support vector classifier, based on [LIBSVM.jl](https://github.com/mpastell/LIBSVM.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nProbabilisticNuSVC = @load ProbabilisticNuSVC pkg=LIBSVM\n```\n\nDo `model = ProbabilisticNuSVC()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `ProbabilisticNuSVC(kernel=...)`.\n\nThis model is identical to [`NuSVC`](@ref) with the exception that it predicts probabilities, instead of actual class labels. Probabilities are computed using Platt scaling, which will add to total computation time.\n\nReference for algorithm and core C-library: C.-C. Chang and C.-J. Lin (2011): \"LIBSVM: a library for support vector machines.\" *ACM Transactions on Intelligent Systems and Technology*, 2(3):27:1–27:27. Updated at [https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf](https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf). \n\n[Platt, John (1999): \"Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods.\"](https://citeseerx.ist.psu.edu/doc_view/pid/42e5ed832d4310ce4378c44d05570439df28a393)\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with:\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have `Continuous` element scitype; check column scitypes with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `<:OrderedFactor` or `<:Multiclass`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `kernel=LIBSVM.Kernel.RadialBasis`: either an object that can be called, as in `kernel(x1, x2)`, or one of the built-in kernels from the LIBSVM.jl package listed below.  Here `x1` and `x2` are vectors whose lengths match the number of columns of the training data `X` (see \"Examples\" below).\n\n      * `LIBSVM.Kernel.Linear`: `(x1, x2) -> x1'*x2`\n      * `LIBSVM.Kernel.Polynomial`: `(x1, x2) -> gamma*x1'*x2 + coef0)^degree`\n      * `LIBSVM.Kernel.RadialBasis`: `(x1, x2) -> (exp(-gamma*norm(x1 - x2)^2))`\n      * `LIBSVM.Kernel.Sigmoid`: `(x1, x2) - > tanh(gamma*x1'*x2 + coef0)`\n\n    Here `gamma`, `coef0`, `degree` are other hyper-parameters. Serialization of models with user-defined kernels comes with some restrictions. See [LIVSVM.jl issue91](https://github.com/JuliaML/LIBSVM.jl/issues/91)\n  * `gamma = 0.0`: kernel parameter (see above); if `gamma==-1.0` then `gamma = 1/nfeatures` is used in training, where `nfeatures` is the number of features (columns of `X`).  If `gamma==0.0` then `gamma = 1/(var(Tables.matrix(X))*nfeatures)` is used. Actual value used appears in the report (see below).\n  * `coef0 = 0.0`: kernel parameter (see above)\n  * `degree::Int32 = Int32(3)`: degree in polynomial kernel (see above)\n\n  * `nu=0.5` (range (0, 1]): An upper bound on the fraction of margin errors and a lower bound of the fraction of support vectors. Denoted `ν` in the cited paper. Changing `nu` changes the thickness of the margin (a neighborhood of the decision surface) and a margin error is said to have occurred if a training observation lies on the wrong side of the surface or within the margin.\n  * `cachesize=200.0` cache memory size in MB\n  * `tolerance=0.001`: tolerance for the stopping criterion\n  * `shrinking=true`: whether to use shrinking heuristics\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given features `Xnew` having the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `libsvm_model`: the trained model object created by the LIBSVM.jl package\n  * `encoding`: class encoding used internally by `libsvm_model` - a dictionary of class labels keyed on the internal integer representation\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `gamma`: actual value of the kernel parameter `gamma` used in training\n\n# Examples\n\n## Using a built-in kernel\n\n```\nusing MLJ\nimport LIBSVM\n\nProbabilisticNuSVC = @load ProbabilisticNuSVC pkg=LIBSVM    # model type\nmodel = ProbabilisticNuSVC(kernel=LIBSVM.Kernel.Polynomial) # instance\n\nX, y = @load_iris # table, vector\nmach = machine(model, X, y) |> fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\n\njulia> probs = predict(mach, Xnew)\n3-element UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float64}:\n UnivariateFinite{Multiclass{3}}(setosa=>0.00313, versicolor=>0.0247, virginica=>0.972)\n UnivariateFinite{Multiclass{3}}(setosa=>0.000598, versicolor=>0.0155, virginica=>0.984)\n UnivariateFinite{Multiclass{3}}(setosa=>2.27e-6, versicolor=>2.73e-6, virginica=>1.0)\n\njulia> yhat = mode.(probs)\n3-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"virginica\"\n \"virginica\"\n \"virginica\"\n```\n\n## User-defined kernels\n\n```\nk(x1, x2) = x1'*x2 # equivalent to `LIBSVM.Kernel.Linear`\nmodel = ProbabilisticNuSVC(kernel=k)\nmach = machine(model, X, y) |> fit!\n\nprobs = predict(mach, Xnew)\n```\n\nSee also the classifiers [`NuSVC`](@ref), [`SVC`](@ref), [`ProbabilisticSVC`](@ref) and [`LinearSVC`](@ref). And see [LIVSVM.jl](https://github.com/JuliaML/LIBSVM.jl) and the original C implementation. [documentation](https://github.com/cjlin1/libsvm/blob/master/README).\n"
":name" = "ProbabilisticNuSVC"
":human_name" = "probabilistic ν-support vector classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:kernel, :gamma, :nu, :cachesize, :degree, :coef0, :tolerance, :shrinking)`"
":hyperparameter_types" = "`(\"Any\", \"Float64\", \"Float64\", \"Float64\", \"Int32\", \"Float64\", \"Float64\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[LIBSVM.EpsilonSVR]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "LIBSVM"
":package_license" = "unknown"
":load_path" = "MLJLIBSVMInterface.EpsilonSVR"
":package_uuid" = "b1bec4e5-fd48-53fe-b0cb-9723c09d164b"
":package_url" = "https://github.com/mpastell/LIBSVM.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nEpsilonSVR\n```\n\nA model type for constructing a ϵ-support vector regressor, based on [LIBSVM.jl](https://github.com/mpastell/LIBSVM.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nEpsilonSVR = @load EpsilonSVR pkg=LIBSVM\n```\n\nDo `model = EpsilonSVR()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `EpsilonSVR(kernel=...)`.\n\nReference for algorithm and core C-library: C.-C. Chang and C.-J. Lin (2011): \"LIBSVM: a library for support vector machines.\" *ACM Transactions on Intelligent Systems and Technology*, 2(3):27:1–27:27. Updated at [https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf](https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf). \n\nThis model is an adaptation of the classifier `SVC` to regression, but has an additional parameter `epsilon` (denoted $ϵ$ in the cited reference).\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with:\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have `Continuous` element scitype; check column scitypes with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `Continuous`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `kernel=LIBSVM.Kernel.RadialBasis`: either an object that can be called, as in `kernel(x1, x2)`, or one of the built-in kernels from the LIBSVM.jl package listed below.  Here `x1` and `x2` are vectors whose lengths match the number of columns of the training data `X` (see \"Examples\" below).\n\n      * `LIBSVM.Kernel.Linear`: `(x1, x2) -> x1'*x2`\n      * `LIBSVM.Kernel.Polynomial`: `(x1, x2) -> gamma*x1'*x2 + coef0)^degree`\n      * `LIBSVM.Kernel.RadialBasis`: `(x1, x2) -> (exp(-gamma*norm(x1 - x2)^2))`\n      * `LIBSVM.Kernel.Sigmoid`: `(x1, x2) - > tanh(gamma*x1'*x2 + coef0)`\n\n    Here `gamma`, `coef0`, `degree` are other hyper-parameters. Serialization of models with user-defined kernels comes with some restrictions. See [LIVSVM.jl issue91](https://github.com/JuliaML/LIBSVM.jl/issues/91)\n  * `gamma = 0.0`: kernel parameter (see above); if `gamma==-1.0` then `gamma = 1/nfeatures` is used in training, where `nfeatures` is the number of features (columns of `X`).  If `gamma==0.0` then `gamma = 1/(var(Tables.matrix(X))*nfeatures)` is used. Actual value used appears in the report (see below).\n  * `coef0 = 0.0`: kernel parameter (see above)\n  * `degree::Int32 = Int32(3)`: degree in polynomial kernel (see above)\n\n  * `cost=1.0` (range (0, `Inf`)): the parameter denoted $C$ in the cited reference; for greater regularization, decrease `cost`\n  * `epsilon=0.1` (range (0, `Inf`)): the parameter denoted $ϵ$ in the cited reference; `epsilon` is the thickness of the penalty-free neighborhood of the graph of the prediction function (\"slab\" or \"tube\"). Specifically, a data point `(x, y)` incurs no training loss unless it is outside this neighborhood; the further away it is from the this neighborhood, the greater the loss penalty.\n  * `cachesize=200.0` cache memory size in MB\n  * `tolerance=0.001`: tolerance for the stopping criterion\n  * `shrinking=true`: whether to use shrinking heuristics\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given features `Xnew` having the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `libsvm_model`: the trained model object created by the LIBSVM.jl package\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `gamma`: actual value of the kernel parameter `gamma` used in training\n\n# Examples\n\n## Using a built-in kernel\n\n```\nusing MLJ\nimport LIBSVM\n\nEpsilonSVR = @load EpsilonSVR pkg=LIBSVM            # model type\nmodel = EpsilonSVR(kernel=LIBSVM.Kernel.Polynomial) # instance\n\nX, y = make_regression(rng=123) # table, vector\nmach = machine(model, X, y) |> fit!\n\nXnew, _ = make_regression(3, rng=123)\n\njulia> yhat = predict(mach, Xnew)\n3-element Vector{Float64}:\n  0.2512132502584155\n  0.007340201523624579\n -0.2482949812264707\n```\n\n## User-defined kernels\n\n```\nk(x1, x2) = x1'*x2 # equivalent to `LIBSVM.Kernel.Linear`\nmodel = EpsilonSVR(kernel=k)\nmach = machine(model, X, y) |> fit!\n\njulia> yhat = predict(mach, Xnew)\n3-element Vector{Float64}:\n  1.1121225361666656\n  0.04667702229741916\n -0.6958148424680672\n```\n\nSee also [`NuSVR`](@ref), [LIVSVM.jl](https://github.com/JuliaML/LIBSVM.jl) and the original C implementation [documentation](https://github.com/cjlin1/libsvm/blob/master/README).\n"
":name" = "EpsilonSVR"
":human_name" = "ϵ-support vector regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:kernel, :gamma, :epsilon, :cost, :cachesize, :degree, :coef0, :tolerance, :shrinking)`"
":hyperparameter_types" = "`(\"Any\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Int32\", \"Float64\", \"Float64\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[LIBSVM.LinearSVC]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}, AbstractDict{ScientificTypesBase.Finite, <:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "LIBSVM"
":package_license" = "unknown"
":load_path" = "MLJLIBSVMInterface.LinearSVC"
":package_uuid" = "b1bec4e5-fd48-53fe-b0cb-9723c09d164b"
":package_url" = "https://github.com/mpastell/LIBSVM.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`true`"
":supports_online" = "`false`"
":docstring" = "```\nLinearSVC\n```\n\nA model type for constructing a linear support vector classifier, based on [LIBSVM.jl](https://github.com/mpastell/LIBSVM.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nLinearSVC = @load LinearSVC pkg=LIBSVM\n```\n\nDo `model = LinearSVC()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `LinearSVC(solver=...)`.\n\nReference for algorithm and core C-library: Rong-En Fan et al (2008): \"LIBLINEAR: A Library for Large Linear Classification.\" *Journal of Machine Learning Research* 9 1871-1874. Available at [https://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf](https://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf). \n\nThis model type is similar to `SVC` from the same package with the setting `kernel=LIBSVM.Kernel.KERNEL.Linear`, but is optimized for the linear case.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with one of:\n\n```\nmach = machine(model, X, y)\nmach = machine(model, X, y, w)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have `Continuous` element scitype; check column scitypes with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `<:OrderedFactor` or `<:Multiclass`; check the scitype with `scitype(y)`\n  * `w`: a dictionary of class weights, keyed on `levels(y)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `solver=LIBSVM.Linearsolver.L2R_L2LOSS_SVC_DUAL`: linear solver, which must be one of the following from the LIBSVM.jl package:\n\n      * `LIBSVM.Linearsolver.L2R_LR`: L2-regularized logistic regression (primal))\n      * `LIBSVM.Linearsolver.L2R_L2LOSS_SVC_DUAL`: L2-regularized L2-loss support vector classification (dual)\n      * `LIBSVM.Linearsolver.L2R_L2LOSS_SVC`: L2-regularized L2-loss support vector classification (primal)\n      * `LIBSVM.Linearsolver.L2R_L1LOSS_SVC_DUAL`: L2-regularized L1-loss support vector classification (dual)\n      * `LIBSVM.Linearsolver.MCSVM_CS`: support vector classification by Crammer and Singer) `LIBSVM.Linearsolver.L1R_L2LOSS_SVC`: L1-regularized L2-loss support vector classification)\n      * `LIBSVM.Linearsolver.L1R_LR`:  L1-regularized logistic regression\n      * `LIBSVM.Linearsolver.L2R_LR_DUAL`: L2-regularized logistic regression (dual)\n  * `tolerance::Float64=Inf`: tolerance for the stopping criterion;\n  * `cost=1.0` (range (0, `Inf`)): the parameter denoted $C$ in the cited reference; for greater regularization, decrease `cost`\n  * `bias= -1.0`: if `bias >= 0`, instance `x` becomes `[x; bias]`; if `bias < 0`, no bias term added (default -1)\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given features `Xnew` having the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `libsvm_model`: the trained model object created by the LIBSVM.jl package\n  * `encoding`: class encoding used internally by `libsvm_model` - a dictionary of class labels keyed on the internal integer representation\n\n# Examples\n\n```\nusing MLJ\nimport LIBSVM\n\nLinearSVC = @load LinearSVC pkg=LIBSVM               # model type\nmodel = LinearSVC(solver=LIBSVM.Linearsolver.L2R_LR) # instance\n\nX, y = @load_iris # table, vector\nmach = machine(model, X, y) |> fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\n\njulia> yhat = predict(mach, Xnew)\n3-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"virginica\"\n \"versicolor\"\n \"virginica\"\n```\n\n## Incorporating class weights\n\n```julia\nweights = Dict(\"virginica\" => 1, \"versicolor\" => 20, \"setosa\" => 1)\nmach = machine(model, X, y, weights) |> fit!\n\njulia> yhat = predict(mach, Xnew)\n3-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"versicolor\"\n \"versicolor\"\n \"versicolor\"\n```\n\nSee also the [`SVC`](@ref) and [`NuSVC`](@ref) classifiers, and [LIVSVM.jl](https://github.com/JuliaML/LIBSVM.jl) and the original C implementation [documentation](https://github.com/cjlin1/liblinear/blob/master/README).\n"
":name" = "LinearSVC"
":human_name" = "linear support vector classifier"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:solver, :tolerance, :cost, :bias)`"
":hyperparameter_types" = "`(\"LIBSVM.Linearsolver.LINEARSOLVER\", \"Float64\", \"Float64\", \"Float64\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[LIBSVM.ProbabilisticSVC]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}, AbstractDict{ScientificTypesBase.Finite, <:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "LIBSVM"
":package_license" = "unknown"
":load_path" = "MLJLIBSVMInterface.ProbabilisticSVC"
":package_uuid" = "b1bec4e5-fd48-53fe-b0cb-9723c09d164b"
":package_url" = "https://github.com/mpastell/LIBSVM.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`true`"
":supports_online" = "`false`"
":docstring" = "```\nProbabilisticSVC\n```\n\nA model type for constructing a probabilistic C-support vector classifier, based on [LIBSVM.jl](https://github.com/mpastell/LIBSVM.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nProbabilisticSVC = @load ProbabilisticSVC pkg=LIBSVM\n```\n\nDo `model = ProbabilisticSVC()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `ProbabilisticSVC(kernel=...)`.\n\nThis model is identical to [`SVC`](@ref) with the exception that it predicts probabilities, instead of actual class labels. Probabilities are computed using Platt scaling, which will add to the total computation time.\n\nReference for algorithm and core C-library: C.-C. Chang and C.-J. Lin (2011): \"LIBSVM: a library for support vector machines.\" *ACM Transactions on Intelligent Systems and Technology*, 2(3):27:1–27:27. Updated at [https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf](https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf). \n\n[Platt, John (1999): \"Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods.\"](https://citeseerx.ist.psu.edu/doc_view/pid/42e5ed832d4310ce4378c44d05570439df28a393)\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with one of:\n\n```\nmach = machine(model, X, y)\nmach = machine(model, X, y, w)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have `Continuous` element scitype; check column scitypes with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `<:OrderedFactor` or `<:Multiclass`; check the scitype with `scitype(y)`\n  * `w`: a dictionary of class weights, keyed on `levels(y)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `kernel=LIBSVM.Kernel.RadialBasis`: either an object that can be called, as in `kernel(x1, x2)`, or one of the built-in kernels from the LIBSVM.jl package listed below.  Here `x1` and `x2` are vectors whose lengths match the number of columns of the training data `X` (see \"Examples\" below).\n\n      * `LIBSVM.Kernel.Linear`: `(x1, x2) -> x1'*x2`\n      * `LIBSVM.Kernel.Polynomial`: `(x1, x2) -> gamma*x1'*x2 + coef0)^degree`\n      * `LIBSVM.Kernel.RadialBasis`: `(x1, x2) -> (exp(-gamma*norm(x1 - x2)^2))`\n      * `LIBSVM.Kernel.Sigmoid`: `(x1, x2) - > tanh(gamma*x1'*x2 + coef0)`\n\n    Here `gamma`, `coef0`, `degree` are other hyper-parameters. Serialization of models with user-defined kernels comes with some restrictions. See [LIVSVM.jl issue91](https://github.com/JuliaML/LIBSVM.jl/issues/91)\n  * `gamma = 0.0`: kernel parameter (see above); if `gamma==-1.0` then `gamma = 1/nfeatures` is used in training, where `nfeatures` is the number of features (columns of `X`).  If `gamma==0.0` then `gamma = 1/(var(Tables.matrix(X))*nfeatures)` is used. Actual value used appears in the report (see below).\n  * `coef0 = 0.0`: kernel parameter (see above)\n  * `degree::Int32 = Int32(3)`: degree in polynomial kernel (see above)\n\n  * `cost=1.0` (range (0, `Inf`)): the parameter denoted $C$ in the cited reference; for greater regularization, decrease `cost`\n  * `cachesize=200.0` cache memory size in MB\n  * `tolerance=0.001`: tolerance for the stopping criterion\n  * `shrinking=true`: whether to use shrinking heuristics\n\n# Operations\n\n  * `predict(mach, Xnew)`: return probabilistic predictions of the target given features `Xnew` having the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `libsvm_model`: the trained model object created by the LIBSVM.jl package\n  * `encoding`: class encoding used internally by `libsvm_model` - a dictionary of class labels keyed on the internal integer representation\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `gamma`: actual value of the kernel parameter `gamma` used in training\n\n# Examples\n\n## Using a built-in kernel\n\n```\nusing MLJ\nimport LIBSVM\n\nProbabilisticSVC = @load ProbabilisticSVC pkg=LIBSVM      # model type\nmodel = ProbabilisticSVC(kernel=LIBSVM.Kernel.Polynomial) # instance\n\nX, y = @load_iris # table, vector\nmach = machine(model, X, y) |> fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\n\njulia> probs = predict(mach, Xnew)\n3-element UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float64}:\n UnivariateFinite{Multiclass{3}}(setosa=>0.00186, versicolor=>0.003, virginica=>0.995)\n UnivariateFinite{Multiclass{3}}(setosa=>0.000563, versicolor=>0.0554, virginica=>0.944)\n UnivariateFinite{Multiclass{3}}(setosa=>1.4e-6, versicolor=>1.68e-6, virginica=>1.0)\n\n\njulia> labels = mode.(probs)\n3-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"virginica\"\n \"virginica\"\n \"virginica\"\n```\n\n## User-defined kernels\n\n```\nk(x1, x2) = x1'*x2 # equivalent to `LIBSVM.Kernel.Linear`\nmodel = ProbabilisticSVC(kernel=k)\nmach = machine(model, X, y) |> fit!\n\nprobs = predict(mach, Xnew)\n```\n\n## Incorporating class weights\n\nIn either scenario above, we can do:\n\n```julia\nweights = Dict(\"virginica\" => 1, \"versicolor\" => 20, \"setosa\" => 1)\nmach = machine(model, X, y, weights) |> fit!\n\nprobs = predict(mach, Xnew)\n```\n\nSee also the classifiers [`SVC`](@ref), [`NuSVC`](@ref) and [`LinearSVC`](@ref), and [LIVSVM.jl](https://github.com/JuliaML/LIBSVM.jl) and the original C implementation [documentation](https://github.com/cjlin1/libsvm/blob/master/README).\n"
":name" = "ProbabilisticSVC"
":human_name" = "probabilistic C-support vector classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:kernel, :gamma, :cost, :cachesize, :degree, :coef0, :tolerance, :shrinking)`"
":hyperparameter_types" = "`(\"Any\", \"Float64\", \"Float64\", \"Float64\", \"Int32\", \"Float64\", \"Float64\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[LIBSVM.NuSVR]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "LIBSVM"
":package_license" = "unknown"
":load_path" = "MLJLIBSVMInterface.NuSVR"
":package_uuid" = "b1bec4e5-fd48-53fe-b0cb-9723c09d164b"
":package_url" = "https://github.com/mpastell/LIBSVM.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nNuSVR\n```\n\nA model type for constructing a ν-support vector regressor, based on [LIBSVM.jl](https://github.com/mpastell/LIBSVM.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nNuSVR = @load NuSVR pkg=LIBSVM\n```\n\nDo `model = NuSVR()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `NuSVR(kernel=...)`.\n\nReference for algorithm and core C-library: C.-C. Chang and C.-J. Lin (2011): \"LIBSVM: a library for support vector machines.\" *ACM Transactions on Intelligent Systems and Technology*, 2(3):27:1–27:27. Updated at [https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf](https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf). \n\nThis model is a re-parameterization of `EpsilonSVR` in which the `epsilon` hyper-parameter is replaced with a new parameter `nu` (denoted $ν$ in the cited reference) which attempts to control the number of support vectors directly.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with:\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have `Continuous` element scitype; check column scitypes with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `Continuous`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  *   * `kernel=LIBSVM.Kernel.RadialBasis`: either an object that can be\n\n    called, as in `kernel(x1, x2)`, or one of the built-in kernels from the LIBSVM.jl package listed below.  Here `x1` and `x2` are vectors whose lengths match the number of columns of the training data `X` (see \"Examples\" below).\n\n      * `LIBSVM.Kernel.Linear`: `(x1, x2) -> x1'*x2`\n      * `LIBSVM.Kernel.Polynomial`: `(x1, x2) -> gamma*x1'*x2 + coef0)^degree`\n      * `LIBSVM.Kernel.RadialBasis`: `(x1, x2) -> (exp(-gamma*norm(x1 - x2)^2))`\n      * `LIBSVM.Kernel.Sigmoid`: `(x1, x2) - > tanh(gamma*x1'*x2 + coef0)`\n\n    Here `gamma`, `coef0`, `degree` are other hyper-parameters. Serialization of models with user-defined kernels comes with some restrictions. See [LIVSVM.jl issue91](https://github.com/JuliaML/LIBSVM.jl/issues/91)\n  * `gamma = 0.0`: kernel parameter (see above); if `gamma==-1.0` then `gamma = 1/nfeatures` is used in training, where `nfeatures` is the number of features (columns of `X`).  If `gamma==0.0` then `gamma = 1/(var(Tables.matrix(X))*nfeatures)` is used. Actual value used appears in the report (see below).\n  * `coef0 = 0.0`: kernel parameter (see above)\n  * `degree::Int32 = Int32(3)`: degree in polynomial kernel (see above)\n\n  * `cost=1.0` (range (0, `Inf`)): the parameter denoted $C$ in the cited reference; for greater regularization, decrease `cost`\n  * `nu=0.5` (range (0, 1]): An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. Denoted $ν$ in the cited paper. Changing `nu` changes the thickness of some neighborhood of the graph of the prediction function (\"tube\" or \"slab\") and a training error is said to occur when a data point `(x, y)` lies outside of that neighborhood.\n  * `cachesize=200.0` cache memory size in MB\n  * `tolerance=0.001`: tolerance for the stopping criterion\n  * `shrinking=true`: whether to use shrinking heuristics\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given features `Xnew` having the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `libsvm_model`: the trained model object created by the LIBSVM.jl package\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `gamma`: actual value of the kernel parameter `gamma` used in training\n\n# Examples\n\n## Using a built-in kernel\n\n```\nusing MLJ\nimport LIBSVM\n\nNuSVR = @load NuSVR pkg=LIBSVM                 # model type\nmodel = NuSVR(kernel=LIBSVM.Kernel.Polynomial) # instance\n\nX, y = make_regression(rng=123) # table, vector\nmach = machine(model, X, y) |> fit!\n\nXnew, _ = make_regression(3, rng=123)\n\njulia> yhat = predict(mach, Xnew)\n3-element Vector{Float64}:\n  0.2008156459920009\n  0.1131520519131709\n -0.2076156254934889\n```\n\n## User-defined kernels\n\n```\nk(x1, x2) = x1'*x2 # equivalent to `LIBSVM.Kernel.Linear`\nmodel = NuSVR(kernel=k)\nmach = machine(model, X, y) |> fit!\n\njulia> yhat = predict(mach, Xnew)\n3-element Vector{Float64}:\n  1.1211558175964662\n  0.06677125944808422\n -0.6817578942749346\n```\n\nSee also [`EpsilonSVR`](@ref), [LIVSVM.jl](https://github.com/JuliaML/LIBSVM.jl) and the original C implementation [documentation](https://github.com/cjlin1/libsvm/blob/master/README).\n"
":name" = "NuSVR"
":human_name" = "ν-support vector regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:kernel, :gamma, :nu, :cost, :cachesize, :degree, :coef0, :tolerance, :shrinking)`"
":hyperparameter_types" = "`(\"Any\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Int32\", \"Float64\", \"Float64\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[LIBSVM.NuSVC]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "LIBSVM"
":package_license" = "unknown"
":load_path" = "MLJLIBSVMInterface.NuSVC"
":package_uuid" = "b1bec4e5-fd48-53fe-b0cb-9723c09d164b"
":package_url" = "https://github.com/mpastell/LIBSVM.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nNuSVC\n```\n\nA model type for constructing a ν-support vector classifier, based on [LIBSVM.jl](https://github.com/mpastell/LIBSVM.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nNuSVC = @load NuSVC pkg=LIBSVM\n```\n\nDo `model = NuSVC()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `NuSVC(kernel=...)`.\n\nThis model is a re-parameterization of the [`SVC`](@ref) classifier, where `nu` replaces `cost`, and is mathematically equivalent to it. The parameter `nu` allows more direct control over the number of support vectors (see under \"Hyper-parameters\").\n\nThis model always predicts actual class labels. For probabilistic predictions, use instead [`ProbabilisticNuSVC`](@ref).\n\nReference for algorithm and core C-library: C.-C. Chang and C.-J. Lin (2011): \"LIBSVM: a library for support vector machines.\" *ACM Transactions on Intelligent Systems and Technology*, 2(3):27:1–27:27. Updated at [https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf](https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf). \n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with:\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have `Continuous` element scitype; check column scitypes with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `<:OrderedFactor` or `<:Multiclass`; check the scitype with `scitype(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `kernel=LIBSVM.Kernel.RadialBasis`: either an object that can be called, as in `kernel(x1, x2)`, or one of the built-in kernels from the LIBSVM.jl package listed below.  Here `x1` and `x2` are vectors whose lengths match the number of columns of the training data `X` (see \"Examples\" below).\n\n      * `LIBSVM.Kernel.Linear`: `(x1, x2) -> x1'*x2`\n      * `LIBSVM.Kernel.Polynomial`: `(x1, x2) -> gamma*x1'*x2 + coef0)^degree`\n      * `LIBSVM.Kernel.RadialBasis`: `(x1, x2) -> (exp(-gamma*norm(x1 - x2)^2))`\n      * `LIBSVM.Kernel.Sigmoid`: `(x1, x2) - > tanh(gamma*x1'*x2 + coef0)`\n\n    Here `gamma`, `coef0`, `degree` are other hyper-parameters. Serialization of models with user-defined kernels comes with some restrictions. See [LIVSVM.jl issue91](https://github.com/JuliaML/LIBSVM.jl/issues/91)\n  * `gamma = 0.0`: kernel parameter (see above); if `gamma==-1.0` then `gamma = 1/nfeatures` is used in training, where `nfeatures` is the number of features (columns of `X`).  If `gamma==0.0` then `gamma = 1/(var(Tables.matrix(X))*nfeatures)` is used. Actual value used appears in the report (see below).\n  * `coef0 = 0.0`: kernel parameter (see above)\n  * `degree::Int32 = Int32(3)`: degree in polynomial kernel (see above)\n\n  * `nu=0.5` (range (0, 1]): An upper bound on the fraction of margin errors and a lower bound of the fraction of support vectors. Denoted `ν` in the cited paper. Changing `nu` changes the thickness of the margin (a neighborhood of the decision surface) and a margin error is said to have occurred if a training observation lies on the wrong side of the surface or within the margin.\n  * `cachesize=200.0` cache memory size in MB\n  * `tolerance=0.001`: tolerance for the stopping criterion\n  * `shrinking=true`: whether to use shrinking heuristics\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given features `Xnew` having the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `libsvm_model`: the trained model object created by the LIBSVM.jl package\n  * `encoding`: class encoding used internally by `libsvm_model` - a dictionary of class labels keyed on the internal integer representation\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `gamma`: actual value of the kernel parameter `gamma` used in training\n\n# Examples\n\n## Using a built-in kernel\n\n```\nusing MLJ\nimport LIBSVM\n\nNuSVC = @load NuSVC pkg=LIBSVM                 # model type\nmodel = NuSVC(kernel=LIBSVM.Kernel.Polynomial) # instance\n\nX, y = @load_iris # table, vector\nmach = machine(model, X, y) |> fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\n\njulia> yhat = predict(mach, Xnew)\n3-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"virginica\"\n \"virginica\"\n \"virginica\"\n```\n\n## User-defined kernels\n\n```\nk(x1, x2) = x1'*x2 # equivalent to `LIBSVM.Kernel.Linear`\nmodel = NuSVC(kernel=k)\nmach = machine(model, X, y) |> fit!\n\njulia> yhat = predict(mach, Xnew)\n3-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"virginica\"\n \"virginica\"\n \"virginica\"\n```\n\nSee also the classifiers [`SVC`](@ref) and [`LinearSVC`](@ref), [LIVSVM.jl](https://github.com/JuliaML/LIBSVM.jl) and the original C implementation. [documentation](https://github.com/cjlin1/libsvm/blob/master/README).\n"
":name" = "NuSVC"
":human_name" = "ν-support vector classifier"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:kernel, :gamma, :nu, :cachesize, :degree, :coef0, :tolerance, :shrinking)`"
":hyperparameter_types" = "`(\"Any\", \"Float64\", \"Float64\", \"Float64\", \"Int32\", \"Float64\", \"Float64\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[LIBSVM.SVC]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}, AbstractDict{ScientificTypesBase.Finite, <:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "LIBSVM"
":package_license" = "unknown"
":load_path" = "MLJLIBSVMInterface.SVC"
":package_uuid" = "b1bec4e5-fd48-53fe-b0cb-9723c09d164b"
":package_url" = "https://github.com/mpastell/LIBSVM.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`true`"
":supports_online" = "`false`"
":docstring" = "```\nSVC\n```\n\nA model type for constructing a C-support vector classifier, based on [LIBSVM.jl](https://github.com/mpastell/LIBSVM.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nSVC = @load SVC pkg=LIBSVM\n```\n\nDo `model = SVC()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `SVC(kernel=...)`.\n\nThis model predicts actual class labels. To predict probabilities, use instead [`ProbabilisticSVC`](@ref).\n\nReference for algorithm and core C-library: C.-C. Chang and C.-J. Lin (2011): \"LIBSVM: a library for support vector machines.\" *ACM Transactions on Intelligent Systems and Technology*, 2(3):27:1–27:27. Updated at [https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf](https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf). \n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with one of:\n\n```\nmach = machine(model, X, y)\nmach = machine(model, X, y, w)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have `Continuous` element scitype; check column scitypes with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `<:OrderedFactor` or `<:Multiclass`; check the scitype with `scitype(y)`\n  * `w`: a dictionary of class weights, keyed on `levels(y)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `kernel=LIBSVM.Kernel.RadialBasis`: either an object that can be called, as in `kernel(x1, x2)`, or one of the built-in kernels from the LIBSVM.jl package listed below.  Here `x1` and `x2` are vectors whose lengths match the number of columns of the training data `X` (see \"Examples\" below).\n\n      * `LIBSVM.Kernel.Linear`: `(x1, x2) -> x1'*x2`\n      * `LIBSVM.Kernel.Polynomial`: `(x1, x2) -> gamma*x1'*x2 + coef0)^degree`\n      * `LIBSVM.Kernel.RadialBasis`: `(x1, x2) -> (exp(-gamma*norm(x1 - x2)^2))`\n      * `LIBSVM.Kernel.Sigmoid`: `(x1, x2) - > tanh(gamma*x1'*x2 + coef0)`\n\n    Here `gamma`, `coef0`, `degree` are other hyper-parameters. Serialization of models with user-defined kernels comes with some restrictions. See [LIVSVM.jl issue91](https://github.com/JuliaML/LIBSVM.jl/issues/91)\n  * `gamma = 0.0`: kernel parameter (see above); if `gamma==-1.0` then `gamma = 1/nfeatures` is used in training, where `nfeatures` is the number of features (columns of `X`).  If `gamma==0.0` then `gamma = 1/(var(Tables.matrix(X))*nfeatures)` is used. Actual value used appears in the report (see below).\n  * `coef0 = 0.0`: kernel parameter (see above)\n  * `degree::Int32 = Int32(3)`: degree in polynomial kernel (see above)\n\n  * `cost=1.0` (range (0, `Inf`)): the parameter denoted $C$ in the cited reference; for greater regularization, decrease `cost`\n  * `cachesize=200.0` cache memory size in MB\n  * `tolerance=0.001`: tolerance for the stopping criterion\n  * `shrinking=true`: whether to use shrinking heuristics\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given features `Xnew` having the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `libsvm_model`: the trained model object created by the LIBSVM.jl package\n  * `encoding`: class encoding used internally by `libsvm_model` - a dictionary of class labels keyed on the internal integer representation\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `gamma`: actual value of the kernel parameter `gamma` used in training\n\n# Examples\n\n## Using a built-in kernel\n\n```\nusing MLJ\nimport LIBSVM\n\nSVC = @load SVC pkg=LIBSVM                   # model type\nmodel = SVC(kernel=LIBSVM.Kernel.Polynomial) # instance\n\nX, y = @load_iris # table, vector\nmach = machine(model, X, y) |> fit!\n\nXnew = (sepal_length = [6.4, 7.2, 7.4],\n        sepal_width = [2.8, 3.0, 2.8],\n        petal_length = [5.6, 5.8, 6.1],\n        petal_width = [2.1, 1.6, 1.9],)\n\njulia> yhat = predict(mach, Xnew)\n3-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"virginica\"\n \"virginica\"\n \"virginica\"\n```\n\n## User-defined kernels\n\n```\nk(x1, x2) = x1'*x2 # equivalent to `LIBSVM.Kernel.Linear`\nmodel = SVC(kernel=k)\nmach = machine(model, X, y) |> fit!\n\njulia> yhat = predict(mach, Xnew)\n3-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"virginica\"\n \"virginica\"\n \"virginica\"\n```\n\n## Incorporating class weights\n\nIn either scenario above, we can do:\n\n```julia\nweights = Dict(\"virginica\" => 1, \"versicolor\" => 20, \"setosa\" => 1)\nmach = machine(model, X, y, weights) |> fit!\n\njulia> yhat = predict(mach, Xnew)\n3-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"versicolor\"\n \"versicolor\"\n \"versicolor\"\n```\n\nSee also the classifiers [`ProbabilisticSVC`](@ref), [`NuSVC`](@ref) and [`LinearSVC`](@ref). And see [LIVSVM.jl](https://github.com/JuliaML/LIBSVM.jl) and the original C implementation [documentation](https://github.com/cjlin1/libsvm/blob/master/README).\n"
":name" = "SVC"
":human_name" = "C-support vector classifier"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":hyperparameters" = "`(:kernel, :gamma, :cost, :cachesize, :degree, :coef0, :tolerance, :shrinking)`"
":hyperparameter_types" = "`(\"Any\", \"Float64\", \"Float64\", \"Float64\", \"Int32\", \"Float64\", \"Float64\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[LIBSVM.OneClassSVM]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Binary}`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`false`"
":package_name" = "LIBSVM"
":package_license" = "unknown"
":load_path" = "MLJLIBSVMInterface.OneClassSVM"
":package_uuid" = "b1bec4e5-fd48-53fe-b0cb-9723c09d164b"
":package_url" = "https://github.com/mpastell/LIBSVM.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nOneClassSVM\n```\n\nA model type for constructing a one-class support vector machine, based on [LIBSVM.jl](https://github.com/mpastell/LIBSVM.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nOneClassSVM = @load OneClassSVM pkg=LIBSVM\n```\n\nDo `model = OneClassSVM()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `OneClassSVM(kernel=...)`.\n\nReference for algorithm and core C-library: C.-C. Chang and C.-J. Lin (2011): \"LIBSVM: a library for support vector machines.\" *ACM Transactions on Intelligent Systems and Technology*, 2(3):27:1–27:27. Updated at [https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf](https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf). \n\nThis model is an outlier detection model delivering raw scores based on the decision function of a support vector machine. Like the [`NuSVC`](@ref) classifier, it uses the `nu` re-parameterization of the `cost` parameter appearing in standard support vector classification [`SVC`](@ref).\n\nTo extract normalized scores (\"probabilities\") wrap the model using `ProbabilisticDetector` from [OutlierDetection.jl](https://github.com/OutlierDetectionJL/OutlierDetection.jl). For threshold-based classification, wrap the probabilistic model using MLJ's `BinaryThresholdPredictor`. Examples of wrapping appear below.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with:\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have `Continuous` element scitype; check column scitypes with `schema(X)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `kernel=LIBSVM.Kernel.RadialBasis`: either an object that can be called, as in `kernel(x1, x2)`, or one of the built-in kernels from the LIBSVM.jl package listed below.  Here `x1` and `x2` are vectors whose lengths match the number of columns of the training data `X` (see \"Examples\" below).\n\n      * `LIBSVM.Kernel.Linear`: `(x1, x2) -> x1'*x2`\n      * `LIBSVM.Kernel.Polynomial`: `(x1, x2) -> gamma*x1'*x2 + coef0)^degree`\n      * `LIBSVM.Kernel.RadialBasis`: `(x1, x2) -> (exp(-gamma*norm(x1 - x2)^2))`\n      * `LIBSVM.Kernel.Sigmoid`: `(x1, x2) - > tanh(gamma*x1'*x2 + coef0)`\n\n    Here `gamma`, `coef0`, `degree` are other hyper-parameters. Serialization of models with user-defined kernels comes with some restrictions. See [LIVSVM.jl issue91](https://github.com/JuliaML/LIBSVM.jl/issues/91)\n  * `gamma = 0.0`: kernel parameter (see above); if `gamma==-1.0` then `gamma = 1/nfeatures` is used in training, where `nfeatures` is the number of features (columns of `X`).  If `gamma==0.0` then `gamma = 1/(var(Tables.matrix(X))*nfeatures)` is used. Actual value used appears in the report (see below).\n  * `coef0 = 0.0`: kernel parameter (see above)\n  * `degree::Int32 = Int32(3)`: degree in polynomial kernel (see above)\n\n  * `nu=0.5` (range (0, 1]): An upper bound on the fraction of margin errors and a lower bound of the fraction of support vectors. Denoted `ν` in the cited paper. Changing `nu` changes the thickness of the margin (a neighborhood of the decision surface) and a margin error is said to have occurred if a training observation lies on the wrong side of the surface or within the margin.\n  * `cachesize=200.0` cache memory size in MB\n  * `tolerance=0.001`: tolerance for the stopping criterion\n  * `shrinking=true`: whether to use shrinking heuristics\n\n# Operations\n\n  * `transform(mach, Xnew)`: return scores for outlierness, given features `Xnew` having the same scitype as `X` above. The greater the score, the more likely it is an outlier. This score is based on the SVM decision function. For normalized scores, wrap `model` using `ProbabilisticDetector` from OutlierDetection.jl and call `predict` instead, and for threshold-based classification, wrap again using `BinaryThresholdPredictor`. See the examples below.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `libsvm_model`: the trained model object created by the LIBSVM.jl package\n  * `orientation`: this equals `1` if the decision function for `libsvm_model` is increasing with increasing outlierness, and `-1` if it is decreasing instead. Correspondingly, the `libsvm_model` attaches `true` to outliers in the first case, and `false` in the second. (The `scores` given in the MLJ report and generated by `MLJ.transform` already correct for this ambiguity, which is therefore only an issue for users directly accessing `libsvm_model`.)\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `gamma`: actual value of the kernel parameter `gamma` used in training\n\n# Examples\n\n## Generating raw scores for outlierness\n\n```\nusing MLJ\nimport LIBSVM\nimport StableRNGs.StableRNG\n\nOneClassSVM = @load OneClassSVM pkg=LIBSVM           # model type\nmodel = OneClassSVM(kernel=LIBSVM.Kernel.Polynomial) # instance\n\nrng = StableRNG(123)\nXmatrix = randn(rng, 5, 3)\nXmatrix[1, 1] = 100.0\nX = MLJ.table(Xmatrix)\n\nmach = machine(model, X) |> fit!\n\n# training scores (outliers have larger scores):\njulia> report(mach).scores\n5-element Vector{Float64}:\n  6.711689156091755e-7\n -6.740101976655081e-7\n -6.711632439648446e-7\n -6.743015858874887e-7\n -6.745393717880104e-7\n\n# scores for new data:\nXnew = MLJ.table(rand(rng, 2, 3))\n\njulia> transform(mach, rand(rng, 2, 3))\n2-element Vector{Float64}:\n -6.746293022511047e-7\n -6.744289265348623e-7\n```\n\n## Generating probabilistic predictions of outlierness\n\nContinuing the previous example:\n\n```\nusing OutlierDetection\npmodel = ProbabilisticDetector(model)\npmach = machine(pmodel, X) |> fit!\n\n# probabilistic predictions on new data:\n\njulia> y_prob = predict(pmach, Xnew)\n2-element UnivariateFiniteVector{OrderedFactor{2}, String, UInt8, Float64}:\n UnivariateFinite{OrderedFactor{2}}(normal=>1.0, outlier=>9.57e-5)\n UnivariateFinite{OrderedFactor{2}}(normal=>1.0, outlier=>0.0)\n\n# probabilities for outlierness:\n\njulia> pdf.(y_prob, \"outlier\")\n2-element Vector{Float64}:\n 9.572583265925801e-5\n 0.0\n\n# raw scores are still available using `transform`:\n\njulia> transform(pmach, Xnew)\n2-element Vector{Float64}:\n 9.572583265925801e-5\n 0.0\n```\n\n## Outlier classification using a probability threshold:\n\nContinuing the previous example:\n\n```\ndmodel = BinaryThresholdPredictor(pmodel, threshold=0.9)\ndmach = machine(dmodel, X) |> fit!\n\njulia> yhat = predict(dmach, Xnew)\n2-element CategoricalArrays.CategoricalArray{String,1,UInt8}:\n \"normal\"\n \"normal\"\n```\n\n## User-defined kernels\n\nContinuing the first example:\n\n```\nk(x1, x2) = x1'*x2 # equivalent to `LIBSVM.Kernel.Linear`\nmodel = OneClassSVM(kernel=k)\nmach = machine(model, X) |> fit!\n\njulia> yhat = transform(mach, Xnew)\n2-element Vector{Float64}:\n -0.4825363352732942\n -0.4848772169720227\n```\n\nSee also [LIVSVM.jl](https://github.com/JuliaML/LIBSVM.jl) and the original C implementation [documentation](https://github.com/cjlin1/libsvm/blob/master/README). For an alternative source of outlier detection models with an MLJ interface, see [OutlierDetection.jl](https://outlierdetectionjl.github.io/OutlierDetection.jl/dev/).\n"
":name" = "OneClassSVM"
":human_name" = "one-class support vector machine"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":transform"]
":hyperparameters" = "`(:kernel, :gamma, :nu, :cachesize, :degree, :coef0, :tolerance, :shrinking)`"
":hyperparameter_types" = "`(\"Any\", \"Float64\", \"Float64\", \"Float64\", \"Int32\", \"Float64\", \"Float64\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[SIRUS.StableRulesClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "SIRUS"
":package_license" = "MIT"
":load_path" = "SIRUS.StableRulesClassifier"
":package_uuid" = "cdeec39e-fb35-4959-aadb-a1dd5dede958"
":package_url" = "https://github.com/rikhuijzer/SIRUS.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "    StableRulesClassifier(;\n        rng::AbstractRNG=default_rng(),\n        partial_sampling::Real=0.7,\n        n_trees::Int=1_000,\n        max_depth::Int=2,\n        q::Int=10,\n        min_data_in_leaf::Int=5,\n        max_rules::Int=10\n    ) -> MLJModelInterface.Probabilistic\n\nExplainable rule-based model based on a random forest.\nThis SIRUS algorithm extracts rules from a stabilized random forest.\nSee the [main page of the documentation](https://huijzer.xyz/StableTrees.jl/dev/) for details about how it works.\n\n# Example\n\nThe classifier satisfies the MLJ interface, so it can be used like any other MLJ model.\nFor example, it can be used to create a machine:\n\n```julia\njulia> using SIRUS, MLJ\n\njulia> mach = machine(StableRulesClassifier(; max_rules=15), X, y);\n```\n\n# Arguments\n\n- `rng`: Random number generator. `StableRNGs` are advised.\n- `partial_sampling`:\n    Ratio of samples to use in each subset of the data.\n    The default of 0.7 should be fine for most cases.\n- `n_trees`:\n    The number of trees to use.\n    The higher the number, the more likely it is that the correct rules are extracted from the trees, but also the longer model fitting will take.\n    In most cases, 1000 rules should be more than enough, but it might be useful to run 2000 rules one time and verify that the model performance does not change much.\n- `max_depth`:\n    The depth of the tree.\n    A lower depth decreases model complexity and can therefore improve accuracy when the sample size is small (reduce overfitting).\n- `q`: Number of cutpoints to use per feature.\n    The default value of 10 should be good for most situations.\n- `min_data_in_leaf`: Minimum number of data points per leaf.\n- `max_rules`:\n    This is the most important hyperparameter.\n    In general, the more rules, the more accurate the model.\n    However, more rules will also decrease model interpretability.\n    So, it is important to find a good balance here.\n    In most cases, 10-40 rules should provide reasonable accuracy while remaining interpretable.\n- `lambda`:\n    The weights of the final rules are determined via a regularized regression over each rule as a binary feature.\n    This hyperparameter specifies the strength of the ridge (L2) regularizer.\n    Since the rules are quite strongly correlated, the ridge regularizer is the most useful to stabilize the weight estimates.\n"
":name" = "StableRulesClassifier"
":human_name" = "stable rule-based classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":fit", ":predict"]
":hyperparameters" = "`(:rng, :partial_sampling, :n_trees, :max_depth, :q, :min_data_in_leaf, :max_rules)`"
":hyperparameter_types" = "`(\"Random.AbstractRNG\", \"Real\", \"Int\", \"Int\", \"Int\", \"Int\", \"Int\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[SIRUS.StableRulesRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "SIRUS"
":package_license" = "MIT"
":load_path" = "SIRUS.StableRulesClassifier"
":package_uuid" = "cdeec39e-fb35-4959-aadb-a1dd5dede958"
":package_url" = "https://github.com/rikhuijzer/SIRUS.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "    StableForestRegressor(;\n        rng::AbstractRNG=default_rng(),\n        partial_sampling::Real=0.7,\n        n_trees::Int=1_000,\n        max_depth::Int=2,\n        q::Int=10,\n        min_data_in_leaf::Int=5\n    ) <: MLJModelInterface.Probabilistic\n\nRandom forest regressor with a stabilized forest structure (Bénard et al., [2021](http://proceedings.mlr.press/v130/benard21a.html)).\nSee the documentation for the `StableForestClassifier` for more information about the hyperparameters.\n\n# Example\n\nThe classifier satisfies the MLJ interface, so it can be used like any other MLJ model.\nFor example, it can be used to create a machine:\n\n```julia\njulia> using SIRUS, MLJ\n\njulia> mach = machine(StableForestRegressor(), X, y);\n```\n"
":name" = "StableRulesRegressor"
":human_name" = "stable rule-based regressor"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":fit", ":predict"]
":hyperparameters" = "`(:rng, :partial_sampling, :n_trees, :max_depth, :q, :min_data_in_leaf, :max_rules)`"
":hyperparameter_types" = "`(\"Random.AbstractRNG\", \"Real\", \"Int\", \"Int\", \"Int\", \"Int\", \"Int\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[TSVD.TSVDTransformer]
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":inverse_transform_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":is_pure_julia" = "`true`"
":package_name" = "TSVD"
":package_license" = "MIT"
":load_path" = "MLJTSVDInterface.TSVDTransformer"
":package_uuid" = "9449cd9e-2762-5aa3-a617-5413e99d722e"
":package_url" = "https://github.com/JuliaLinearAlgebra/TSVD.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "Truncated SVD dimensionality reduction"
":name" = "TSVDTransformer"
":human_name" = "truncated SVD transformer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":transform"]
":hyperparameters" = "`(:nvals, :maxiter, :rng)`"
":hyperparameter_types" = "`(\"Int64\", \"Int64\", \"Union{Int64, Random.AbstractRNG}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[GLM.LinearBinaryClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Binary}`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Binary}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Binary}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Binary}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "GLM"
":package_license" = "MIT"
":load_path" = "MLJGLMInterface.LinearBinaryClassifier"
":package_uuid" = "38e38edf-8417-5370-95a0-9cbb8c7f171a"
":package_url" = "https://github.com/JuliaStats/GLM.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLinearBinaryClassifier\n```\n\nA model type for constructing a linear binary classifier, based on [GLM.jl](https://github.com/JuliaStats/GLM.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nLinearBinaryClassifier = @load LinearBinaryClassifier pkg=GLM\n```\n\nDo `model = LinearBinaryClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `LinearBinaryClassifier(fit_intercept=...)`.\n\n`LinearBinaryClassifier` is a [generalized linear model](https://en.wikipedia.org/wiki/Generalized_linear_model#Variance_function), specialised to the case of a binary target variable, with a user-specified link function. Options exist to specify an intercept or offset feature.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with one of:\n\n```\nmach = machine(model, X, y)\nmach = machine(model, X, y, w)\n```\n\nHere\n\n  * `X`: is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check the scitype with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `<:OrderedFactor(2)` or `<:Multiclass(2)`; check the scitype with `schema(y)`\n  * `w`: is a vector of `Real` per-observation weights\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `fit_intercept=true`: Whether to calculate the intercept for this model.  If set to false,  no intercept will be calculated (e.g. the data is expected to be centered)\n  * `link=GLM.LogitLink`: The function which links the linear prediction function to the  probability of a particular outcome or class. This must have type `GLM.Link01`. Options  include `GLM.LogitLink()`, `GLM.ProbitLink()`, `CloglogLink(),`CauchitLink()`.\n  * `offsetcol=nothing`: Name of the column to be used as an offset, if any.  An offset is a  variable which is known to have a coefficient of 1.\n  * `maxiter::Integer=30`: The maximum number of iterations allowed to achieve convergence.\n  * `atol::Real=1e-6`: Absolute threshold for convergence. Convergence is achieved when the  relative change in deviance is less than `max(rtol*dev, atol). This term exists to avoid  failure when deviance is unchanged except for rounding errors.\n  * `rtol::Real=1e-6`: Relative threshold for convergence. Convergence is achieved when the  relative change in deviance is less than `max(rtol*dev, atol). This term exists to avoid  failure when deviance is unchanged except for rounding errors.\n  * `minstepfac::Real=0.001`: Minimum step fraction. Must be between 0 and 1. Lower bound for the factor used to update the linear fit.\n  * `report_keys::Union{Symbol, Nothing}=DEFAULT_KEYS`: keys to be used in the report. Should be one of: `:deviance`, `:dof_residual`, `:stderror`, `:vcov`, `:coef_table`.\n\n# Operations\n\n  * `predict(mach, Xnew)`: Return predictions of the target given features `Xnew` having the same scitype as `X` above. Predictions are probabilistic.\n  * `predict_mode(mach, Xnew)`: Return the modes of the probabilistic predictions returned  above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `features`: The names of the features used during model fitting.\n  * `coef`: The linear coefficients determined by the model.\n  * `intercept`: The intercept determined by the model.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `deviance`: Measure of deviance of fitted model with respect to a perfectly fitted model. For a linear model, this is the weighted residual sum of squares\n  * `dof_residual`: The degrees of freedom for residuals, when meaningful.\n  * `stderror`: The standard errors of the coefficients.\n  * `vcov`: The estimated variance-covariance matrix of the coefficient estimates.\n  * `coef_table`: Table which displays coefficients and summarizes their significance and confidence intervals.\n\n# Examples\n\n```\nusing MLJ\nimport GLM # namespace must be available\n\nLinearBinaryClassifier = @load LinearBinaryClassifier pkg=GLM\nclf = LinearBinaryClassifier(fit_intercept=false, link=GLM.ProbitLink())\n\nX, y = @load_crabs\n\nmach = machine(clf, X, y) |> fit!\n\nXnew = (;FL = [8.1, 24.8, 7.2],\n        RW = [5.1, 25.7, 6.4],\n        CL = [15.9, 46.7, 14.3],\n        CW = [18.7, 59.7, 12.2],\n        BD = [6.2, 23.6, 8.4],)\n\nyhat = predict(mach, Xnew) # probabilistic predictions\npdf(yhat, levels(y)) # probability matrix\np_B = pdf.(yhat, \"B\")\nclass_labels = predict_mode(mach, Xnew)\n\nfitted_params(mach).features\nfitted_params(mach).coef\nfitted_params(mach).intercept\n\nreport(mach)\n```\n\nSee also [`LinearRegressor`](@ref), [`LinearCountRegressor`](@ref)\n"
":name" = "LinearBinaryClassifier"
":human_name" = "linear binary classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":predict_mean"]
":hyperparameters" = "`(:fit_intercept, :link, :offsetcol, :maxiter, :atol, :rtol, :minstepfac, :report_keys)`"
":hyperparameter_types" = "`(\"Bool\", \"GLM.Link01\", \"Union{Nothing, Symbol}\", \"Integer\", \"Real\", \"Real\", \"Real\", \"Union{Nothing, AbstractVector{Symbol}}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[GLM.LinearCountRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Count}`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Count}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Count}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{ScientificTypesBase.Count}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "GLM"
":package_license" = "MIT"
":load_path" = "MLJGLMInterface.LinearCountRegressor"
":package_uuid" = "38e38edf-8417-5370-95a0-9cbb8c7f171a"
":package_url" = "https://github.com/JuliaStats/GLM.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLinearCountRegressor\n```\n\nA model type for constructing a linear count regressor, based on [GLM.jl](https://github.com/JuliaStats/GLM.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nLinearCountRegressor = @load LinearCountRegressor pkg=GLM\n```\n\nDo `model = LinearCountRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `LinearCountRegressor(fit_intercept=...)`.\n\n`LinearCountRegressor` is a [generalized linear model](https://en.wikipedia.org/wiki/Generalized_linear_model#Variance_function), specialised to the case of a `Count` target variable (non-negative, unbounded integer) with user-specified link function. Options exist to specify an intercept or offset feature.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with one of:\n\n```\nmach = machine(model, X, y)\nmach = machine(model, X, y, w)\n```\n\nHere\n\n  * `X`: is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check the scitype with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `Count`; check the scitype with `schema(y)`\n  * `w`: is a vector of `Real` per-observation weights\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `fit_intercept=true`: Whether to calculate the intercept for this model. If set to false,  no intercept will be calculated (e.g. the data is expected to be centered)\n  * `distribution=Distributions.Poisson()`: The distribution which the residuals/errors of the  model should fit.\n  * `link=GLM.LogLink()`: The function which links the linear prediction function to the  probability of a particular outcome or class. This should be one of the following:  `GLM.IdentityLink()`, `GLM.InverseLink()`, `GLM.InverseSquareLink()`, `GLM.LogLink()`,  `GLM.SqrtLink()`.\n  * `offsetcol=nothing`: Name of the column to be used as an offset, if any.  An offset is a  variable which is known to have a coefficient of 1.\n  * `maxiter::Integer=30`: The maximum number of iterations allowed to achieve convergence.\n  * `atol::Real=1e-6`: Absolute threshold for convergence. Convergence is achieved when the  relative change in deviance is less than `max(rtol*dev, atol). This term exists to avoid  failure when deviance is unchanged except for rounding errors.\n  * `rtol::Real=1e-6`: Relative threshold for convergence. Convergence is achieved when the  relative change in deviance is less than `max(rtol*dev, atol). This term exists to avoid  failure when deviance is unchanged except for rounding errors.\n  * `minstepfac::Real=0.001`: Minimum step fraction. Must be between 0 and 1. Lower bound for the factor used to update the linear fit.\n  * `report_keys::Union{Symbol, Nothing}=DEFAULT_KEYS`: keys to be used in the report. Should be one of: `:deviance`, `:dof_residual`, `:stderror`, `:vcov`, `:coef_table`.\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given new features `Xnew` having  the same Scitype as `X` above. Predictions are probabilistic.\n  * `predict_mean(mach, Xnew)`: instead return the mean of each prediction above\n  * `predict_median(mach, Xnew)`: instead return the median of each prediction above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `features`: The names of the features encountered during model fitting.\n  * `coef`: The linear coefficients determined by the model.\n  * `intercept`: The intercept determined by the model.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `deviance`: Measure of deviance of fitted model with respect to a perfectly fitted model. For a linear model, this is the weighted residual sum of squares\n  * `dof_residual`: The degrees of freedom for residuals, when meaningful.\n  * `stderror`: The standard errors of the coefficients.\n  * `vcov`: The estimated variance-covariance matrix of the coefficient estimates.\n  * `coef_table`: Table which displays coefficients and summarizes their significance and confidence intervals.\n\n# Examples\n\n```\nusing MLJ\nimport MLJ.Distributions.Poisson\n\n# Generate some data whose target y looks Poisson when conditioned on\n# X:\nN = 10_000\nw = [1.0, -2.0, 3.0]\nmu(x) = exp(w'x) # mean for a log link function\nXmat = rand(N, 3)\nX = MLJ.table(Xmat)\ny = map(1:N) do i\n    x = Xmat[i, :]\n    rand(Poisson(mu(x)))\nend;\n\nCountRegressor = @load LinearCountRegressor pkg=GLM\nmodel = CountRegressor(fit_intercept=false)\nmach = machine(model, X, y)\nfit!(mach)\n\nXnew = MLJ.table(rand(3, 3))\nyhat = predict(mach, Xnew)\nyhat_point = predict_mean(mach, Xnew)\n\n# get coefficients approximating `w`:\njulia> fitted_params(mach).coef\n3-element Vector{Float64}:\n  0.9969008753103842\n -2.0255901752504775\n  3.014407534033522\n\nreport(mach)\n```\n\nSee also [`LinearRegressor`](@ref), [`LinearBinaryClassifier`](@ref)\n"
":name" = "LinearCountRegressor"
":human_name" = "linear count regressor"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":predict_mean"]
":hyperparameters" = "`(:fit_intercept, :distribution, :link, :offsetcol, :maxiter, :atol, :rtol, :minstepfac, :report_keys)`"
":hyperparameter_types" = "`(\"Bool\", \"Distributions.Distribution\", \"GLM.Link\", \"Union{Nothing, Symbol}\", \"Integer\", \"Real\", \"Real\", \"Real\", \"Union{Nothing, AbstractVector{Symbol}}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[GLM.LinearRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "GLM"
":package_license" = "MIT"
":load_path" = "MLJGLMInterface.LinearRegressor"
":package_uuid" = "38e38edf-8417-5370-95a0-9cbb8c7f171a"
":package_url" = "https://github.com/JuliaStats/GLM.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nLinearRegressor\n```\n\nA model type for constructing a linear regressor, based on [GLM.jl](https://github.com/JuliaStats/GLM.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nLinearRegressor = @load LinearRegressor pkg=GLM\n```\n\nDo `model = LinearRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `LinearRegressor(fit_intercept=...)`.\n\n`LinearRegressor` assumes the target is a continuous variable whose conditional distribution is normal with constant variance, and whose expected value is a linear combination of the features (identity link function). Options exist to specify an intercept or offset feature.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with one of:\n\n```\nmach = machine(model, X, y)\nmach = machine(model, X, y, w)\n```\n\nHere\n\n  * `X`: is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check the scitype with `schema(X)`\n  * `y`: is the target, which can be any `AbstractVector` whose element scitype is `Continuous`; check the scitype with `scitype(y)`\n  * `w`: is a vector of `Real` per-observation weights\n\n# Hyper-parameters\n\n  * `fit_intercept=true`: Whether to calculate the intercept for this model.  If set to false, no intercept will be calculated (e.g. the data is expected  to be centered)\n  * `dropcollinear=false`: Whether to drop features in the training data to ensure linear independence.  If true , only the first of each set of linearly-dependent features is used. The coefficient for redundant linearly dependent features is `0.0` and all associated statistics are set to `NaN`.\n  * `offsetcol=nothing`: Name of the column to be used as an offset, if any.  An offset is a variable which is known to have a coefficient of 1.\n  * `report_keys::Union{Symbol, Nothing}=DEFAULT_KEYS`: keys to be used in the report. Should be one of: `:deviance`, `:dof_residual`, `:stderror`, `:vcov`, `:coef_table`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given new  features `Xnew` having the same Scitype as `X` above. Predictions are  probabilistic.\n  * `predict_mean(mach, Xnew)`: instead return the mean of  each prediction above\n  * `predict_median(mach, Xnew)`: instead return the median of  each prediction above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `features`: The names of the features encountered during model fitting.\n  * `coef`: The linear coefficients determined by the model.\n  * `intercept`: The intercept determined by the model.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `deviance`: Measure of deviance of fitted model with respect to a perfectly fitted model. For a linear model, this is the weighted residual sum of squares\n  * `dof_residual`: The degrees of freedom for residuals, when meaningful.\n  * `stderror`: The standard errors of the coefficients.\n  * `vcov`: The estimated variance-covariance matrix of the coefficient estimates.\n  * `coef_table`: Table which displays coefficients and summarizes their significance and confidence intervals.\n\n# Examples\n\n```\nusing MLJ\nLinearRegressor = @load LinearRegressor pkg=GLM\nglm = LinearRegressor()\n\nX, y = make_regression(100, 2) # synthetic data\nmach = machine(glm, X, y) |> fit!\n\nXnew, _ = make_regression(3, 2)\nyhat = predict(mach, Xnew) # new predictions\nyhat_point = predict_mean(mach, Xnew) # new predictions\n\nfitted_params(mach).features\nfitted_params(mach).coef # x1, x2, intercept\nfitted_params(mach).intercept\n\nreport(mach)\n```\n\nSee also [`LinearCountRegressor`](@ref), [`LinearBinaryClassifier`](@ref)\n"
":name" = "LinearRegressor"
":human_name" = "linear regressor"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":predict_mean"]
":hyperparameters" = "`(:fit_intercept, :dropcollinear, :offsetcol, :report_keys)`"
":hyperparameter_types" = "`(\"Bool\", \"Bool\", \"Union{Nothing, Symbol}\", \"Union{Nothing, AbstractVector{Symbol}}\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"

[MLJFlux.MultitargetNeuralNetworkRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":predict_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJFlux"
":package_license" = "MIT"
":load_path" = "MLJFlux.MultitargetNeuralNetworkRegressor"
":package_uuid" = "094fc8d1-fd35-5302-93ea-dabda2abf845"
":package_url" = "https://github.com/alan-turing-institute/MLJFlux.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nMultitargetNeuralNetworkRegressor\n```\n\nA model type for constructing a multitarget neural network regressor, based on [MLJFlux.jl](https://github.com/alan-turing-institute/MLJFlux.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nMultitargetNeuralNetworkRegressor = @load MultitargetNeuralNetworkRegressor pkg=MLJFlux\n```\n\nDo `model = MultitargetNeuralNetworkRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `MultitargetNeuralNetworkRegressor(builder=...)`.\n\n`MultitargetNeuralNetworkRegressor` is for training a data-dependent Flux.jl neural network to predict a multi-valued `Continuous` target, represented as a table, given a table of `Continuous` features. Users provide a recipe for constructing the network, based on properties of the data that is encountered, by specifying an appropriate `builder`. See MLJFlux documentation for more on builders.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n  * `y` is the target, which can be any table of output targets whose element scitype is `Continuous`; check column scitypes with `schema(y)`.\n\n# Hyper-parameters\n\n  * `builder=MLJFlux.Linear(σ=Flux.relu)`: An MLJFlux builder that constructs a neural network. Possible `builders` include: `Linear`, `Short`, and `MLP`. See MLJFlux documentation for more on builders, and the example below for using the `@builder` convenience macro.\n  * `optimiser::Flux.Adam()`: A `Flux.Optimise` optimiser. The optimiser performs the updating of the weights of the network. For further reference, see [the Flux optimiser documentation](https://fluxml.ai/Flux.jl/stable/training/optimisers/). To choose a learning rate (the update rate of the optimizer), a good rule of thumb is to start out at `10e-3`, and tune using powers of 10 between `1` and `1e-7`.\n  * `loss=Flux.mse`: The loss function which the network will optimize. Should be a function which can be called in the form `loss(yhat, y)`.  Possible loss functions are listed in [the Flux loss function documentation](https://fluxml.ai/Flux.jl/stable/models/losses/). For a regression task, natural loss functions are:\n\n      * `Flux.mse`\n      * `Flux.mae`\n      * `Flux.msle`\n      * `Flux.huber_loss`\n\n    Currently MLJ measures are not supported as loss functions here.\n  * `epochs::Int=10`: The duration of training, in epochs. Typically, one epoch represents one pass through the complete the training dataset.\n  * `batch_size::int=1`: the batch size to be used for training, representing the number of samples per update of the network weights. Typically, batch size is between 8 and\n\n    512. Increassing batch size may accelerate training if `acceleration=CUDALibs()` and a\n\n    GPU is available.\n  * `lambda::Float64=0`: The strength of the weight regularization penalty. Can be any value in the range `[0, ∞)`.\n  * `alpha::Float64=0`: The L2/L1 mix of regularization, in the range `[0, 1]`. A value of 0 represents L2 regularization, and a value of 1 represents L1 regularization.\n  * `rng::Union{AbstractRNG, Int64}`: The random number generator or seed used during training.\n  * `optimizer_changes_trigger_retraining::Bool=false`: Defines what happens when re-fitting a machine if the associated optimiser has changed. If `true`, the associated machine will retrain from scratch on `fit!` call, otherwise it will not.\n  * `acceleration::AbstractResource=CPU1()`: Defines on what hardware training is done. For Training on GPU, use `CUDALibs()`.\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given new features `Xnew` having the same scitype as `X` above. Predictions are deterministic.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `chain`: The trained \"chain\" (Flux.jl model), namely the series of layers,  functions, and activations  which make up the neural network.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `training_losses`: A vector of training losses (penalised if `lambda != 0`) in  historical order, of length `epochs + 1`.  The first element is the pre-training loss.\n\n# Examples\n\nIn this example we apply a multi-target regression model to synthetic data:\n\n```julia\nusing MLJ\nimport MLJFlux\nusing Flux\n```\n\nFirst, we generate some synthetic data (needs MLJBase 0.20.16 or higher):\n\n```julia\nX, y = make_regression(100, 9; n_targets = 2) # both tables\nschema(y)\nschema(X)\n```\n\nSplitting off a test set:\n\n```julia\n(X, Xtest), (y, ytest) = partition((X, y), 0.7, multi=true);\n```\n\nNext, we can define a `builder`, making use of a convenience macro to do so.  In the following `@builder` call, `n_in` is a proxy for the number input features and `n_out` the number of target variables (both known at `fit!` time), while `rng` is a proxy for a RNG (which will be passed from the `rng` field of `model` defined below).\n\n```julia\nbuilder = MLJFlux.@builder begin\n    init=Flux.glorot_uniform(rng)\n    Chain(\n        Dense(n_in, 64, relu, init=init),\n        Dense(64, 32, relu, init=init),\n        Dense(32, n_out, init=init),\n    )\nend\n```\n\nInstantiating the regression model:\n\n```julia\nMultitargetNeuralNetworkRegressor = @load MultitargetNeuralNetworkRegressor\nmodel = MultitargetNeuralNetworkRegressor(builder=builder, rng=123, epochs=20)\n```\n\nWe will arrange for standardization of the the target by wrapping our model in  `TransformedTargetModel`, and standardization of the features by inserting the wrapped  model in a pipeline:\n\n```julia\npipe = Standardizer |> TransformedTargetModel(model, target=Standardizer)\n```\n\nIf we fit with a high verbosity (>1), we will see the losses during training. We can also see the losses in the output of `report(mach)`\n\n```julia\nmach = machine(pipe, X, y)\nfit!(mach, verbosity=2)\n\n# first element initial loss, 2:end per epoch training losses\nreport(mach).transformed_target_model_deterministic.model.training_losses\n```\n\nFor experimenting with learning rate, see the [`NeuralNetworkRegressor`](@ref) example.\n\n```\npipe.transformed_target_model_deterministic.model.optimiser.eta = 0.0001\n```\n\nWith the learning rate fixed, we can now compute a CV estimate of the performance (using all data bound to `mach`) and compare this with performance on the test set:\n\n```julia\n# custom MLJ loss:\nmulti_loss(yhat, y) = l2(MLJ.matrix(yhat), MLJ.matrix(y)) |> mean\n\n# CV estimate, based on `(X, y)`:\nevaluate!(mach, resampling=CV(nfolds=5), measure=multi_loss)\n\n# loss for `(Xtest, test)`:\nfit!(mach) # trains on all data `(X, y)`\nyhat = predict(mach, Xtest)\nmulti_loss(yhat, ytest)\n```\n\nSee also [`NeuralNetworkRegressor`](@ref)\n"
":name" = "MultitargetNeuralNetworkRegressor"
":human_name" = "multitarget neural network regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":predict"]
":hyperparameters" = "`(:builder, :optimiser, :loss, :epochs, :batch_size, :lambda, :alpha, :rng, :optimiser_changes_trigger_retraining, :acceleration)`"
":hyperparameter_types" = "`(\"Any\", \"Any\", \"Any\", \"Int64\", \"Int64\", \"Float64\", \"Float64\", \"Union{Integer, Random.AbstractRNG}\", \"Bool\", \"ComputationalResources.AbstractResource\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = ":epochs"
":supports_training_losses" = "`true`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`(:optimiser, :builder)`"
":reporting_operations" = "`()`"

[MLJFlux.NeuralNetworkClassifier]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJFlux"
":package_license" = "MIT"
":load_path" = "MLJFlux.NeuralNetworkClassifier"
":package_uuid" = "094fc8d1-fd35-5302-93ea-dabda2abf845"
":package_url" = "https://github.com/alan-turing-institute/MLJFlux.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nNeuralNetworkClassifier\n```\n\nA model type for constructing a neural network classifier, based on [MLJFlux.jl](https://github.com/alan-turing-institute/MLJFlux.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nNeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux\n```\n\nDo `model = NeuralNetworkClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `NeuralNetworkClassifier(builder=...)`.\n\n`NeuralNetworkClassifier` is for training a data-dependent Flux.jl neural network for making probabilistic predictions of a `Multiclass` or `OrderedFactor` target, given a table of `Continuous` features. Users provide a recipe for constructing  the network, based on properties of the data that is encountered, by specifying  an appropriate `builder`. See MLJFlux documentation for more on builders.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `Multiclass` or `OrderedFactor`; check the scitype with `scitype(y)`\n\nTrain the machine with `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `builder=MLJFlux.Short()`: An MLJFlux builder that constructs a neural network. Possible  `builders` include: `MLJFlux.Linear`, `MLJFlux.Short`, and `MLJFlux.MLP`. See  MLJFlux.jl documentation for examples of user-defined builders. See also `finaliser`  below.\n  * `optimiser::Flux.Adam()`: A `Flux.Optimise` optimiser. The optimiser performs the updating of the weights of the network. For further reference, see [the Flux optimiser documentation](https://fluxml.ai/Flux.jl/stable/training/optimisers/). To choose a learning rate (the update rate of the optimizer), a good rule of thumb is to start out at `10e-3`, and tune using powers of 10 between `1` and `1e-7`.\n  * `loss=Flux.crossentropy`: The loss function which the network will optimize. Should be a function which can be called in the form `loss(yhat, y)`.  Possible loss functions are listed in [the Flux loss function documentation](https://fluxml.ai/Flux.jl/stable/models/losses/). For a classification task, the most natural loss functions are:\n\n      * `Flux.crossentropy`: Standard multiclass classification loss, also known as the log loss.\n      * `Flux.logitcrossentopy`: Mathematically equal to crossentropy, but numerically more stable than finalising the outputs with `softmax` and then calculating crossentropy. You will need to specify `finaliser=identity` to remove MLJFlux's default softmax finaliser, and understand that the output of `predict` is then unnormalized (no longer probabilistic).\n      * `Flux.tversky_loss`: Used with imbalanced data to give more weight to false negatives.\n      * `Flux.focal_loss`: Used with highly imbalanced data. Weights harder examples more than easier examples.\n\n    Currently MLJ measures are not supported values of `loss`.\n  * `epochs::Int=10`: The duration of training, in epochs. Typically, one epoch represents one pass through the complete the training dataset.\n  * `batch_size::int=1`: the batch size to be used for training, representing the number of samples per update of the network weights. Typically, batch size is between 8 and\n\n    512. Increassing batch size may accelerate training if `acceleration=CUDALibs()` and a\n\n    GPU is available.\n  * `lambda::Float64=0`: The strength of the weight regularization penalty. Can be any value in the range `[0, ∞)`.\n  * `alpha::Float64=0`: The L2/L1 mix of regularization, in the range `[0, 1]`. A value of 0 represents L2 regularization, and a value of 1 represents L1 regularization.\n  * `rng::Union{AbstractRNG, Int64}`: The random number generator or seed used during training.\n  * `optimizer_changes_trigger_retraining::Bool=false`: Defines what happens when re-fitting a machine if the associated optimiser has changed. If `true`, the associated machine will retrain from scratch on `fit!` call, otherwise it will not.\n  * `acceleration::AbstractResource=CPU1()`: Defines on what hardware training is done. For Training on GPU, use `CUDALibs()`.\n  * `finaliser=Flux.softmax`: The final activation function of the neural network (applied after the network defined by `builder`). Defaults to `Flux.softmax`.\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given new features `Xnew`, which should have the same scitype as `X` above. Predictions are probabilistic but uncalibrated.\n  * `predict_mode(mach, Xnew)`: Return the modes of the probabilistic predictions returned above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `chain`: The trained \"chain\" (Flux.jl model), namely the series of layers,  functions, and activations which make up the neural network. This includes  the final layer specified by `finaliser` (eg, `softmax`).\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `training_losses`: A vector of training losses (penalised if `lambda != 0`) in  historical order, of length `epochs + 1`.  The first element is the pre-training loss.\n\n# Examples\n\nIn this example we build a classification model using the Iris dataset. This is a very basic example, using a default builder and no standardization.  For a more advanced illustration, see [`NeuralNetworkRegressor`](@ref) or [`ImageClassifier`](@ref), and examples in the MLJFlux.jl documentation.\n\n```julia\nusing MLJ\nusing Flux\nimport RDatasets\n```\n\nFirst, we can load the data:\n\n```julia\niris = RDatasets.dataset(\"datasets\", \"iris\");\ny, X = unpack(iris, ==(:Species), rng=123); # a vector and a table\nNeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux\nclf = NeuralNetworkClassifier()\n```\n\nNext, we can train the model:\n\n```julia\nmach = machine(clf, X, y)\nfit!(mach)\n```\n\nWe can train the model in an incremental fashion, altering the learning rate as we go, provided `optimizer_changes_trigger_retraining` is `false` (the default). Here, we also change the number of (total) iterations:\n\n```julia\nclf.optimiser.eta = clf.optimiser.eta * 2\nclf.epochs = clf.epochs + 5\n\nfit!(mach, verbosity=2) # trains 5 more epochs\n```\n\nWe can inspect the mean training loss using the `cross_entropy` function:\n\n```julia\ntraining_loss = cross_entropy(predict(mach, X), y) |> mean\n```\n\nAnd we can access the Flux chain (model) using `fitted_params`:\n\n```julia\nchain = fitted_params(mach).chain\n```\n\nFinally, we can see how the out-of-sample performance changes over time, using MLJ's `learning_curve` function:\n\n```julia\nr = range(clf, :epochs, lower=1, upper=200, scale=:log10)\ncurve = learning_curve(clf, X, y,\n                     range=r,\n                     resampling=Holdout(fraction_train=0.7),\n                     measure=cross_entropy)\nusing Plots\nplot(curve.parameter_values,\n     curve.measurements,\n     xlab=curve.parameter_name,\n     xscale=curve.parameter_scale,\n     ylab = \"Cross Entropy\")\n\n```\n\nSee also [`ImageClassifier`](@ref).\n"
":name" = "NeuralNetworkClassifier"
":human_name" = "neural network classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":predict"]
":hyperparameters" = "`(:builder, :finaliser, :optimiser, :loss, :epochs, :batch_size, :lambda, :alpha, :rng, :optimiser_changes_trigger_retraining, :acceleration)`"
":hyperparameter_types" = "`(\"Any\", \"Any\", \"Any\", \"Any\", \"Int64\", \"Int64\", \"Float64\", \"Float64\", \"Union{Int64, Random.AbstractRNG}\", \"Bool\", \"ComputationalResources.AbstractResource\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = ":epochs"
":supports_training_losses" = "`true`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`(:optimiser, :builder)`"
":reporting_operations" = "`()`"

[MLJFlux.ImageClassifier]
":input_scitype" = "`AbstractVector{<:ScientificTypesBase.Image}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Multiclass}`"
":fit_data_scitype" = "`Tuple{AbstractVector{<:ScientificTypesBase.Image}, AbstractVector{<:ScientificTypesBase.Multiclass}}`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Multiclass}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJFlux"
":package_license" = "MIT"
":load_path" = "MLJFlux.ImageClassifier"
":package_uuid" = "094fc8d1-fd35-5302-93ea-dabda2abf845"
":package_url" = "https://github.com/alan-turing-institute/MLJFlux.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nImageClassifier\n```\n\nA model type for constructing a image classifier, based on [MLJFlux.jl](https://github.com/alan-turing-institute/MLJFlux.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nImageClassifier = @load ImageClassifier pkg=MLJFlux\n```\n\nDo `model = ImageClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `ImageClassifier(builder=...)`.\n\n`ImageClassifier` classifies images using a neural network adapted to the type of images provided (color or gray scale). Predictions are probabilistic. Users provide a recipe for constructing the network, based on properties of the image encountered, by specifying an appropriate `builder`. See MLJFlux documentation for more on builders.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any `AbstractVector` of images with `ColorImage` or `GrayImage` scitype; check  the scitype with `scitype(X)` and refer to ScientificTypes.jl documentation on coercing  typical image formats into an appropriate type.\n  * `y` is the target, which can be any `AbstractVector` whose element  scitype is `Multiclass`; check the scitype with `scitype(y)`.\n\nTrain the machine with `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `builder`: An MLJFlux builder that constructs the neural network.  The fallback builds a  depth-16 VGG architecture adapted to the image size and number of target classes, with  no batch normalization; see the Metalhead.jl documentation for details. See the example  below for a user-specified builder. A convenience macro `@builder` is also  available. See also `finaliser` below.\n  * `optimiser::Flux.Adam()`: A `Flux.Optimise` optimiser. The optimiser performs the updating of the weights of the network. For further reference, see [the Flux optimiser documentation](https://fluxml.ai/Flux.jl/stable/training/optimisers/). To choose a learning rate (the update rate of the optimizer), a good rule of thumb is to start out at `10e-3`, and tune using powers of 10 between `1` and `1e-7`.\n  * `loss=Flux.crossentropy`: The loss function which the network will optimize. Should be a function which can be called in the form `loss(yhat, y)`.  Possible loss functions are listed in [the Flux loss function documentation](https://fluxml.ai/Flux.jl/stable/models/losses/). For a classification task, the most natural loss functions are:\n\n      * `Flux.crossentropy`: Standard multiclass classification loss, also known as the log loss.\n      * `Flux.logitcrossentopy`: Mathematically equal to crossentropy, but numerically more stable than finalising the outputs with `softmax` and then calculating crossentropy. You will need to specify `finaliser=identity` to remove MLJFlux's default softmax finaliser, and understand that the output of `predict` is then unnormalized (no longer probabilistic).\n      * `Flux.tversky_loss`: Used with imbalanced data to give more weight to false negatives.\n      * `Flux.focal_loss`: Used with highly imbalanced data. Weights harder examples more than easier examples.\n\n    Currently MLJ measures are not supported values of `loss`.\n  * `epochs::Int=10`: The duration of training, in epochs. Typically, one epoch represents one pass through the complete the training dataset.\n  * `batch_size::int=1`: the batch size to be used for training, representing the number of samples per update of the network weights. Typically, batch size is between 8 and\n\n    512. Increassing batch size may accelerate training if `acceleration=CUDALibs()` and a\n\n    GPU is available.\n  * `lambda::Float64=0`: The strength of the weight regularization penalty. Can be any value in the range `[0, ∞)`.\n  * `alpha::Float64=0`: The L2/L1 mix of regularization, in the range `[0, 1]`. A value of 0 represents L2 regularization, and a value of 1 represents L1 regularization.\n  * `rng::Union{AbstractRNG, Int64}`: The random number generator or seed used during training.\n  * `optimizer_changes_trigger_retraining::Bool=false`: Defines what happens when re-fitting a machine if the associated optimiser has changed. If `true`, the associated machine will retrain from scratch on `fit!` call, otherwise it will not.\n  * `acceleration::AbstractResource=CPU1()`: Defines on what hardware training is done. For Training on GPU, use `CUDALibs()`.\n  * `finaliser=Flux.softmax`: The final activation function of the neural network (applied after the network defined by `builder`). Defaults to `Flux.softmax`.\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given new features `Xnew`, which should have the same scitype as `X` above. Predictions are probabilistic but uncalibrated.\n  * `predict_mode(mach, Xnew)`: Return the modes of the probabilistic predictions returned above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `chain`: The trained \"chain\" (Flux.jl model), namely the series of layers,  functions, and activations  which make up the neural network. This includes  the final layer specified by `finaliser` (eg, `softmax`).\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `training_losses`: A vector of training losses (penalised if `lambda != 0`) in  historical order, of length `epochs + 1`.  The first element is the pre-training loss.\n\n# Examples\n\nIn this example we use MLJFlux and a custom builder to classify the MNIST image dataset.\n\n```julia\nusing MLJ\nusing Flux\nimport MLJFlux\nimport MLJIteration # for `skip` control\n```\n\nFirst we want to download the MNIST dataset, and unpack into images and labels:\n\n```julia\nimport MLDatasets: MNIST\ndata = MNIST(split=:train)\nimages, labels = data.features, data.targets\n```\n\nIn MLJ, integers cannot be used for encoding categorical data, so we must coerce them into the `Multiclass` scitype:\n\n```julia\nlabels = coerce(labels, Multiclass);\n```\n\nAbove `images` is a single array but MLJFlux requires the images to be a vector of individual image arrays:\n\n```\nimages = coerce(images, GrayImage);\nimages[1]\n```\n\nWe start by defining a suitable `builder` object. This is a recipe for building the neural network. Our builder will work for images of any (constant) size, whether they be color or black and white (ie, single or multi-channel).  The architecture always consists of six alternating convolution and max-pool layers, and a final dense layer; the filter size and the number of channels after each convolution layer is customizable.\n\n```julia\nimport MLJFlux\n\nstruct MyConvBuilder\n    filter_size::Int\n    channels1::Int\n    channels2::Int\n    channels3::Int\nend\n\nmake2d(x::AbstractArray) = reshape(x, :, size(x)[end])\n\nfunction MLJFlux.build(b::MyConvBuilder, rng, n_in, n_out, n_channels)\n    k, c1, c2, c3 = b.filter_size, b.channels1, b.channels2, b.channels3\n    mod(k, 2) == 1 || error(\"`filter_size` must be odd. \")\n    p = div(k - 1, 2) # padding to preserve image size\n    init = Flux.glorot_uniform(rng)\n    front = Chain(\n        Conv((k, k), n_channels => c1, pad=(p, p), relu, init=init),\n        MaxPool((2, 2)),\n        Conv((k, k), c1 => c2, pad=(p, p), relu, init=init),\n        MaxPool((2, 2)),\n        Conv((k, k), c2 => c3, pad=(p, p), relu, init=init),\n        MaxPool((2 ,2)),\n        make2d)\n    d = Flux.outputsize(front, (n_in..., n_channels, 1)) |> first\n    return Chain(front, Dense(d, n_out, init=init))\nend\n```\n\nIt is important to note that in our `build` function, there is no final `softmax`. This is applied by default in all MLJFlux classifiers (override this using the `finaliser` hyperparameter).\n\nNow that our builder is defined, we can instantiate the actual MLJFlux model. If you have a GPU, you can substitute in `acceleration=CUDALibs()` below to speed up training.\n\n```julia\nImageClassifier = @load ImageClassifier pkg=MLJFlux\nclf = ImageClassifier(builder=MyConvBuilder(3, 16, 32, 32),\n                      batch_size=50,\n                      epochs=10,\n                      rng=123)\n```\n\nYou can add Flux options such as `optimiser` and `loss` in the snippet above. Currently, `loss` must be a flux-compatible loss, and not an MLJ measure.\n\nNext, we can bind the model with the data in a machine, and train using the first 500 images:\n\n```julia\nmach = machine(clf, images, labels);\nfit!(mach, rows=1:500, verbosity=2);\nreport(mach)\nchain = fitted_params(mach)\nFlux.params(chain)[2]\n```\n\nWe can tack on 20 more epochs by modifying the `epochs` field, and iteratively fit some more:\n\n```julia\nclf.epochs = clf.epochs + 20\nfit!(mach, rows=1:500, verbosity=2);\n```\n\nWe can also make predictions and calculate an out-of-sample loss estimate, using any MLJ measure (loss/score):\n\n```julia\npredicted_labels = predict(mach, rows=501:1000);\ncross_entropy(predicted_labels, labels[501:1000]) |> mean\n```\n\nThe preceding `fit!`/`predict`/evaluate workflow can be alternatively executed as follows:\n\n```julia\nevaluate!(mach,\n          resampling=Holdout(fraction_train=0.5),\n          measure=cross_entropy,\n          rows=1:1000,\n          verbosity=0)\n```\n\nSee also [`NeuralNetworkClassifier`](@ref).\n"
":name" = "ImageClassifier"
":human_name" = "image classifier"
":is_supervised" = "`true`"
":prediction_type" = ":probabilistic"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":implemented_methods" = [":predict"]
":hyperparameters" = "`(:builder, :finaliser, :optimiser, :loss, :epochs, :batch_size, :lambda, :alpha, :rng, :optimiser_changes_trigger_retraining, :acceleration)`"
":hyperparameter_types" = "`(\"Any\", \"Any\", \"Any\", \"Any\", \"Int64\", \"Int64\", \"Float64\", \"Float64\", \"Union{Int64, Random.AbstractRNG}\", \"Bool\", \"ComputationalResources.AbstractResource\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = ":epochs"
":supports_training_losses" = "`true`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`(:optimiser, :builder)`"
":reporting_operations" = "`()`"

[MLJFlux.NeuralNetworkRegressor]
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_pure_julia" = "`true`"
":package_name" = "MLJFlux"
":package_license" = "MIT"
":load_path" = "MLJFlux.NeuralNetworkRegressor"
":package_uuid" = "094fc8d1-fd35-5302-93ea-dabda2abf845"
":package_url" = "https://github.com/alan-turing-institute/MLJFlux.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = "```\nNeuralNetworkRegressor\n```\n\nA model type for constructing a neural network regressor, based on [MLJFlux.jl](https://github.com/alan-turing-institute/MLJFlux.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nNeuralNetworkRegressor = @load NeuralNetworkRegressor pkg=MLJFlux\n```\n\nDo `model = NeuralNetworkRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `NeuralNetworkRegressor(builder=...)`.\n\n`NeuralNetworkRegressor` is for training a data-dependent Flux.jl neural network to predict a `Continuous` target, given a table of `Continuous` features. Users provide a recipe for constructing the network, based on properties of the data that is encountered, by specifying an appropriate `builder`. See MLJFlux documentation for more on builders.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check the column scitypes with `schema(X)`.\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `Continuous`; check the scitype with `scitype(y)`\n\nTrain the machine with `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `builder=MLJFlux.Linear(σ=Flux.relu)`: An MLJFlux builder that constructs a neural  network. Possible `builders` include: `MLJFlux.Linear`, `MLJFlux.Short`, and  `MLJFlux.MLP`. See MLJFlux documentation for more on builders, and the example below  for using the `@builder` convenience macro.\n  * `optimiser::Flux.Adam()`: A `Flux.Optimise` optimiser. The optimiser performs the updating of the weights of the network. For further reference, see [the Flux optimiser documentation](https://fluxml.ai/Flux.jl/stable/training/optimisers/). To choose a learning rate (the update rate of the optimizer), a good rule of thumb is to start out at `10e-3`, and tune using powers of 10 between `1` and `1e-7`.\n  * `loss=Flux.mse`: The loss function which the network will optimize. Should be a function which can be called in the form `loss(yhat, y)`.  Possible loss functions are listed in [the Flux loss function documentation](https://fluxml.ai/Flux.jl/stable/models/losses/). For a regression task, natural loss functions are:\n\n      * `Flux.mse`\n      * `Flux.mae`\n      * `Flux.msle`\n      * `Flux.huber_loss`\n\n    Currently MLJ measures are not supported as loss functions here.\n  * `epochs::Int=10`: The duration of training, in epochs. Typically, one epoch represents one pass through the complete the training dataset.\n  * `batch_size::int=1`: the batch size to be used for training, representing the number of samples per update of the network weights. Typically, batch size is between 8 and\n\n    512. Increasing batch size may accelerate training if `acceleration=CUDALibs()` and a\n\n    GPU is available.\n  * `lambda::Float64=0`: The strength of the weight regularization penalty. Can be any value in the range `[0, ∞)`.\n  * `alpha::Float64=0`: The L2/L1 mix of regularization, in the range `[0, 1]`. A value of 0 represents L2 regularization, and a value of 1 represents L1 regularization.\n  * `rng::Union{AbstractRNG, Int64}`: The random number generator or seed used during training.\n  * `optimizer_changes_trigger_retraining::Bool=false`: Defines what happens when re-fitting a machine if the associated optimiser has changed. If `true`, the associated machine will retrain from scratch on `fit!` call, otherwise it will not.\n  * `acceleration::AbstractResource=CPU1()`: Defines on what hardware training is done. For Training on GPU, use `CUDALibs()`.\n\n# Operations\n\n  * `predict(mach, Xnew)`: return predictions of the target given new features `Xnew`, which should have the same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `chain`: The trained \"chain\" (Flux.jl model), namely the series of layers, functions,  and activations which make up the neural network.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `training_losses`: A vector of training losses (penalized if `lambda != 0`) in  historical order, of length `epochs + 1`.  The first element is the pre-training loss.\n\n# Examples\n\nIn this example we build a regression model for the Boston house price dataset.\n\n```julia\nusing MLJ\nimport MLJFlux\nusing Flux\n```\n\nFirst, we load in the data: The `:MEDV` column becomes the target vector `y`, and all remaining columns go into a table `X`, with the exception of `:CHAS`:\n\n```julia\ndata = OpenML.load(531); # Loads from https://www.openml.org/d/531\ny, X = unpack(data, ==(:MEDV), !=(:CHAS); rng=123);\n\nscitype(y)\nschema(X)\n```\n\nSince MLJFlux models do not handle ordered factors, we'll treat `:RAD` as `Continuous`:\n\n```julia\nX = coerce(X, :RAD=>Continuous)\n```\n\nSplitting off a test set:\n\n```julia\n(X, Xtest), (y, ytest) = partition((X, y), 0.7, multi=true);\n```\n\nNext, we can define a `builder`, making use of a convenience macro to do so.  In the following `@builder` call, `n_in` is a proxy for the number input features (which will be known at `fit!` time) and `rng` is a proxy for a RNG (which will be passed from the `rng` field of `model` defined below). We also have the parameter `n_out` which is the number of output features. As we are doing single target regression, the value passed will always be `1`, but the builder we define will also work for [`MultitargetNeuralRegressor`](@ref).\n\n```julia\nbuilder = MLJFlux.@builder begin\n    init=Flux.glorot_uniform(rng)\n    Chain(\n        Dense(n_in, 64, relu, init=init),\n        Dense(64, 32, relu, init=init),\n        Dense(32, n_out, init=init),\n    )\nend\n```\n\nInstantiating a model:\n\n```julia\nNeuralNetworkRegressor = @load NeuralNetworkRegressor pkg=MLJFlux\nmodel = NeuralNetworkRegressor(\n    builder=builder,\n    rng=123,\n    epochs=20\n)\n```\n\nWe arrange for standardization of the the target by wrapping our model in `TransformedTargetModel`, and standardization of the features by inserting the wrapped model in a pipeline:\n\n```julia\npipe = Standardizer |> TransformedTargetModel(model, target=Standardizer)\n```\n\nIf we fit with a high verbosity (>1), we will see the losses during training. We can also see the losses in the output of `report(mach)`.\n\n```julia\nmach = machine(pipe, X, y)\nfit!(mach, verbosity=2)\n\n# first element initial loss, 2:end per epoch training losses\nreport(mach).transformed_target_model_deterministic.model.training_losses\n```\n\n## Experimenting with learning rate\n\nWe can visually compare how the learning rate affects the predictions:\n\n```julia\nusing Plots\n\nrates = rates = [5e-5, 1e-4, 0.005, 0.001, 0.05]\nplt=plot()\n\nforeach(rates) do η\n  pipe.transformed_target_model_deterministic.model.optimiser.eta = η\n  fit!(mach, force=true, verbosity=0)\n  losses =\n      report(mach).transformed_target_model_deterministic.model.training_losses[3:end]\n  plot!(1:length(losses), losses, label=η)\nend\n\nplt\n\npipe.transformed_target_model_deterministic.model.optimiser.eta = 0.0001\n```\n\nWith the learning rate fixed, we compute a CV estimate of the performance (using all data bound to `mach`) and compare this with performance on the test set:\n\n```julia\n# CV estimate, based on `(X, y)`:\nevaluate!(mach, resampling=CV(nfolds=5), measure=l2)\n\n# loss for `(Xtest, test)`:\nfit!(mach) # train on `(X, y)`\nyhat = predict(mach, Xtest)\nl2(yhat, ytest)  |> mean\n```\n\nThese losses, for the pipeline model, refer to the target on the original, unstandardized, scale.\n\nFor implementing stopping criterion and other iteration controls, refer to examples linked from the MLJFlux documentation.\n\nSee also [`MultitargetNeuralNetworkRegressor`](@ref)\n"
":name" = "NeuralNetworkRegressor"
":human_name" = "neural network regressor"
":is_supervised" = "`true`"
":prediction_type" = ":deterministic"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":implemented_methods" = [":predict"]
":hyperparameters" = "`(:builder, :optimiser, :loss, :epochs, :batch_size, :lambda, :alpha, :rng, :optimiser_changes_trigger_retraining, :acceleration)`"
":hyperparameter_types" = "`(\"Any\", \"Any\", \"Any\", \"Int64\", \"Int64\", \"Float64\", \"Float64\", \"Union{Integer, Random.AbstractRNG}\", \"Bool\", \"ComputationalResources.AbstractResource\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = ":epochs"
":supports_training_losses" = "`true`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`(:optimiser, :builder)`"
":reporting_operations" = "`()`"
